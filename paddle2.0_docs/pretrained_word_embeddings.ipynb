{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用预训练的词向量\n",
    "\n",
    "Author: [Dongyang Yan]（623320480@qq.com, github.com/fiyen ）\n",
    "\n",
    "Data created: 2020/11/23\n",
    "\n",
    "Last modified: 2020/11/24\n",
    "\n",
    "Description: Tutorial to classify Imdb data using pre-trained word embeddings in paddlepaddle 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 摘要\n",
    "\n",
    "在这个示例中，我们将使用飞桨2.0完成针对Imdb数据集（电影评论情感二分类数据集）的分类训练和测试。Imbd将直接调用自飞桨2.0，同时，\n",
    "利用预训练的词向量（[GloVe embedding](http://nlp.stanford.edu/projects/glove/))完成任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle as pd\r\n",
    "from paddle.io import Dataset\r\n",
    "import numpy as np\r\n",
    "import paddle.text as pt\r\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 用飞桨2.0调用Imdb数据集\n",
    "由于飞桨2.0提供了经过处理的Imdb数据集，我们可以方便地调用所需要的数据实例，省去了数据预处理的麻烦。目前，飞桨2.0以及内置的高质量\n",
    "数据集包括Conll05st、Imdb、Imikolov、Movielens、HCIHousing、WMT14和WMT16等，未来还将提供更多常用数据集的调用接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_train = pt.Imdb(mode='train', cutoff=150)\r\n",
    "imdb_test = pt.Imdb(mode='test', cutoff=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "调用Imdb得到的是经过编码的内容。每个样本表示一个文档，以list的形式储存，list中的每个元素都由一个数字表示，对应文档相应位置的某个单词，\n",
    "而单词和数字编码是一一对应的。其对应关系可以通过imdb_train.word_idx查看。我们可以检查一下以上生成的数据内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量: 25000; 测试集样本数量: 25000\n",
      "样本标签: {0, 1}\n",
      "样本字典: [(b'the', 0), (b'and', 1), (b'a', 2), (b'of', 3), (b'to', 4), (b'is', 5), (b'in', 6), (b'it', 7), (b'i', 8), (b'this', 9)]\n",
      "单个样本: [5146, 43, 71, 6, 1092, 14, 0, 878, 130, 151, 5146, 18, 281, 747, 0, 5146, 3, 5146, 2165, 37, 5146, 46, 5, 71, 4089, 377, 162, 46, 5, 32, 1287, 300, 35, 203, 2136, 565, 14, 2, 253, 26, 146, 61, 372, 1, 615, 5146, 5, 30, 0, 50, 3290, 6, 2148, 14, 0, 5146, 11, 17, 451, 24, 4, 127, 10, 0, 878, 130, 43, 2, 50, 5146, 751, 5146, 5, 2, 221, 3727, 6, 9, 1167, 373, 9, 5, 5146, 7, 5, 1343, 13, 2, 5146, 1, 250, 7, 98, 4270, 56, 2316, 0, 928, 11, 11, 9, 16, 5, 5146, 5146, 6, 50, 69, 27, 280, 27, 108, 1045, 0, 2633, 4177, 3180, 17, 1675, 1, 2571]\n",
      "最小样本长度: 10;最大样本长度: 2469\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集样本数量: %d; 测试集样本数量: %d\" % (len(imdb_train), len(imdb_test)))\r\n",
    "print(f\"样本标签: {set(imdb_train.labels)}\")\r\n",
    "print(f\"样本字典: {list(imdb_train.word_idx.items())[:10]}\")\r\n",
    "print(f\"单个样本: {imdb_train.docs[0]}\")\r\n",
    "print(f\"最小样本长度: {min([len(x) for x in imdb_train.docs])};最大样本长度: {max([len(x) for x in imdb_train.docs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上参数中，cutoff定义了构建词典的截止大小，即数据集中出现频率在cutoff以下的不予考虑；mode定义了返回的数据用于何种用途（test: \n",
    "测试集，train: 训练集）。对于训练集，我们将数据的顺序打乱，以优化将要进行的分类模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle_index = list(range(len(imdb_train)))\r\n",
    "random.shuffle(shuffle_index)\r\n",
    "train_x = [imdb_train.docs[i] for i in shuffle_index]\r\n",
    "train_y = [imdb_train.labels[i] for i in shuffle_index]\r\n",
    "\r\n",
    "test_x = imdb_test.docs\r\n",
    "test_y = imdb_test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从样本长度上可以看到，每个样本的长度是不相同的。然而，在模型的训练过程中，需要保证每个样本的长度相同，以便于构造矩阵进行批量运算。\n",
    "因此，我们需要先对所有样本进行填充或截断，使样本的长度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorizer(input, label=None, length=2000):\r\n",
    "    if label is not None:\r\n",
    "        for x, y in zip(input, label):\r\n",
    "            yield np.array((x + [0]*length)[:2000]).astype('int64'), np.array([y]).astype('int64')\r\n",
    "    else:\r\n",
    "        for x in input:\r\n",
    "            yield np.array((x + [0]*length)[:2000]).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 载入预训练向量。\n",
    "以下给出的文件较小，可以直接完全载入内存。对于大型的预训练向量，无法一次载入内存的，可以采用分批载入，并行\n",
    "处理的方式进行匹配。这里略过此部分，如果感兴趣可以参考[此链接](https://aistudio.baidu.com/aistudio/projectdetail/496368)进一步了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下载预训练向量文件，此链接下载较慢，较快下载请转网址：https://aistudio.baidu.com/aistudio/datasetdetail/42051\r\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
    "!unzip -q glove.6B.zip\r\n",
    "\r\n",
    "glove_path = \"./glove.6B.100d.txt\"  # 请修改至glove.6B.100d.txt所在位置\r\n",
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们先观察上述GloVe预训练向量文件一行的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe单行数据：'the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# 使用utf8编码解码\r\n",
    "with open(glove_path, encoding='utf-8') as gf:\r\n",
    "    line = gf.readline()\r\n",
    "    print(\"GloVe单行数据：'%s'\" % line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，每一行都以单词开头，其后接上该单词的向量值，各个值之间用空格隔开。基于此，可以用如下方法得到所有词向量的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练词向量总数：400000\n",
      "单词'the'的向量是：[-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477, -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]\n"
     ]
    }
   ],
   "source": [
    "with open(glove_path, encoding='utf-8') as gf:\r\n",
    "    for glove in gf:\r\n",
    "        word, embedding = glove.split(maxsplit=1)\r\n",
    "        embedding = [float(s) for s in embedding.split(' ')]\r\n",
    "        embeddings[word] = embedding\r\n",
    "print(\"预训练词向量总数：%d\" % len(embeddings))\r\n",
    "print(f\"单词'the'的向量是：{embeddings['the']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 给数据集的词表匹配词向量\n",
    "接下来，我们提取数据集的词表，需要注意的是，词表中的词编码的先后顺序是按照词出现的频率排列的，频率越高的词编码值越小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表的前5个单词：[b'the', b'and', b'a', b'of', b'to']\n",
      "词表的后5个单词：[b'troubles', b'virtual', b'warriors', b'widely', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "word_idx = imdb_train.word_idx\r\n",
    "vocab = [w for w in word_idx.keys()]\r\n",
    "print(f\"词表的前5个单词：{vocab[:5]}\")\r\n",
    "print(f\"词表的后5个单词：{vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "观察词表的后5个单词，我们发现，最后一个词是\"<unk>\"，这个符号代表所有词表以外的词。另外，对于形式b'the'，是字符串'the'\n",
    "的二进制编码形式，使用中注意使用b'the'.decode()来进行转换（'$<unk>$'并没有进行二进制编码，注意区分）。\n",
    "接下来，我们给词表中的每个词匹配对应的词向量。预训练词向量可能没有覆盖数据集词表中的所有词，对于没有的词，我们设该词的词\n",
    "向量为零向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义词向量的维度，注意与预训练词向量保持一致\r\n",
    "dim = 100\r\n",
    "\r\n",
    "vocab_embeddings = np.zeros((len(vocab), dim))\r\n",
    "for ind, word in enumerate(vocab):\r\n",
    "    if word != '<unk>':\r\n",
    "        word = word.decode()\r\n",
    "    embedding = embeddings.get(word, np.zeros((dim,)))\r\n",
    "    vocab_embeddings[ind, :] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 构建基于预训练向量的Embedding\n",
    "对于预训练向量的Embedding，我们一般期望它的参数不再变动，所以要设置trainable=False。如果希望在此基础上训练参数，则需要\n",
    "设置trainable=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained_attr = pd.ParamAttr(name='embedding',\r\n",
    "                               initializer=pd.nn.initializer.Assign(vocab_embeddings),\r\n",
    "                               trainable=False)\r\n",
    "embedding_layer = pd.nn.Embedding(num_embeddings=len(vocab),\r\n",
    "                                  embedding_dim=dim,\r\n",
    "                                  padding_idx=word_idx['<unk>'],\r\n",
    "                                  weight_attr=pretrained_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 构建分类器\n",
    "这里，我们构建简单的基于一维卷积的分类模型，其结构为：Embedding->Conv1D->Pool1D->Linear。在定义Linear时，由于需要知\n",
    "道输入向量的维度，我们可以按照公式[官方文档](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-beta/api/paddle/nn/layer/conv/Conv2d_cn.html)\n",
    "来进行计算。这里给出计算的函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "  Embedding-1       [[1, 2000]]         [1, 2000, 100]        514,700    \n",
      "   Conv1D-1       [[1, 2000, 100]]       [1, 998, 10]          5,010     \n",
      "    ReLU-1         [[1, 998, 10]]        [1, 998, 10]            0       \n",
      "  MaxPool1D-1      [[1, 998, 10]]        [1, 998, 5]             0       \n",
      "   Flatten-1       [[1, 998, 5]]          [1, 4990]              0       \n",
      "   Linear-1         [[1, 4990]]             [1, 2]             9,982     \n",
      "   Softmax-1          [[1, 2]]              [1, 2]               0       \n",
      "===========================================================================\n",
      "Total params: 529,692\n",
      "Trainable params: 529,692\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.75\n",
      "Params size (MB): 2.02\n",
      "Estimated Total Size (MB): 3.78\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 529692, 'trainable_params': 529692}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_output_shape(input_shape, out_channels, kernel_size, stride, padding=0, dilation=1):\r\n",
    "    return out_channels, int((input_shape + 2*padding - (dilation*(kernel_size - 1) + 1)) / stride) + 1\r\n",
    "\r\n",
    "\r\n",
    "# 定义每个样本的长度\r\n",
    "length = 2000\r\n",
    "\r\n",
    "# 定义卷积层参数\r\n",
    "kernel_size = 5\r\n",
    "out_channels = 10\r\n",
    "stride = 2\r\n",
    "padding = 0\r\n",
    "\r\n",
    "output_shape = cal_output_shape(length, out_channels, kernel_size, stride, padding)\r\n",
    "output_shape = cal_output_shape(output_shape[1], output_shape[0], 2, 2, 0)\r\n",
    "sim_model = pd.nn.Sequential(embedding_layer,\r\n",
    "                         pd.nn.Conv1D(in_channels=dim, out_channels=out_channels, kernel_size=kernel_size,\r\n",
    "                                      stride=stride, padding=padding, data_format='NLC', bias_attr=True),\r\n",
    "                         pd.nn.ReLU(),\r\n",
    "                         pd.nn.MaxPool1D(kernel_size=2, stride=2),\r\n",
    "                         pd.nn.Flatten(),\r\n",
    "                         pd.nn.Linear(in_features=np.prod(output_shape), out_features=2, bias_attr=True),\r\n",
    "                         pd.nn.Softmax())\r\n",
    "\r\n",
    "pd.summary(sim_model, input_size=(-1, length), dtypes='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 读取数据，进行训练\n",
    "我们可以利用飞桨2.0的io.Dataset模块来构建一个数据的读取器，方便地将数据进行分批训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "step  10/586 - loss: 0.8757 - acc: 0.4813 - 18ms/step\n",
      "step  20/586 - loss: 0.8331 - acc: 0.4828 - 13ms/step\n",
      "step  30/586 - loss: 0.6944 - acc: 0.5042 - 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  40/586 - loss: 0.7220 - acc: 0.5070 - 10ms/step\n",
      "step  50/586 - loss: 0.6808 - acc: 0.4981 - 9ms/step\n",
      "step  60/586 - loss: 0.7056 - acc: 0.5010 - 9ms/step\n",
      "step  70/586 - loss: 0.6920 - acc: 0.5004 - 8ms/step\n",
      "step  80/586 - loss: 0.6837 - acc: 0.5035 - 8ms/step\n",
      "step  90/586 - loss: 0.6995 - acc: 0.4997 - 8ms/step\n",
      "step 100/586 - loss: 0.6805 - acc: 0.5056 - 8ms/step\n",
      "step 110/586 - loss: 0.6981 - acc: 0.5051 - 8ms/step\n",
      "step 120/586 - loss: 0.7033 - acc: 0.5070 - 8ms/step\n",
      "step 130/586 - loss: 0.7437 - acc: 0.5108 - 8ms/step\n",
      "step 140/586 - loss: 0.6721 - acc: 0.5109 - 8ms/step\n",
      "step 150/586 - loss: 0.6856 - acc: 0.5083 - 7ms/step\n",
      "step 160/586 - loss: 0.6862 - acc: 0.5119 - 7ms/step\n",
      "step 170/586 - loss: 0.6881 - acc: 0.5132 - 7ms/step\n",
      "step 180/586 - loss: 0.6655 - acc: 0.5141 - 7ms/step\n",
      "step 190/586 - loss: 0.6620 - acc: 0.5155 - 7ms/step\n",
      "step 200/586 - loss: 0.6299 - acc: 0.5219 - 7ms/step\n",
      "step 210/586 - loss: 0.7355 - acc: 0.5228 - 7ms/step\n",
      "step 220/586 - loss: 0.6562 - acc: 0.5267 - 7ms/step\n",
      "step 230/586 - loss: 0.6495 - acc: 0.5318 - 7ms/step\n",
      "step 240/586 - loss: 0.6333 - acc: 0.5375 - 7ms/step\n",
      "step 250/586 - loss: 0.6000 - acc: 0.5427 - 8ms/step\n",
      "step 260/586 - loss: 0.5711 - acc: 0.5496 - 8ms/step\n",
      "step 270/586 - loss: 0.5693 - acc: 0.5546 - 8ms/step\n",
      "step 280/586 - loss: 0.6908 - acc: 0.5616 - 8ms/step\n",
      "step 290/586 - loss: 0.6217 - acc: 0.5685 - 8ms/step\n",
      "step 300/586 - loss: 0.5417 - acc: 0.5743 - 8ms/step\n",
      "step 310/586 - loss: 0.5207 - acc: 0.5780 - 8ms/step\n",
      "step 320/586 - loss: 0.5410 - acc: 0.5841 - 8ms/step\n",
      "step 330/586 - loss: 0.5647 - acc: 0.5883 - 8ms/step\n",
      "step 340/586 - loss: 0.4975 - acc: 0.5930 - 8ms/step\n",
      "step 350/586 - loss: 0.5611 - acc: 0.5988 - 8ms/step\n",
      "step 360/586 - loss: 0.5176 - acc: 0.6044 - 8ms/step\n",
      "step 370/586 - loss: 0.4878 - acc: 0.6087 - 8ms/step\n",
      "step 380/586 - loss: 0.5079 - acc: 0.6131 - 8ms/step\n",
      "step 390/586 - loss: 0.4918 - acc: 0.6178 - 8ms/step\n",
      "step 400/586 - loss: 0.4999 - acc: 0.6220 - 8ms/step\n",
      "step 410/586 - loss: 0.5087 - acc: 0.6254 - 8ms/step\n",
      "step 420/586 - loss: 0.4500 - acc: 0.6286 - 8ms/step\n",
      "step 430/586 - loss: 0.4677 - acc: 0.6338 - 8ms/step\n",
      "step 440/586 - loss: 0.4354 - acc: 0.6377 - 8ms/step\n",
      "step 450/586 - loss: 0.4049 - acc: 0.6424 - 8ms/step\n",
      "step 460/586 - loss: 0.4874 - acc: 0.6459 - 8ms/step\n",
      "step 470/586 - loss: 0.6287 - acc: 0.6497 - 8ms/step\n",
      "step 480/586 - loss: 0.4633 - acc: 0.6535 - 8ms/step\n",
      "step 490/586 - loss: 0.4972 - acc: 0.6573 - 8ms/step\n",
      "step 500/586 - loss: 0.5369 - acc: 0.6603 - 8ms/step\n",
      "step 510/586 - loss: 0.5170 - acc: 0.6634 - 8ms/step\n",
      "step 520/586 - loss: 0.4569 - acc: 0.6665 - 8ms/step\n",
      "step 530/586 - loss: 0.4837 - acc: 0.6696 - 8ms/step\n",
      "step 540/586 - loss: 0.4510 - acc: 0.6726 - 8ms/step\n",
      "step 550/586 - loss: 0.5162 - acc: 0.6756 - 8ms/step\n",
      "step 560/586 - loss: 0.4821 - acc: 0.6781 - 8ms/step\n",
      "step 570/586 - loss: 0.4589 - acc: 0.6806 - 8ms/step\n",
      "step 580/586 - loss: 0.4688 - acc: 0.6830 - 8ms/step\n",
      "step 586/586 - loss: 0.4162 - acc: 0.6847 - 8ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4399 - acc: 0.8313 - 3ms/step\n",
      "step  20/196 - loss: 0.4896 - acc: 0.8266 - 2ms/step\n",
      "step  30/196 - loss: 0.6432 - acc: 0.8187 - 2ms/step\n",
      "step  40/196 - loss: 0.4953 - acc: 0.8156 - 2ms/step\n",
      "step  50/196 - loss: 0.4499 - acc: 0.8081 - 2ms/step\n",
      "step  60/196 - loss: 0.4401 - acc: 0.8130 - 2ms/step\n",
      "step  70/196 - loss: 0.4320 - acc: 0.8121 - 2ms/step\n",
      "step  80/196 - loss: 0.5158 - acc: 0.8102 - 2ms/step\n",
      "step  90/196 - loss: 0.6223 - acc: 0.8115 - 2ms/step\n",
      "step 100/196 - loss: 0.4908 - acc: 0.8172 - 2ms/step\n",
      "step 110/196 - loss: 0.4968 - acc: 0.8173 - 2ms/step\n",
      "step 120/196 - loss: 0.4446 - acc: 0.8161 - 2ms/step\n",
      "step 130/196 - loss: 0.4763 - acc: 0.8159 - 2ms/step\n",
      "step 140/196 - loss: 0.4702 - acc: 0.8174 - 2ms/step\n",
      "step 150/196 - loss: 0.5083 - acc: 0.8163 - 2ms/step\n",
      "step 160/196 - loss: 0.5015 - acc: 0.8139 - 2ms/step\n",
      "step 170/196 - loss: 0.5416 - acc: 0.8116 - 2ms/step\n",
      "step 180/196 - loss: 0.4286 - acc: 0.8120 - 2ms/step\n",
      "step 190/196 - loss: 0.5156 - acc: 0.8123 - 2ms/step\n",
      "step 196/196 - loss: 0.5552 - acc: 0.8122 - 2ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 2/10\n",
      "step  10/586 - loss: 0.4843 - acc: 0.8375 - 7ms/step\n",
      "step  20/586 - loss: 0.4507 - acc: 0.8516 - 7ms/step\n",
      "step  30/586 - loss: 0.5005 - acc: 0.8521 - 7ms/step\n",
      "step  40/586 - loss: 0.4608 - acc: 0.8531 - 7ms/step\n",
      "step  50/586 - loss: 0.4466 - acc: 0.8481 - 7ms/step\n",
      "step  60/586 - loss: 0.5826 - acc: 0.8406 - 7ms/step\n",
      "step  70/586 - loss: 0.4946 - acc: 0.8415 - 7ms/step\n",
      "step  80/586 - loss: 0.4346 - acc: 0.8410 - 7ms/step\n",
      "step  90/586 - loss: 0.4112 - acc: 0.8465 - 7ms/step\n",
      "step 100/586 - loss: 0.4780 - acc: 0.8472 - 7ms/step\n",
      "step 110/586 - loss: 0.4085 - acc: 0.8477 - 7ms/step\n",
      "step 120/586 - loss: 0.4291 - acc: 0.8490 - 7ms/step\n",
      "step 130/586 - loss: 0.4203 - acc: 0.8498 - 7ms/step\n",
      "step 140/586 - loss: 0.4696 - acc: 0.8496 - 7ms/step\n",
      "step 150/586 - loss: 0.4195 - acc: 0.8502 - 7ms/step\n",
      "step 160/586 - loss: 0.4378 - acc: 0.8520 - 7ms/step\n",
      "step 170/586 - loss: 0.4465 - acc: 0.8528 - 7ms/step\n",
      "step 180/586 - loss: 0.4533 - acc: 0.8535 - 7ms/step\n",
      "step 190/586 - loss: 0.4143 - acc: 0.8556 - 7ms/step\n",
      "step 200/586 - loss: 0.4385 - acc: 0.8567 - 7ms/step\n",
      "step 210/586 - loss: 0.4712 - acc: 0.8580 - 7ms/step\n",
      "step 220/586 - loss: 0.4541 - acc: 0.8587 - 7ms/step\n",
      "step 230/586 - loss: 0.5102 - acc: 0.8598 - 7ms/step\n",
      "step 240/586 - loss: 0.4461 - acc: 0.8604 - 7ms/step\n",
      "step 250/586 - loss: 0.4888 - acc: 0.8598 - 7ms/step\n",
      "step 260/586 - loss: 0.4808 - acc: 0.8594 - 7ms/step\n",
      "step 270/586 - loss: 0.3762 - acc: 0.8600 - 7ms/step\n",
      "step 280/586 - loss: 0.4755 - acc: 0.8609 - 7ms/step\n",
      "step 290/586 - loss: 0.4851 - acc: 0.8610 - 7ms/step\n",
      "step 300/586 - loss: 0.4570 - acc: 0.8615 - 7ms/step\n",
      "step 310/586 - loss: 0.4403 - acc: 0.8611 - 7ms/step\n",
      "step 320/586 - loss: 0.3967 - acc: 0.8611 - 7ms/step\n",
      "step 330/586 - loss: 0.5665 - acc: 0.8614 - 7ms/step\n",
      "step 340/586 - loss: 0.4581 - acc: 0.8616 - 7ms/step\n",
      "step 350/586 - loss: 0.4790 - acc: 0.8614 - 7ms/step\n",
      "step 360/586 - loss: 0.4301 - acc: 0.8619 - 7ms/step\n",
      "step 370/586 - loss: 0.4055 - acc: 0.8617 - 7ms/step\n",
      "step 380/586 - loss: 0.3873 - acc: 0.8626 - 7ms/step\n",
      "step 390/586 - loss: 0.3884 - acc: 0.8635 - 7ms/step\n",
      "step 400/586 - loss: 0.3815 - acc: 0.8634 - 7ms/step\n",
      "step 410/586 - loss: 0.4561 - acc: 0.8633 - 7ms/step\n",
      "step 420/586 - loss: 0.4677 - acc: 0.8631 - 7ms/step\n",
      "step 430/586 - loss: 0.4463 - acc: 0.8624 - 7ms/step\n",
      "step 440/586 - loss: 0.4642 - acc: 0.8624 - 7ms/step\n",
      "step 450/586 - loss: 0.4780 - acc: 0.8626 - 7ms/step\n",
      "step 460/586 - loss: 0.4521 - acc: 0.8627 - 7ms/step\n",
      "step 470/586 - loss: 0.4318 - acc: 0.8628 - 7ms/step\n",
      "step 480/586 - loss: 0.4390 - acc: 0.8628 - 7ms/step\n",
      "step 490/586 - loss: 0.4787 - acc: 0.8629 - 7ms/step\n",
      "step 500/586 - loss: 0.4620 - acc: 0.8631 - 7ms/step\n",
      "step 510/586 - loss: 0.5165 - acc: 0.8631 - 7ms/step\n",
      "step 520/586 - loss: 0.4316 - acc: 0.8623 - 7ms/step\n",
      "step 530/586 - loss: 0.3964 - acc: 0.8627 - 7ms/step\n",
      "step 540/586 - loss: 0.4333 - acc: 0.8631 - 7ms/step\n",
      "step 550/586 - loss: 0.3577 - acc: 0.8629 - 7ms/step\n",
      "step 560/586 - loss: 0.4475 - acc: 0.8631 - 7ms/step\n",
      "step 570/586 - loss: 0.3820 - acc: 0.8634 - 7ms/step\n",
      "step 580/586 - loss: 0.4899 - acc: 0.8636 - 7ms/step\n",
      "step 586/586 - loss: 0.3425 - acc: 0.8641 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4062 - acc: 0.8781 - 3ms/step\n",
      "step  20/196 - loss: 0.4372 - acc: 0.8781 - 3ms/step\n",
      "step  30/196 - loss: 0.5886 - acc: 0.8750 - 3ms/step\n",
      "step  40/196 - loss: 0.4661 - acc: 0.8648 - 3ms/step\n",
      "step  50/196 - loss: 0.4340 - acc: 0.8612 - 3ms/step\n",
      "step  60/196 - loss: 0.4301 - acc: 0.8604 - 3ms/step\n",
      "step  70/196 - loss: 0.4055 - acc: 0.8616 - 3ms/step\n",
      "step  80/196 - loss: 0.4645 - acc: 0.8590 - 3ms/step\n",
      "step  90/196 - loss: 0.5809 - acc: 0.8597 - 3ms/step\n",
      "step 100/196 - loss: 0.4399 - acc: 0.8606 - 3ms/step\n",
      "step 110/196 - loss: 0.4577 - acc: 0.8608 - 3ms/step\n",
      "step 120/196 - loss: 0.3500 - acc: 0.8581 - 3ms/step\n",
      "step 130/196 - loss: 0.4330 - acc: 0.8587 - 3ms/step\n",
      "step 140/196 - loss: 0.4096 - acc: 0.8603 - 3ms/step\n",
      "step 150/196 - loss: 0.4189 - acc: 0.8602 - 3ms/step\n",
      "step 160/196 - loss: 0.4849 - acc: 0.8588 - 3ms/step\n",
      "step 170/196 - loss: 0.4570 - acc: 0.8590 - 3ms/step\n",
      "step 180/196 - loss: 0.3667 - acc: 0.8601 - 3ms/step\n",
      "step 190/196 - loss: 0.4623 - acc: 0.8604 - 3ms/step\n",
      "step 196/196 - loss: 0.5284 - acc: 0.8619 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 3/10\n",
      "step  10/586 - loss: 0.4269 - acc: 0.8875 - 7ms/step\n",
      "step  20/586 - loss: 0.3295 - acc: 0.9031 - 7ms/step\n",
      "step  30/586 - loss: 0.4543 - acc: 0.9062 - 7ms/step\n",
      "step  40/586 - loss: 0.3627 - acc: 0.9102 - 7ms/step\n",
      "step  50/586 - loss: 0.4724 - acc: 0.9087 - 7ms/step\n",
      "step  60/586 - loss: 0.4065 - acc: 0.9104 - 7ms/step\n",
      "step  70/586 - loss: 0.3910 - acc: 0.9134 - 7ms/step\n",
      "step  80/586 - loss: 0.4536 - acc: 0.9086 - 7ms/step\n",
      "step  90/586 - loss: 0.4164 - acc: 0.9052 - 7ms/step\n",
      "step 100/586 - loss: 0.5490 - acc: 0.8994 - 7ms/step\n",
      "step 110/586 - loss: 0.4750 - acc: 0.8952 - 7ms/step\n",
      "step 120/586 - loss: 0.3541 - acc: 0.8964 - 7ms/step\n",
      "step 130/586 - loss: 0.3955 - acc: 0.8974 - 7ms/step\n",
      "step 140/586 - loss: 0.4073 - acc: 0.8971 - 7ms/step\n",
      "step 150/586 - loss: 0.4303 - acc: 0.8985 - 7ms/step\n",
      "step 160/586 - loss: 0.4012 - acc: 0.8984 - 7ms/step\n",
      "step 170/586 - loss: 0.4510 - acc: 0.8987 - 7ms/step\n",
      "step 180/586 - loss: 0.4806 - acc: 0.8993 - 7ms/step\n",
      "step 190/586 - loss: 0.4275 - acc: 0.8998 - 7ms/step\n",
      "step 200/586 - loss: 0.4005 - acc: 0.8995 - 7ms/step\n",
      "step 210/586 - loss: 0.4164 - acc: 0.8994 - 7ms/step\n",
      "step 220/586 - loss: 0.4389 - acc: 0.8999 - 7ms/step\n",
      "step 230/586 - loss: 0.4320 - acc: 0.9003 - 7ms/step\n",
      "step 240/586 - loss: 0.4554 - acc: 0.8995 - 7ms/step\n",
      "step 250/586 - loss: 0.4506 - acc: 0.8986 - 7ms/step\n",
      "step 260/586 - loss: 0.3554 - acc: 0.8987 - 7ms/step\n",
      "step 270/586 - loss: 0.4138 - acc: 0.8992 - 7ms/step\n",
      "step 280/586 - loss: 0.3524 - acc: 0.8987 - 7ms/step\n",
      "step 290/586 - loss: 0.3577 - acc: 0.8995 - 7ms/step\n",
      "step 300/586 - loss: 0.3739 - acc: 0.8996 - 7ms/step\n",
      "step 310/586 - loss: 0.3896 - acc: 0.8996 - 7ms/step\n",
      "step 320/586 - loss: 0.3983 - acc: 0.9000 - 7ms/step\n",
      "step 330/586 - loss: 0.4169 - acc: 0.9001 - 7ms/step\n",
      "step 340/586 - loss: 0.4219 - acc: 0.8982 - 7ms/step\n",
      "step 350/586 - loss: 0.5360 - acc: 0.8988 - 7ms/step\n",
      "step 360/586 - loss: 0.3557 - acc: 0.8984 - 7ms/step\n",
      "step 370/586 - loss: 0.4556 - acc: 0.8978 - 7ms/step\n",
      "step 380/586 - loss: 0.3822 - acc: 0.8975 - 7ms/step\n",
      "step 390/586 - loss: 0.4795 - acc: 0.8967 - 7ms/step\n",
      "step 400/586 - loss: 0.4399 - acc: 0.8965 - 7ms/step\n",
      "step 410/586 - loss: 0.4165 - acc: 0.8963 - 7ms/step\n",
      "step 420/586 - loss: 0.4211 - acc: 0.8968 - 7ms/step\n",
      "step 430/586 - loss: 0.3752 - acc: 0.8971 - 7ms/step\n",
      "step 440/586 - loss: 0.4722 - acc: 0.8962 - 7ms/step\n",
      "step 450/586 - loss: 0.3402 - acc: 0.8963 - 7ms/step\n",
      "step 460/586 - loss: 0.4418 - acc: 0.8967 - 7ms/step\n",
      "step 470/586 - loss: 0.3263 - acc: 0.8975 - 7ms/step\n",
      "step 480/586 - loss: 0.3991 - acc: 0.8974 - 7ms/step\n",
      "step 490/586 - loss: 0.3989 - acc: 0.8979 - 7ms/step\n",
      "step 500/586 - loss: 0.4587 - acc: 0.8978 - 7ms/step\n",
      "step 510/586 - loss: 0.3556 - acc: 0.8975 - 7ms/step\n",
      "step 520/586 - loss: 0.4912 - acc: 0.8977 - 7ms/step\n",
      "step 530/586 - loss: 0.4094 - acc: 0.8979 - 7ms/step\n",
      "step 540/586 - loss: 0.3773 - acc: 0.8984 - 7ms/step\n",
      "step 550/586 - loss: 0.4833 - acc: 0.8980 - 7ms/step\n",
      "step 560/586 - loss: 0.3811 - acc: 0.8980 - 7ms/step\n",
      "step 570/586 - loss: 0.4198 - acc: 0.8978 - 7ms/step\n",
      "step 580/586 - loss: 0.3985 - acc: 0.8984 - 7ms/step\n",
      "step 586/586 - loss: 0.4302 - acc: 0.8987 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4235 - acc: 0.8531 - 3ms/step\n",
      "step  20/196 - loss: 0.4380 - acc: 0.8562 - 3ms/step\n",
      "step  30/196 - loss: 0.5421 - acc: 0.8583 - 3ms/step\n",
      "step  40/196 - loss: 0.4682 - acc: 0.8562 - 3ms/step\n",
      "step  50/196 - loss: 0.4120 - acc: 0.8588 - 3ms/step\n",
      "step  60/196 - loss: 0.3863 - acc: 0.8589 - 3ms/step\n",
      "step  70/196 - loss: 0.4057 - acc: 0.8634 - 3ms/step\n",
      "step  80/196 - loss: 0.4562 - acc: 0.8633 - 3ms/step\n",
      "step  90/196 - loss: 0.5596 - acc: 0.8632 - 3ms/step\n",
      "step 100/196 - loss: 0.4493 - acc: 0.8653 - 3ms/step\n",
      "step 110/196 - loss: 0.4656 - acc: 0.8639 - 3ms/step\n",
      "step 120/196 - loss: 0.3922 - acc: 0.8604 - 3ms/step\n",
      "step 130/196 - loss: 0.4482 - acc: 0.8608 - 3ms/step\n",
      "step 140/196 - loss: 0.3829 - acc: 0.8632 - 3ms/step\n",
      "step 150/196 - loss: 0.4171 - acc: 0.8638 - 3ms/step\n",
      "step 160/196 - loss: 0.4876 - acc: 0.8615 - 3ms/step\n",
      "step 170/196 - loss: 0.4649 - acc: 0.8608 - 3ms/step\n",
      "step 180/196 - loss: 0.3737 - acc: 0.8627 - 3ms/step\n",
      "step 190/196 - loss: 0.4659 - acc: 0.8620 - 3ms/step\n",
      "step 196/196 - loss: 0.4331 - acc: 0.8634 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 4/10\n",
      "step  10/586 - loss: 0.4649 - acc: 0.8938 - 7ms/step\n",
      "step  20/586 - loss: 0.4502 - acc: 0.8891 - 7ms/step\n",
      "step  30/586 - loss: 0.3967 - acc: 0.8969 - 7ms/step\n",
      "step  40/586 - loss: 0.3733 - acc: 0.9000 - 7ms/step\n",
      "step  50/586 - loss: 0.4118 - acc: 0.9094 - 7ms/step\n",
      "step  60/586 - loss: 0.3935 - acc: 0.9094 - 7ms/step\n",
      "step  70/586 - loss: 0.3910 - acc: 0.9125 - 7ms/step\n",
      "step  80/586 - loss: 0.3524 - acc: 0.9168 - 7ms/step\n",
      "step  90/586 - loss: 0.3936 - acc: 0.9184 - 7ms/step\n",
      "step 100/586 - loss: 0.3414 - acc: 0.9219 - 7ms/step\n",
      "step 110/586 - loss: 0.3739 - acc: 0.9244 - 7ms/step\n",
      "step 120/586 - loss: 0.4057 - acc: 0.9237 - 7ms/step\n",
      "step 130/586 - loss: 0.3796 - acc: 0.9226 - 7ms/step\n",
      "step 140/586 - loss: 0.3649 - acc: 0.9219 - 7ms/step\n",
      "step 150/586 - loss: 0.3848 - acc: 0.9208 - 7ms/step\n",
      "step 160/586 - loss: 0.4138 - acc: 0.9207 - 7ms/step\n",
      "step 170/586 - loss: 0.3893 - acc: 0.9219 - 7ms/step\n",
      "step 180/586 - loss: 0.3575 - acc: 0.9229 - 7ms/step\n",
      "step 190/586 - loss: 0.3528 - acc: 0.9248 - 7ms/step\n",
      "step 200/586 - loss: 0.4436 - acc: 0.9231 - 7ms/step\n",
      "step 210/586 - loss: 0.3936 - acc: 0.9232 - 7ms/step\n",
      "step 220/586 - loss: 0.3917 - acc: 0.9213 - 7ms/step\n",
      "step 230/586 - loss: 0.3866 - acc: 0.9219 - 7ms/step\n",
      "step 240/586 - loss: 0.4124 - acc: 0.9224 - 7ms/step\n",
      "step 250/586 - loss: 0.4374 - acc: 0.9215 - 7ms/step\n",
      "step 260/586 - loss: 0.3602 - acc: 0.9218 - 7ms/step\n",
      "step 270/586 - loss: 0.3354 - acc: 0.9223 - 7ms/step\n",
      "step 280/586 - loss: 0.4723 - acc: 0.9220 - 7ms/step\n",
      "step 290/586 - loss: 0.3258 - acc: 0.9230 - 7ms/step\n",
      "step 300/586 - loss: 0.3674 - acc: 0.9236 - 7ms/step\n",
      "step 310/586 - loss: 0.3226 - acc: 0.9241 - 6ms/step\n",
      "step 320/586 - loss: 0.3961 - acc: 0.9241 - 6ms/step\n",
      "step 330/586 - loss: 0.4282 - acc: 0.9237 - 6ms/step\n",
      "step 340/586 - loss: 0.3943 - acc: 0.9235 - 6ms/step\n",
      "step 350/586 - loss: 0.4288 - acc: 0.9224 - 6ms/step\n",
      "step 360/586 - loss: 0.4189 - acc: 0.9221 - 7ms/step\n",
      "step 370/586 - loss: 0.4015 - acc: 0.9227 - 7ms/step\n",
      "step 380/586 - loss: 0.3946 - acc: 0.9230 - 7ms/step\n",
      "step 390/586 - loss: 0.3763 - acc: 0.9233 - 7ms/step\n",
      "step 400/586 - loss: 0.3684 - acc: 0.9232 - 7ms/step\n",
      "step 410/586 - loss: 0.3471 - acc: 0.9233 - 7ms/step\n",
      "step 420/586 - loss: 0.4221 - acc: 0.9234 - 7ms/step\n",
      "step 430/586 - loss: 0.4527 - acc: 0.9232 - 7ms/step\n",
      "step 440/586 - loss: 0.3835 - acc: 0.9233 - 7ms/step\n",
      "step 450/586 - loss: 0.4414 - acc: 0.9233 - 7ms/step\n",
      "step 460/586 - loss: 0.3542 - acc: 0.9235 - 7ms/step\n",
      "step 470/586 - loss: 0.3878 - acc: 0.9236 - 7ms/step\n",
      "step 480/586 - loss: 0.4531 - acc: 0.9235 - 7ms/step\n",
      "step 490/586 - loss: 0.4480 - acc: 0.9234 - 7ms/step\n",
      "step 500/586 - loss: 0.3302 - acc: 0.9239 - 7ms/step\n",
      "step 510/586 - loss: 0.3513 - acc: 0.9238 - 7ms/step\n",
      "step 520/586 - loss: 0.4588 - acc: 0.9237 - 7ms/step\n",
      "step 530/586 - loss: 0.3953 - acc: 0.9238 - 7ms/step\n",
      "step 540/586 - loss: 0.4340 - acc: 0.9242 - 7ms/step\n",
      "step 550/586 - loss: 0.3836 - acc: 0.9243 - 7ms/step\n",
      "step 560/586 - loss: 0.3799 - acc: 0.9241 - 7ms/step\n",
      "step 570/586 - loss: 0.4244 - acc: 0.9240 - 7ms/step\n",
      "step 580/586 - loss: 0.3150 - acc: 0.9236 - 7ms/step\n",
      "step 586/586 - loss: 0.5743 - acc: 0.9230 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.3942 - acc: 0.8906 - 2ms/step\n",
      "step  20/196 - loss: 0.4010 - acc: 0.8891 - 2ms/step\n",
      "step  30/196 - loss: 0.5784 - acc: 0.8750 - 2ms/step\n",
      "step  40/196 - loss: 0.4673 - acc: 0.8703 - 2ms/step\n",
      "step  50/196 - loss: 0.4671 - acc: 0.8669 - 2ms/step\n",
      "step  60/196 - loss: 0.4023 - acc: 0.8656 - 2ms/step\n",
      "step  70/196 - loss: 0.4319 - acc: 0.8679 - 2ms/step\n",
      "step  80/196 - loss: 0.4205 - acc: 0.8664 - 2ms/step\n",
      "step  90/196 - loss: 0.5517 - acc: 0.8656 - 2ms/step\n",
      "step 100/196 - loss: 0.4190 - acc: 0.8675 - 2ms/step\n",
      "step 110/196 - loss: 0.4450 - acc: 0.8682 - 2ms/step\n",
      "step 120/196 - loss: 0.3771 - acc: 0.8651 - 2ms/step\n",
      "step 130/196 - loss: 0.4033 - acc: 0.8659 - 2ms/step\n",
      "step 140/196 - loss: 0.4189 - acc: 0.8667 - 2ms/step\n",
      "step 150/196 - loss: 0.4362 - acc: 0.8660 - 2ms/step\n",
      "step 160/196 - loss: 0.5045 - acc: 0.8643 - 2ms/step\n",
      "step 170/196 - loss: 0.3803 - acc: 0.8651 - 2ms/step\n",
      "step 180/196 - loss: 0.3570 - acc: 0.8672 - 2ms/step\n",
      "step 190/196 - loss: 0.4183 - acc: 0.8679 - 2ms/step\n",
      "step 196/196 - loss: 0.5245 - acc: 0.8683 - 2ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 5/10\n",
      "step  10/586 - loss: 0.3663 - acc: 0.9437 - 7ms/step\n",
      "step  20/586 - loss: 0.3953 - acc: 0.9531 - 7ms/step\n",
      "step  30/586 - loss: 0.4353 - acc: 0.9448 - 7ms/step\n",
      "step  40/586 - loss: 0.4004 - acc: 0.9445 - 7ms/step\n",
      "step  50/586 - loss: 0.3962 - acc: 0.9437 - 7ms/step\n",
      "step  60/586 - loss: 0.3936 - acc: 0.9453 - 7ms/step\n",
      "step  70/586 - loss: 0.3608 - acc: 0.9455 - 6ms/step\n",
      "step  80/586 - loss: 0.3816 - acc: 0.9441 - 6ms/step\n",
      "step  90/586 - loss: 0.4682 - acc: 0.9437 - 6ms/step\n",
      "step 100/586 - loss: 0.3616 - acc: 0.9428 - 6ms/step\n",
      "step 110/586 - loss: 0.4110 - acc: 0.9432 - 6ms/step\n",
      "step 120/586 - loss: 0.3548 - acc: 0.9437 - 6ms/step\n",
      "step 130/586 - loss: 0.3788 - acc: 0.9433 - 6ms/step\n",
      "step 140/586 - loss: 0.3626 - acc: 0.9433 - 6ms/step\n",
      "step 150/586 - loss: 0.3856 - acc: 0.9435 - 6ms/step\n",
      "step 160/586 - loss: 0.4348 - acc: 0.9437 - 6ms/step\n",
      "step 170/586 - loss: 0.3337 - acc: 0.9443 - 6ms/step\n",
      "step 180/586 - loss: 0.3341 - acc: 0.9439 - 6ms/step\n",
      "step 190/586 - loss: 0.3483 - acc: 0.9434 - 6ms/step\n",
      "step 200/586 - loss: 0.3253 - acc: 0.9431 - 6ms/step\n",
      "step 210/586 - loss: 0.3671 - acc: 0.9418 - 6ms/step\n",
      "step 220/586 - loss: 0.3685 - acc: 0.9415 - 6ms/step\n",
      "step 230/586 - loss: 0.4182 - acc: 0.9413 - 6ms/step\n",
      "step 240/586 - loss: 0.3367 - acc: 0.9410 - 6ms/step\n",
      "step 250/586 - loss: 0.4380 - acc: 0.9407 - 6ms/step\n",
      "step 260/586 - loss: 0.3579 - acc: 0.9394 - 6ms/step\n",
      "step 270/586 - loss: 0.3499 - acc: 0.9388 - 6ms/step\n",
      "step 280/586 - loss: 0.4419 - acc: 0.9384 - 6ms/step\n",
      "step 290/586 - loss: 0.4185 - acc: 0.9378 - 6ms/step\n",
      "step 300/586 - loss: 0.4595 - acc: 0.9375 - 6ms/step\n",
      "step 310/586 - loss: 0.3226 - acc: 0.9378 - 6ms/step\n",
      "step 320/586 - loss: 0.3661 - acc: 0.9382 - 6ms/step\n",
      "step 330/586 - loss: 0.3806 - acc: 0.9383 - 6ms/step\n",
      "step 340/586 - loss: 0.4106 - acc: 0.9380 - 6ms/step\n",
      "step 350/586 - loss: 0.4062 - acc: 0.9375 - 6ms/step\n",
      "step 360/586 - loss: 0.3989 - acc: 0.9375 - 6ms/step\n",
      "step 370/586 - loss: 0.3514 - acc: 0.9383 - 6ms/step\n",
      "step 380/586 - loss: 0.3183 - acc: 0.9391 - 6ms/step\n",
      "step 390/586 - loss: 0.3472 - acc: 0.9395 - 6ms/step\n",
      "step 400/586 - loss: 0.3165 - acc: 0.9393 - 6ms/step\n",
      "step 410/586 - loss: 0.3192 - acc: 0.9393 - 6ms/step\n",
      "step 420/586 - loss: 0.3826 - acc: 0.9394 - 7ms/step\n",
      "step 430/586 - loss: 0.3252 - acc: 0.9401 - 7ms/step\n",
      "step 440/586 - loss: 0.3815 - acc: 0.9406 - 7ms/step\n",
      "step 450/586 - loss: 0.3926 - acc: 0.9408 - 7ms/step\n",
      "step 460/586 - loss: 0.4072 - acc: 0.9411 - 7ms/step\n",
      "step 470/586 - loss: 0.4134 - acc: 0.9412 - 7ms/step\n",
      "step 480/586 - loss: 0.3375 - acc: 0.9413 - 7ms/step\n",
      "step 490/586 - loss: 0.3880 - acc: 0.9414 - 7ms/step\n",
      "step 500/586 - loss: 0.3885 - acc: 0.9417 - 7ms/step\n",
      "step 510/586 - loss: 0.3638 - acc: 0.9417 - 7ms/step\n",
      "step 520/586 - loss: 0.4671 - acc: 0.9414 - 7ms/step\n",
      "step 530/586 - loss: 0.3618 - acc: 0.9412 - 7ms/step\n",
      "step 540/586 - loss: 0.3202 - acc: 0.9409 - 7ms/step\n",
      "step 550/586 - loss: 0.3325 - acc: 0.9405 - 7ms/step\n",
      "step 560/586 - loss: 0.3969 - acc: 0.9403 - 7ms/step\n",
      "step 570/586 - loss: 0.3870 - acc: 0.9399 - 7ms/step\n",
      "step 580/586 - loss: 0.3297 - acc: 0.9402 - 7ms/step\n",
      "step 586/586 - loss: 0.3533 - acc: 0.9400 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.3991 - acc: 0.8812 - 3ms/step\n",
      "step  20/196 - loss: 0.4031 - acc: 0.8875 - 2ms/step\n",
      "step  30/196 - loss: 0.5758 - acc: 0.8760 - 2ms/step\n",
      "step  40/196 - loss: 0.4588 - acc: 0.8695 - 3ms/step\n",
      "step  50/196 - loss: 0.4694 - acc: 0.8669 - 3ms/step\n",
      "step  60/196 - loss: 0.4034 - acc: 0.8661 - 3ms/step\n",
      "step  70/196 - loss: 0.4236 - acc: 0.8714 - 3ms/step\n",
      "step  80/196 - loss: 0.4264 - acc: 0.8703 - 3ms/step\n",
      "step  90/196 - loss: 0.5121 - acc: 0.8698 - 3ms/step\n",
      "step 100/196 - loss: 0.3963 - acc: 0.8709 - 3ms/step\n",
      "step 110/196 - loss: 0.4396 - acc: 0.8716 - 3ms/step\n",
      "step 120/196 - loss: 0.3787 - acc: 0.8680 - 3ms/step\n",
      "step 130/196 - loss: 0.4081 - acc: 0.8678 - 3ms/step\n",
      "step 140/196 - loss: 0.4171 - acc: 0.8676 - 3ms/step\n",
      "step 150/196 - loss: 0.4276 - acc: 0.8675 - 3ms/step\n",
      "step 160/196 - loss: 0.5145 - acc: 0.8660 - 3ms/step\n",
      "step 170/196 - loss: 0.3994 - acc: 0.8664 - 3ms/step\n",
      "step 180/196 - loss: 0.3495 - acc: 0.8686 - 3ms/step\n",
      "step 190/196 - loss: 0.4370 - acc: 0.8696 - 3ms/step\n",
      "step 196/196 - loss: 0.4342 - acc: 0.8706 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 6/10\n",
      "step  10/586 - loss: 0.3305 - acc: 0.9656 - 7ms/step\n",
      "step  20/586 - loss: 0.3285 - acc: 0.9641 - 7ms/step\n",
      "step  30/586 - loss: 0.3835 - acc: 0.9563 - 8ms/step\n",
      "step  40/586 - loss: 0.4051 - acc: 0.9492 - 7ms/step\n",
      "step  50/586 - loss: 0.3310 - acc: 0.9506 - 7ms/step\n",
      "step  60/586 - loss: 0.3157 - acc: 0.9542 - 7ms/step\n",
      "step  70/586 - loss: 0.3776 - acc: 0.9540 - 7ms/step\n",
      "step  80/586 - loss: 0.4235 - acc: 0.9531 - 8ms/step\n",
      "step  90/586 - loss: 0.3765 - acc: 0.9538 - 8ms/step\n",
      "step 100/586 - loss: 0.4109 - acc: 0.9537 - 8ms/step\n",
      "step 110/586 - loss: 0.3178 - acc: 0.9548 - 8ms/step\n",
      "step 120/586 - loss: 0.3332 - acc: 0.9560 - 8ms/step\n",
      "step 130/586 - loss: 0.3541 - acc: 0.9560 - 8ms/step\n",
      "step 140/586 - loss: 0.4426 - acc: 0.9551 - 8ms/step\n",
      "step 150/586 - loss: 0.3988 - acc: 0.9550 - 8ms/step\n",
      "step 160/586 - loss: 0.3752 - acc: 0.9553 - 8ms/step\n",
      "step 170/586 - loss: 0.3670 - acc: 0.9548 - 8ms/step\n",
      "step 180/586 - loss: 0.3524 - acc: 0.9542 - 8ms/step\n",
      "step 190/586 - loss: 0.4168 - acc: 0.9531 - 8ms/step\n",
      "step 200/586 - loss: 0.4119 - acc: 0.9536 - 8ms/step\n",
      "step 210/586 - loss: 0.3779 - acc: 0.9533 - 8ms/step\n",
      "step 220/586 - loss: 0.4391 - acc: 0.9536 - 8ms/step\n",
      "step 230/586 - loss: 0.3181 - acc: 0.9537 - 8ms/step\n",
      "step 240/586 - loss: 0.3546 - acc: 0.9543 - 8ms/step\n",
      "step 250/586 - loss: 0.3768 - acc: 0.9545 - 8ms/step\n",
      "step 260/586 - loss: 0.3607 - acc: 0.9544 - 7ms/step\n",
      "step 270/586 - loss: 0.3783 - acc: 0.9546 - 7ms/step\n",
      "step 280/586 - loss: 0.3453 - acc: 0.9542 - 7ms/step\n",
      "step 290/586 - loss: 0.3470 - acc: 0.9552 - 7ms/step\n",
      "step 300/586 - loss: 0.3719 - acc: 0.9547 - 7ms/step\n",
      "step 310/586 - loss: 0.3817 - acc: 0.9542 - 7ms/step\n",
      "step 320/586 - loss: 0.3873 - acc: 0.9546 - 7ms/step\n",
      "step 330/586 - loss: 0.3214 - acc: 0.9545 - 7ms/step\n",
      "step 340/586 - loss: 0.3188 - acc: 0.9546 - 7ms/step\n",
      "step 350/586 - loss: 0.4134 - acc: 0.9546 - 7ms/step\n",
      "step 360/586 - loss: 0.3154 - acc: 0.9549 - 7ms/step\n",
      "step 370/586 - loss: 0.3639 - acc: 0.9550 - 7ms/step\n",
      "step 380/586 - loss: 0.3960 - acc: 0.9550 - 7ms/step\n",
      "step 390/586 - loss: 0.3466 - acc: 0.9551 - 7ms/step\n",
      "step 400/586 - loss: 0.3370 - acc: 0.9555 - 7ms/step\n",
      "step 410/586 - loss: 0.3841 - acc: 0.9555 - 7ms/step\n",
      "step 420/586 - loss: 0.3942 - acc: 0.9552 - 7ms/step\n",
      "step 430/586 - loss: 0.3547 - acc: 0.9551 - 7ms/step\n",
      "step 440/586 - loss: 0.3170 - acc: 0.9553 - 7ms/step\n",
      "step 450/586 - loss: 0.3266 - acc: 0.9556 - 7ms/step\n",
      "step 460/586 - loss: 0.3429 - acc: 0.9553 - 7ms/step\n",
      "step 470/586 - loss: 0.3164 - acc: 0.9555 - 7ms/step\n",
      "step 480/586 - loss: 0.3724 - acc: 0.9555 - 7ms/step\n",
      "step 490/586 - loss: 0.3533 - acc: 0.9554 - 7ms/step\n",
      "step 500/586 - loss: 0.4149 - acc: 0.9556 - 7ms/step\n",
      "step 510/586 - loss: 0.3577 - acc: 0.9552 - 7ms/step\n",
      "step 520/586 - loss: 0.3712 - acc: 0.9553 - 7ms/step\n",
      "step 530/586 - loss: 0.3233 - acc: 0.9555 - 7ms/step\n",
      "step 540/586 - loss: 0.3177 - acc: 0.9556 - 7ms/step\n",
      "step 550/586 - loss: 0.3508 - acc: 0.9557 - 7ms/step\n",
      "step 560/586 - loss: 0.3778 - acc: 0.9553 - 7ms/step\n",
      "step 570/586 - loss: 0.3157 - acc: 0.9552 - 7ms/step\n",
      "step 580/586 - loss: 0.3832 - acc: 0.9551 - 7ms/step\n",
      "step 586/586 - loss: 0.3516 - acc: 0.9552 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.3740 - acc: 0.8875 - 3ms/step\n",
      "step  20/196 - loss: 0.3935 - acc: 0.8922 - 3ms/step\n",
      "step  30/196 - loss: 0.5860 - acc: 0.8771 - 3ms/step\n",
      "step  40/196 - loss: 0.4778 - acc: 0.8719 - 3ms/step\n",
      "step  50/196 - loss: 0.4675 - acc: 0.8669 - 3ms/step\n",
      "step  60/196 - loss: 0.3974 - acc: 0.8625 - 3ms/step\n",
      "step  70/196 - loss: 0.4264 - acc: 0.8670 - 3ms/step\n",
      "step  80/196 - loss: 0.4237 - acc: 0.8668 - 3ms/step\n",
      "step  90/196 - loss: 0.5286 - acc: 0.8660 - 3ms/step\n",
      "step 100/196 - loss: 0.3980 - acc: 0.8669 - 3ms/step\n",
      "step 110/196 - loss: 0.4362 - acc: 0.8676 - 3ms/step\n",
      "step 120/196 - loss: 0.3779 - acc: 0.8638 - 3ms/step\n",
      "step 130/196 - loss: 0.4090 - acc: 0.8644 - 3ms/step\n",
      "step 140/196 - loss: 0.4323 - acc: 0.8652 - 3ms/step\n",
      "step 150/196 - loss: 0.4067 - acc: 0.8654 - 3ms/step\n",
      "step 160/196 - loss: 0.5107 - acc: 0.8637 - 3ms/step\n",
      "step 170/196 - loss: 0.4058 - acc: 0.8649 - 3ms/step\n",
      "step 180/196 - loss: 0.3519 - acc: 0.8663 - 3ms/step\n",
      "step 190/196 - loss: 0.4454 - acc: 0.8663 - 3ms/step\n",
      "step 196/196 - loss: 0.4457 - acc: 0.8672 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 7/10\n",
      "step  10/586 - loss: 0.4362 - acc: 0.9437 - 7ms/step\n",
      "step  20/586 - loss: 0.3194 - acc: 0.9469 - 7ms/step\n",
      "step  30/586 - loss: 0.4111 - acc: 0.9510 - 7ms/step\n",
      "step  40/586 - loss: 0.3341 - acc: 0.9531 - 7ms/step\n",
      "step  50/586 - loss: 0.3775 - acc: 0.9563 - 7ms/step\n",
      "step  60/586 - loss: 0.3455 - acc: 0.9578 - 7ms/step\n",
      "step  70/586 - loss: 0.3955 - acc: 0.9563 - 7ms/step\n",
      "step  80/586 - loss: 0.3743 - acc: 0.9586 - 7ms/step\n",
      "step  90/586 - loss: 0.3200 - acc: 0.9587 - 7ms/step\n",
      "step 100/586 - loss: 0.3480 - acc: 0.9578 - 7ms/step\n",
      "step 110/586 - loss: 0.3540 - acc: 0.9594 - 7ms/step\n",
      "step 120/586 - loss: 0.3137 - acc: 0.9609 - 7ms/step\n",
      "step 130/586 - loss: 0.3789 - acc: 0.9606 - 7ms/step\n",
      "step 140/586 - loss: 0.3223 - acc: 0.9603 - 7ms/step\n",
      "step 150/586 - loss: 0.3147 - acc: 0.9608 - 7ms/step\n",
      "step 160/586 - loss: 0.3199 - acc: 0.9617 - 7ms/step\n",
      "step 170/586 - loss: 0.3418 - acc: 0.9625 - 7ms/step\n",
      "step 180/586 - loss: 0.3225 - acc: 0.9634 - 8ms/step\n",
      "step 190/586 - loss: 0.3235 - acc: 0.9645 - 8ms/step\n",
      "step 200/586 - loss: 0.3151 - acc: 0.9655 - 8ms/step\n",
      "step 210/586 - loss: 0.3149 - acc: 0.9658 - 8ms/step\n",
      "step 220/586 - loss: 0.3457 - acc: 0.9659 - 8ms/step\n",
      "step 230/586 - loss: 0.3459 - acc: 0.9662 - 8ms/step\n",
      "step 240/586 - loss: 0.3166 - acc: 0.9664 - 8ms/step\n",
      "step 250/586 - loss: 0.3819 - acc: 0.9661 - 8ms/step\n",
      "step 260/586 - loss: 0.3473 - acc: 0.9660 - 8ms/step\n",
      "step 270/586 - loss: 0.3214 - acc: 0.9661 - 8ms/step\n",
      "step 280/586 - loss: 0.4032 - acc: 0.9660 - 8ms/step\n",
      "step 290/586 - loss: 0.3486 - acc: 0.9659 - 8ms/step\n",
      "step 300/586 - loss: 0.3309 - acc: 0.9663 - 8ms/step\n",
      "step 310/586 - loss: 0.3581 - acc: 0.9664 - 7ms/step\n",
      "step 320/586 - loss: 0.4081 - acc: 0.9657 - 7ms/step\n",
      "step 330/586 - loss: 0.3550 - acc: 0.9653 - 7ms/step\n",
      "step 340/586 - loss: 0.3379 - acc: 0.9657 - 7ms/step\n",
      "step 350/586 - loss: 0.3423 - acc: 0.9652 - 7ms/step\n",
      "step 360/586 - loss: 0.3774 - acc: 0.9649 - 7ms/step\n",
      "step 370/586 - loss: 0.3143 - acc: 0.9651 - 7ms/step\n",
      "step 380/586 - loss: 0.3399 - acc: 0.9651 - 7ms/step\n",
      "step 390/586 - loss: 0.3416 - acc: 0.9655 - 7ms/step\n",
      "step 400/586 - loss: 0.3877 - acc: 0.9652 - 7ms/step\n",
      "step 410/586 - loss: 0.4009 - acc: 0.9649 - 7ms/step\n",
      "step 420/586 - loss: 0.3149 - acc: 0.9647 - 7ms/step\n",
      "step 430/586 - loss: 0.3817 - acc: 0.9646 - 7ms/step\n",
      "step 440/586 - loss: 0.3468 - acc: 0.9649 - 7ms/step\n",
      "step 450/586 - loss: 0.3474 - acc: 0.9650 - 7ms/step\n",
      "step 460/586 - loss: 0.3547 - acc: 0.9649 - 7ms/step\n",
      "step 470/586 - loss: 0.3495 - acc: 0.9651 - 7ms/step\n",
      "step 480/586 - loss: 0.3674 - acc: 0.9647 - 7ms/step\n",
      "step 490/586 - loss: 0.3634 - acc: 0.9647 - 7ms/step\n",
      "step 500/586 - loss: 0.3542 - acc: 0.9647 - 7ms/step\n",
      "step 510/586 - loss: 0.3150 - acc: 0.9650 - 7ms/step\n",
      "step 520/586 - loss: 0.3141 - acc: 0.9652 - 7ms/step\n",
      "step 530/586 - loss: 0.3235 - acc: 0.9652 - 7ms/step\n",
      "step 540/586 - loss: 0.3867 - acc: 0.9653 - 7ms/step\n",
      "step 550/586 - loss: 0.3493 - acc: 0.9655 - 7ms/step\n",
      "step 560/586 - loss: 0.4191 - acc: 0.9656 - 7ms/step\n",
      "step 570/586 - loss: 0.3169 - acc: 0.9650 - 7ms/step\n",
      "step 580/586 - loss: 0.3171 - acc: 0.9649 - 7ms/step\n",
      "step 586/586 - loss: 0.3298 - acc: 0.9648 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4102 - acc: 0.8781 - 3ms/step\n",
      "step  20/196 - loss: 0.3831 - acc: 0.8906 - 3ms/step\n",
      "step  30/196 - loss: 0.5540 - acc: 0.8802 - 3ms/step\n",
      "step  40/196 - loss: 0.5060 - acc: 0.8727 - 2ms/step\n",
      "step  50/196 - loss: 0.4351 - acc: 0.8750 - 2ms/step\n",
      "step  60/196 - loss: 0.3830 - acc: 0.8698 - 2ms/step\n",
      "step  70/196 - loss: 0.4603 - acc: 0.8723 - 2ms/step\n",
      "step  80/196 - loss: 0.4188 - acc: 0.8703 - 2ms/step\n",
      "step  90/196 - loss: 0.5685 - acc: 0.8691 - 2ms/step\n",
      "step 100/196 - loss: 0.4086 - acc: 0.8719 - 2ms/step\n",
      "step 110/196 - loss: 0.4628 - acc: 0.8722 - 3ms/step\n",
      "step 120/196 - loss: 0.3791 - acc: 0.8674 - 3ms/step\n",
      "step 130/196 - loss: 0.4087 - acc: 0.8673 - 3ms/step\n",
      "step 140/196 - loss: 0.4109 - acc: 0.8688 - 3ms/step\n",
      "step 150/196 - loss: 0.4144 - acc: 0.8688 - 3ms/step\n",
      "step 160/196 - loss: 0.5291 - acc: 0.8666 - 3ms/step\n",
      "step 170/196 - loss: 0.4071 - acc: 0.8678 - 3ms/step\n",
      "step 180/196 - loss: 0.3402 - acc: 0.8703 - 3ms/step\n",
      "step 190/196 - loss: 0.4466 - acc: 0.8707 - 3ms/step\n",
      "step 196/196 - loss: 0.4286 - acc: 0.8712 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 8/10\n",
      "step  10/586 - loss: 0.3689 - acc: 0.9531 - 7ms/step\n",
      "step  20/586 - loss: 0.3800 - acc: 0.9531 - 7ms/step\n",
      "step  30/586 - loss: 0.3609 - acc: 0.9583 - 7ms/step\n",
      "step  40/586 - loss: 0.3177 - acc: 0.9586 - 7ms/step\n",
      "step  50/586 - loss: 0.4016 - acc: 0.9594 - 7ms/step\n",
      "step  60/586 - loss: 0.3537 - acc: 0.9609 - 8ms/step\n",
      "step  70/586 - loss: 0.3203 - acc: 0.9616 - 7ms/step\n",
      "step  80/586 - loss: 0.4411 - acc: 0.9609 - 7ms/step\n",
      "step  90/586 - loss: 0.3150 - acc: 0.9639 - 7ms/step\n",
      "step 100/586 - loss: 0.3471 - acc: 0.9641 - 7ms/step\n",
      "step 110/586 - loss: 0.3501 - acc: 0.9642 - 7ms/step\n",
      "step 120/586 - loss: 0.3169 - acc: 0.9648 - 7ms/step\n",
      "step 130/586 - loss: 0.3198 - acc: 0.9656 - 7ms/step\n",
      "step 140/586 - loss: 0.3776 - acc: 0.9658 - 7ms/step\n",
      "step 150/586 - loss: 0.3153 - acc: 0.9660 - 7ms/step\n",
      "step 160/586 - loss: 0.3440 - acc: 0.9664 - 7ms/step\n",
      "step 170/586 - loss: 0.3279 - acc: 0.9675 - 7ms/step\n",
      "step 180/586 - loss: 0.3984 - acc: 0.9674 - 7ms/step\n",
      "step 190/586 - loss: 0.3500 - acc: 0.9666 - 7ms/step\n",
      "step 200/586 - loss: 0.3219 - acc: 0.9675 - 7ms/step\n",
      "step 210/586 - loss: 0.3145 - acc: 0.9683 - 7ms/step\n",
      "step 220/586 - loss: 0.3149 - acc: 0.9688 - 7ms/step\n",
      "step 230/586 - loss: 0.3151 - acc: 0.9688 - 7ms/step\n",
      "step 240/586 - loss: 0.3338 - acc: 0.9689 - 7ms/step\n",
      "step 250/586 - loss: 0.3511 - acc: 0.9692 - 7ms/step\n",
      "step 260/586 - loss: 0.3172 - acc: 0.9692 - 7ms/step\n",
      "step 270/586 - loss: 0.3563 - acc: 0.9691 - 7ms/step\n",
      "step 280/586 - loss: 0.3349 - acc: 0.9693 - 7ms/step\n",
      "step 290/586 - loss: 0.3152 - acc: 0.9692 - 7ms/step\n",
      "step 300/586 - loss: 0.3159 - acc: 0.9694 - 7ms/step\n",
      "step 310/586 - loss: 0.3461 - acc: 0.9695 - 7ms/step\n",
      "step 320/586 - loss: 0.3141 - acc: 0.9696 - 7ms/step\n",
      "step 330/586 - loss: 0.3141 - acc: 0.9694 - 7ms/step\n",
      "step 340/586 - loss: 0.3457 - acc: 0.9698 - 7ms/step\n",
      "step 350/586 - loss: 0.4385 - acc: 0.9690 - 7ms/step\n",
      "step 360/586 - loss: 0.3157 - acc: 0.9684 - 7ms/step\n",
      "step 370/586 - loss: 0.3917 - acc: 0.9679 - 7ms/step\n",
      "step 380/586 - loss: 0.3459 - acc: 0.9679 - 7ms/step\n",
      "step 390/586 - loss: 0.3501 - acc: 0.9683 - 7ms/step\n",
      "step 400/586 - loss: 0.3754 - acc: 0.9684 - 7ms/step\n",
      "step 410/586 - loss: 0.3550 - acc: 0.9685 - 7ms/step\n",
      "step 420/586 - loss: 0.3843 - acc: 0.9689 - 7ms/step\n",
      "step 430/586 - loss: 0.3151 - acc: 0.9691 - 7ms/step\n",
      "step 440/586 - loss: 0.3450 - acc: 0.9692 - 7ms/step\n",
      "step 450/586 - loss: 0.3548 - acc: 0.9696 - 7ms/step\n",
      "step 460/586 - loss: 0.3773 - acc: 0.9695 - 7ms/step\n",
      "step 470/586 - loss: 0.3140 - acc: 0.9694 - 7ms/step\n",
      "step 480/586 - loss: 0.3399 - acc: 0.9692 - 7ms/step\n",
      "step 490/586 - loss: 0.3873 - acc: 0.9690 - 7ms/step\n",
      "step 500/586 - loss: 0.3411 - acc: 0.9691 - 7ms/step\n",
      "step 510/586 - loss: 0.3454 - acc: 0.9691 - 7ms/step\n",
      "step 520/586 - loss: 0.3436 - acc: 0.9690 - 7ms/step\n",
      "step 530/586 - loss: 0.3766 - acc: 0.9688 - 7ms/step\n",
      "step 540/586 - loss: 0.3471 - acc: 0.9685 - 7ms/step\n",
      "step 550/586 - loss: 0.3464 - acc: 0.9688 - 7ms/step\n",
      "step 560/586 - loss: 0.3193 - acc: 0.9689 - 7ms/step\n",
      "step 570/586 - loss: 0.3206 - acc: 0.9688 - 7ms/step\n",
      "step 580/586 - loss: 0.4221 - acc: 0.9688 - 7ms/step\n",
      "step 586/586 - loss: 0.4137 - acc: 0.9687 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4021 - acc: 0.8750 - 3ms/step\n",
      "step  20/196 - loss: 0.3832 - acc: 0.8938 - 3ms/step\n",
      "step  30/196 - loss: 0.5667 - acc: 0.8802 - 3ms/step\n",
      "step  40/196 - loss: 0.4844 - acc: 0.8727 - 3ms/step\n",
      "step  50/196 - loss: 0.4485 - acc: 0.8712 - 3ms/step\n",
      "step  60/196 - loss: 0.3879 - acc: 0.8661 - 3ms/step\n",
      "step  70/196 - loss: 0.4494 - acc: 0.8719 - 3ms/step\n",
      "step  80/196 - loss: 0.4179 - acc: 0.8707 - 3ms/step\n",
      "step  90/196 - loss: 0.5252 - acc: 0.8712 - 3ms/step\n",
      "step 100/196 - loss: 0.3908 - acc: 0.8728 - 3ms/step\n",
      "step 110/196 - loss: 0.4374 - acc: 0.8730 - 2ms/step\n",
      "step 120/196 - loss: 0.3779 - acc: 0.8685 - 2ms/step\n",
      "step 130/196 - loss: 0.4083 - acc: 0.8680 - 2ms/step\n",
      "step 140/196 - loss: 0.4196 - acc: 0.8688 - 2ms/step\n",
      "step 150/196 - loss: 0.3966 - acc: 0.8683 - 2ms/step\n",
      "step 160/196 - loss: 0.5057 - acc: 0.8670 - 2ms/step\n",
      "step 170/196 - loss: 0.3764 - acc: 0.8676 - 2ms/step\n",
      "step 180/196 - loss: 0.3452 - acc: 0.8693 - 2ms/step\n",
      "step 190/196 - loss: 0.4252 - acc: 0.8689 - 2ms/step\n",
      "step 196/196 - loss: 0.4172 - acc: 0.8696 - 2ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 9/10\n",
      "step  10/586 - loss: 0.3192 - acc: 0.9875 - 7ms/step\n",
      "step  20/586 - loss: 0.3457 - acc: 0.9844 - 8ms/step\n",
      "step  30/586 - loss: 0.3765 - acc: 0.9771 - 7ms/step\n",
      "step  40/586 - loss: 0.3740 - acc: 0.9680 - 7ms/step\n",
      "step  50/586 - loss: 0.3542 - acc: 0.9656 - 7ms/step\n",
      "step  60/586 - loss: 0.3400 - acc: 0.9625 - 7ms/step\n",
      "step  70/586 - loss: 0.3535 - acc: 0.9625 - 7ms/step\n",
      "step  80/586 - loss: 0.3456 - acc: 0.9645 - 7ms/step\n",
      "step  90/586 - loss: 0.3141 - acc: 0.9663 - 7ms/step\n",
      "step 100/586 - loss: 0.3465 - acc: 0.9650 - 7ms/step\n",
      "step 110/586 - loss: 0.3315 - acc: 0.9645 - 7ms/step\n",
      "step 120/586 - loss: 0.3145 - acc: 0.9659 - 7ms/step\n",
      "step 130/586 - loss: 0.3475 - acc: 0.9668 - 7ms/step\n",
      "step 140/586 - loss: 0.3171 - acc: 0.9683 - 7ms/step\n",
      "step 150/586 - loss: 0.3462 - acc: 0.9681 - 7ms/step\n",
      "step 160/586 - loss: 0.3492 - acc: 0.9682 - 7ms/step\n",
      "step 170/586 - loss: 0.3475 - acc: 0.9689 - 7ms/step\n",
      "step 180/586 - loss: 0.3466 - acc: 0.9694 - 7ms/step\n",
      "step 190/586 - loss: 0.4103 - acc: 0.9696 - 7ms/step\n",
      "step 200/586 - loss: 0.3672 - acc: 0.9700 - 7ms/step\n",
      "step 210/586 - loss: 0.4100 - acc: 0.9695 - 7ms/step\n",
      "step 220/586 - loss: 0.4084 - acc: 0.9699 - 7ms/step\n",
      "step 230/586 - loss: 0.3141 - acc: 0.9707 - 7ms/step\n",
      "step 240/586 - loss: 0.3450 - acc: 0.9708 - 7ms/step\n",
      "step 250/586 - loss: 0.3462 - acc: 0.9705 - 7ms/step\n",
      "step 260/586 - loss: 0.3178 - acc: 0.9706 - 7ms/step\n",
      "step 270/586 - loss: 0.3451 - acc: 0.9703 - 7ms/step\n",
      "step 280/586 - loss: 0.3493 - acc: 0.9705 - 7ms/step\n",
      "step 290/586 - loss: 0.3174 - acc: 0.9711 - 7ms/step\n",
      "step 300/586 - loss: 0.3171 - acc: 0.9716 - 7ms/step\n",
      "step 310/586 - loss: 0.3478 - acc: 0.9720 - 7ms/step\n",
      "step 320/586 - loss: 0.3220 - acc: 0.9723 - 7ms/step\n",
      "step 330/586 - loss: 0.3139 - acc: 0.9724 - 7ms/step\n",
      "step 340/586 - loss: 0.3137 - acc: 0.9730 - 7ms/step\n",
      "step 350/586 - loss: 0.4082 - acc: 0.9728 - 7ms/step\n",
      "step 360/586 - loss: 0.3447 - acc: 0.9727 - 7ms/step\n",
      "step 370/586 - loss: 0.3136 - acc: 0.9728 - 7ms/step\n",
      "step 380/586 - loss: 0.3284 - acc: 0.9728 - 7ms/step\n",
      "step 390/586 - loss: 0.4076 - acc: 0.9726 - 7ms/step\n",
      "step 400/586 - loss: 0.3646 - acc: 0.9726 - 7ms/step\n",
      "step 410/586 - loss: 0.3137 - acc: 0.9723 - 7ms/step\n",
      "step 420/586 - loss: 0.3452 - acc: 0.9724 - 7ms/step\n",
      "step 430/586 - loss: 0.3210 - acc: 0.9720 - 7ms/step\n",
      "step 440/586 - loss: 0.3764 - acc: 0.9719 - 7ms/step\n",
      "step 450/586 - loss: 0.3449 - acc: 0.9721 - 7ms/step\n",
      "step 460/586 - loss: 0.3808 - acc: 0.9724 - 7ms/step\n",
      "step 470/586 - loss: 0.3767 - acc: 0.9723 - 7ms/step\n",
      "step 480/586 - loss: 0.3582 - acc: 0.9720 - 7ms/step\n",
      "step 490/586 - loss: 0.4074 - acc: 0.9721 - 7ms/step\n",
      "step 500/586 - loss: 0.3281 - acc: 0.9724 - 7ms/step\n",
      "step 510/586 - loss: 0.3197 - acc: 0.9725 - 7ms/step\n",
      "step 520/586 - loss: 0.3449 - acc: 0.9725 - 7ms/step\n",
      "step 530/586 - loss: 0.3772 - acc: 0.9723 - 7ms/step\n",
      "step 540/586 - loss: 0.3460 - acc: 0.9723 - 7ms/step\n",
      "step 550/586 - loss: 0.3758 - acc: 0.9719 - 7ms/step\n",
      "step 560/586 - loss: 0.3837 - acc: 0.9720 - 7ms/step\n",
      "step 570/586 - loss: 0.3185 - acc: 0.9718 - 7ms/step\n",
      "step 580/586 - loss: 0.3173 - acc: 0.9720 - 7ms/step\n",
      "step 586/586 - loss: 0.3142 - acc: 0.9721 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4118 - acc: 0.8562 - 3ms/step\n",
      "step  20/196 - loss: 0.4136 - acc: 0.8688 - 3ms/step\n",
      "step  30/196 - loss: 0.5431 - acc: 0.8729 - 3ms/step\n",
      "step  40/196 - loss: 0.4878 - acc: 0.8641 - 2ms/step\n",
      "step  50/196 - loss: 0.4139 - acc: 0.8675 - 2ms/step\n",
      "step  60/196 - loss: 0.3872 - acc: 0.8646 - 2ms/step\n",
      "step  70/196 - loss: 0.4269 - acc: 0.8692 - 2ms/step\n",
      "step  80/196 - loss: 0.4665 - acc: 0.8668 - 2ms/step\n",
      "step  90/196 - loss: 0.5964 - acc: 0.8670 - 2ms/step\n",
      "step 100/196 - loss: 0.4225 - acc: 0.8709 - 2ms/step\n",
      "step 110/196 - loss: 0.4720 - acc: 0.8696 - 2ms/step\n",
      "step 120/196 - loss: 0.3814 - acc: 0.8635 - 2ms/step\n",
      "step 130/196 - loss: 0.4242 - acc: 0.8635 - 3ms/step\n",
      "step 140/196 - loss: 0.3902 - acc: 0.8661 - 3ms/step\n",
      "step 150/196 - loss: 0.4303 - acc: 0.8648 - 3ms/step\n",
      "step 160/196 - loss: 0.5004 - acc: 0.8633 - 3ms/step\n",
      "step 170/196 - loss: 0.4446 - acc: 0.8632 - 3ms/step\n",
      "step 180/196 - loss: 0.3417 - acc: 0.8656 - 3ms/step\n",
      "step 190/196 - loss: 0.4667 - acc: 0.8660 - 3ms/step\n",
      "step 196/196 - loss: 0.4134 - acc: 0.8664 - 3ms/step\n",
      "Eval samples: 6250\n",
      "Epoch 10/10\n",
      "step  10/586 - loss: 0.3144 - acc: 0.9781 - 7ms/step\n",
      "step  20/586 - loss: 0.3819 - acc: 0.9719 - 7ms/step\n",
      "step  30/586 - loss: 0.3147 - acc: 0.9698 - 7ms/step\n",
      "step  40/586 - loss: 0.3139 - acc: 0.9727 - 7ms/step\n",
      "step  50/586 - loss: 0.3788 - acc: 0.9738 - 7ms/step\n",
      "step  60/586 - loss: 0.3472 - acc: 0.9724 - 7ms/step\n",
      "step  70/586 - loss: 0.3139 - acc: 0.9714 - 7ms/step\n",
      "step  80/586 - loss: 0.3453 - acc: 0.9727 - 7ms/step\n",
      "step  90/586 - loss: 0.3769 - acc: 0.9729 - 7ms/step\n",
      "step 100/586 - loss: 0.3460 - acc: 0.9734 - 7ms/step\n",
      "step 110/586 - loss: 0.3137 - acc: 0.9727 - 7ms/step\n",
      "step 120/586 - loss: 0.3137 - acc: 0.9721 - 7ms/step\n",
      "step 130/586 - loss: 0.3458 - acc: 0.9724 - 7ms/step\n",
      "step 140/586 - loss: 0.3453 - acc: 0.9732 - 7ms/step\n",
      "step 150/586 - loss: 0.3457 - acc: 0.9729 - 7ms/step\n",
      "step 160/586 - loss: 0.3145 - acc: 0.9740 - 7ms/step\n",
      "step 170/586 - loss: 0.3614 - acc: 0.9732 - 7ms/step\n",
      "step 180/586 - loss: 0.3550 - acc: 0.9731 - 7ms/step\n",
      "step 190/586 - loss: 0.3135 - acc: 0.9735 - 7ms/step\n",
      "step 200/586 - loss: 0.3638 - acc: 0.9739 - 7ms/step\n",
      "step 210/586 - loss: 0.3447 - acc: 0.9737 - 7ms/step\n",
      "step 220/586 - loss: 0.3136 - acc: 0.9734 - 7ms/step\n",
      "step 230/586 - loss: 0.3480 - acc: 0.9735 - 7ms/step\n",
      "step 240/586 - loss: 0.3144 - acc: 0.9734 - 7ms/step\n",
      "step 250/586 - loss: 0.3147 - acc: 0.9740 - 7ms/step\n",
      "step 260/586 - loss: 0.3135 - acc: 0.9742 - 7ms/step\n",
      "step 270/586 - loss: 0.3768 - acc: 0.9748 - 7ms/step\n",
      "step 280/586 - loss: 0.3455 - acc: 0.9749 - 7ms/step\n",
      "step 290/586 - loss: 0.3147 - acc: 0.9748 - 7ms/step\n",
      "step 300/586 - loss: 0.3765 - acc: 0.9745 - 7ms/step\n",
      "step 310/586 - loss: 0.3761 - acc: 0.9742 - 7ms/step\n",
      "step 320/586 - loss: 0.3487 - acc: 0.9739 - 7ms/step\n",
      "step 330/586 - loss: 0.3621 - acc: 0.9739 - 7ms/step\n",
      "step 340/586 - loss: 0.3145 - acc: 0.9738 - 7ms/step\n",
      "step 350/586 - loss: 0.3135 - acc: 0.9738 - 7ms/step\n",
      "step 360/586 - loss: 0.3454 - acc: 0.9740 - 7ms/step\n",
      "step 370/586 - loss: 0.3145 - acc: 0.9744 - 7ms/step\n",
      "step 380/586 - loss: 0.3454 - acc: 0.9745 - 7ms/step\n",
      "step 390/586 - loss: 0.3462 - acc: 0.9747 - 7ms/step\n",
      "step 400/586 - loss: 0.3152 - acc: 0.9750 - 7ms/step\n",
      "step 410/586 - loss: 0.3473 - acc: 0.9753 - 7ms/step\n",
      "step 420/586 - loss: 0.3449 - acc: 0.9754 - 7ms/step\n",
      "step 430/586 - loss: 0.3154 - acc: 0.9757 - 7ms/step\n",
      "step 440/586 - loss: 0.3457 - acc: 0.9759 - 7ms/step\n",
      "step 450/586 - loss: 0.3457 - acc: 0.9757 - 7ms/step\n",
      "step 460/586 - loss: 0.3447 - acc: 0.9757 - 7ms/step\n",
      "step 470/586 - loss: 0.3137 - acc: 0.9757 - 7ms/step\n",
      "step 480/586 - loss: 0.3139 - acc: 0.9759 - 7ms/step\n",
      "step 490/586 - loss: 0.3473 - acc: 0.9760 - 7ms/step\n",
      "step 500/586 - loss: 0.3155 - acc: 0.9759 - 7ms/step\n",
      "step 510/586 - loss: 0.3760 - acc: 0.9757 - 7ms/step\n",
      "step 520/586 - loss: 0.3452 - acc: 0.9755 - 7ms/step\n",
      "step 530/586 - loss: 0.3139 - acc: 0.9756 - 7ms/step\n",
      "step 540/586 - loss: 0.3139 - acc: 0.9756 - 7ms/step\n",
      "step 550/586 - loss: 0.3143 - acc: 0.9757 - 7ms/step\n",
      "step 560/586 - loss: 0.3144 - acc: 0.9759 - 7ms/step\n",
      "step 570/586 - loss: 0.3450 - acc: 0.9759 - 7ms/step\n",
      "step 580/586 - loss: 0.3245 - acc: 0.9758 - 7ms/step\n",
      "step 586/586 - loss: 0.3829 - acc: 0.9756 - 7ms/step\n",
      "Eval begin...\n",
      "step  10/196 - loss: 0.4100 - acc: 0.8531 - 5ms/step\n",
      "step  20/196 - loss: 0.4061 - acc: 0.8703 - 4ms/step\n",
      "step  30/196 - loss: 0.5566 - acc: 0.8719 - 3ms/step\n",
      "step  40/196 - loss: 0.4805 - acc: 0.8656 - 3ms/step\n",
      "step  50/196 - loss: 0.4235 - acc: 0.8662 - 3ms/step\n",
      "step  60/196 - loss: 0.4023 - acc: 0.8620 - 3ms/step\n",
      "step  70/196 - loss: 0.4327 - acc: 0.8656 - 3ms/step\n",
      "step  80/196 - loss: 0.4856 - acc: 0.8625 - 3ms/step\n",
      "step  90/196 - loss: 0.5713 - acc: 0.8639 - 3ms/step\n",
      "step 100/196 - loss: 0.3963 - acc: 0.8678 - 3ms/step\n",
      "step 110/196 - loss: 0.4678 - acc: 0.8676 - 3ms/step\n",
      "step 120/196 - loss: 0.4025 - acc: 0.8625 - 3ms/step\n",
      "step 130/196 - loss: 0.4336 - acc: 0.8627 - 3ms/step\n",
      "step 140/196 - loss: 0.3946 - acc: 0.8652 - 3ms/step\n",
      "step 150/196 - loss: 0.4038 - acc: 0.8646 - 3ms/step\n",
      "step 160/196 - loss: 0.5087 - acc: 0.8633 - 3ms/step\n",
      "step 170/196 - loss: 0.4656 - acc: 0.8638 - 3ms/step\n",
      "step 180/196 - loss: 0.3433 - acc: 0.8660 - 3ms/step\n",
      "step 190/196 - loss: 0.4656 - acc: 0.8663 - 3ms/step\n",
      "step 196/196 - loss: 0.4132 - acc: 0.8672 - 3ms/step\n",
      "Eval samples: 6250\n"
     ]
    }
   ],
   "source": [
    "class DataReader(Dataset):\r\n",
    "    def __init__(self, input, label, length):\r\n",
    "        self.data = list(vectorizer(input, label, length=length))\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.data[idx]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "\r\n",
    "# 指定训练设备\r\n",
    "device = pd.set_device('gpu')  # 可选：cpu\r\n",
    "\r\n",
    "# 开启动态图模式\r\n",
    "pd.disable_static(device)\r\n",
    "\r\n",
    "# 定义输入格式\r\n",
    "input_form = pd.static.InputSpec(shape=[None, length], dtype='int64', name='input')\r\n",
    "label_form = pd.static.InputSpec(shape=[None, 1], dtype='int64', name='label')\r\n",
    "\r\n",
    "model = pd.Model(sim_model, input_form, label_form)\r\n",
    "model.prepare(optimizer=pd.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()),\r\n",
    "              loss=pd.nn.loss.CrossEntropyLoss(),\r\n",
    "              metrics=pd.metric.Accuracy())\r\n",
    "\r\n",
    "# 分割训练集和验证集\r\n",
    "eval_length = int(len(train_x) * 1/4)\r\n",
    "model.fit(train_data=DataReader(train_x[:-eval_length], train_y[:-eval_length], length),\r\n",
    "          eval_data=DataReader(train_x[-eval_length:], train_y[-eval_length:], length),\r\n",
    "          batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 评估效果并用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "step  10/782 - loss: 0.4515 - acc: 0.8531 - 3ms/step\n",
      "step  20/782 - loss: 0.5053 - acc: 0.8656 - 3ms/step\n",
      "step  30/782 - loss: 0.4896 - acc: 0.8406 - 3ms/step\n",
      "step  40/782 - loss: 0.3849 - acc: 0.8469 - 3ms/step\n",
      "step  50/782 - loss: 0.5705 - acc: 0.8331 - 3ms/step\n",
      "step  60/782 - loss: 0.3480 - acc: 0.8370 - 3ms/step\n",
      "step  70/782 - loss: 0.3403 - acc: 0.8460 - 3ms/step\n",
      "step  80/782 - loss: 0.3370 - acc: 0.8473 - 3ms/step\n",
      "step  90/782 - loss: 0.5180 - acc: 0.8462 - 3ms/step\n",
      "step 100/782 - loss: 0.4266 - acc: 0.8481 - 3ms/step\n",
      "step 110/782 - loss: 0.4605 - acc: 0.8486 - 3ms/step\n",
      "step 120/782 - loss: 0.3836 - acc: 0.8477 - 3ms/step\n",
      "step 130/782 - loss: 0.4657 - acc: 0.8474 - 3ms/step\n",
      "step 140/782 - loss: 0.4203 - acc: 0.8462 - 3ms/step\n",
      "step 150/782 - loss: 0.4735 - acc: 0.8408 - 3ms/step\n",
      "step 160/782 - loss: 0.4959 - acc: 0.8412 - 3ms/step\n",
      "step 170/782 - loss: 0.3490 - acc: 0.8419 - 3ms/step\n",
      "step 180/782 - loss: 0.6037 - acc: 0.8415 - 3ms/step\n",
      "step 190/782 - loss: 0.4110 - acc: 0.8416 - 3ms/step\n",
      "step 200/782 - loss: 0.5318 - acc: 0.8430 - 3ms/step\n",
      "step 210/782 - loss: 0.4332 - acc: 0.8449 - 3ms/step\n",
      "step 220/782 - loss: 0.6212 - acc: 0.8447 - 3ms/step\n",
      "step 230/782 - loss: 0.4884 - acc: 0.8443 - 3ms/step\n",
      "step 240/782 - loss: 0.3646 - acc: 0.8434 - 3ms/step\n",
      "step 250/782 - loss: 0.4735 - acc: 0.8446 - 3ms/step\n",
      "step 260/782 - loss: 0.4272 - acc: 0.8460 - 3ms/step\n",
      "step 270/782 - loss: 0.5258 - acc: 0.8453 - 3ms/step\n",
      "step 280/782 - loss: 0.4614 - acc: 0.8449 - 3ms/step\n",
      "step 290/782 - loss: 0.4773 - acc: 0.8454 - 3ms/step\n",
      "step 300/782 - loss: 0.5187 - acc: 0.8441 - 3ms/step\n",
      "step 310/782 - loss: 0.4952 - acc: 0.8431 - 3ms/step\n",
      "step 320/782 - loss: 0.3959 - acc: 0.8435 - 3ms/step\n",
      "step 330/782 - loss: 0.4840 - acc: 0.8437 - 3ms/step\n",
      "step 340/782 - loss: 0.3650 - acc: 0.8441 - 3ms/step\n",
      "step 350/782 - loss: 0.4842 - acc: 0.8450 - 3ms/step\n",
      "step 360/782 - loss: 0.4866 - acc: 0.8444 - 3ms/step\n",
      "step 370/782 - loss: 0.4882 - acc: 0.8454 - 3ms/step\n",
      "step 380/782 - loss: 0.4428 - acc: 0.8434 - 3ms/step\n",
      "step 390/782 - loss: 0.4084 - acc: 0.8430 - 3ms/step\n",
      "step 400/782 - loss: 0.4584 - acc: 0.8433 - 3ms/step\n",
      "step 410/782 - loss: 0.5239 - acc: 0.8442 - 3ms/step\n",
      "step 420/782 - loss: 0.4221 - acc: 0.8453 - 3ms/step\n",
      "step 430/782 - loss: 0.3200 - acc: 0.8466 - 3ms/step\n",
      "step 440/782 - loss: 0.3503 - acc: 0.8479 - 3ms/step\n",
      "step 450/782 - loss: 0.4750 - acc: 0.8488 - 3ms/step\n",
      "step 460/782 - loss: 0.4753 - acc: 0.8505 - 3ms/step\n",
      "step 470/782 - loss: 0.5096 - acc: 0.8504 - 3ms/step\n",
      "step 480/782 - loss: 0.4834 - acc: 0.8513 - 3ms/step\n",
      "step 490/782 - loss: 0.3860 - acc: 0.8527 - 3ms/step\n",
      "step 500/782 - loss: 0.5332 - acc: 0.8533 - 3ms/step\n",
      "step 510/782 - loss: 0.4014 - acc: 0.8533 - 3ms/step\n",
      "step 520/782 - loss: 0.4066 - acc: 0.8547 - 3ms/step\n",
      "step 530/782 - loss: 0.4554 - acc: 0.8557 - 3ms/step\n",
      "step 540/782 - loss: 0.5141 - acc: 0.8560 - 3ms/step\n",
      "step 550/782 - loss: 0.4621 - acc: 0.8568 - 3ms/step\n",
      "step 560/782 - loss: 0.4383 - acc: 0.8576 - 3ms/step\n",
      "step 570/782 - loss: 0.3677 - acc: 0.8584 - 3ms/step\n",
      "step 580/782 - loss: 0.5716 - acc: 0.8588 - 3ms/step\n",
      "step 590/782 - loss: 0.4613 - acc: 0.8596 - 3ms/step\n",
      "step 600/782 - loss: 0.4694 - acc: 0.8602 - 3ms/step\n",
      "step 610/782 - loss: 0.3561 - acc: 0.8609 - 3ms/step\n",
      "step 620/782 - loss: 0.4349 - acc: 0.8608 - 3ms/step\n",
      "step 630/782 - loss: 0.4117 - acc: 0.8618 - 3ms/step\n",
      "step 640/782 - loss: 0.3703 - acc: 0.8621 - 3ms/step\n",
      "step 650/782 - loss: 0.3898 - acc: 0.8623 - 3ms/step\n",
      "step 660/782 - loss: 0.4767 - acc: 0.8625 - 3ms/step\n",
      "step 670/782 - loss: 0.4580 - acc: 0.8626 - 3ms/step\n",
      "step 680/782 - loss: 0.4189 - acc: 0.8622 - 3ms/step\n",
      "step 690/782 - loss: 0.4569 - acc: 0.8622 - 3ms/step\n",
      "step 700/782 - loss: 0.3807 - acc: 0.8627 - 3ms/step\n",
      "step 710/782 - loss: 0.4707 - acc: 0.8632 - 3ms/step\n",
      "step 720/782 - loss: 0.3709 - acc: 0.8633 - 3ms/step\n",
      "step 730/782 - loss: 0.4519 - acc: 0.8643 - 3ms/step\n",
      "step 740/782 - loss: 0.4227 - acc: 0.8651 - 3ms/step\n",
      "step 750/782 - loss: 0.4386 - acc: 0.8651 - 3ms/step\n",
      "step 760/782 - loss: 0.3844 - acc: 0.8653 - 3ms/step\n",
      "step 770/782 - loss: 0.3988 - acc: 0.8657 - 3ms/step\n",
      "step 780/782 - loss: 0.3374 - acc: 0.8662 - 3ms/step\n",
      "step 782/782 - loss: 0.4368 - acc: 0.8664 - 3ms/step\n",
      "Eval samples: 25000\n",
      "Predict begin...\n",
      "step 10/10 [==============================] - 2ms/step        \n",
      "Predict samples: 10\n",
      "预测的标签是：0, 实际标签是：0\n",
      "预测的标签是：0, 实际标签是：0\n",
      "预测的标签是：0, 实际标签是：0\n",
      "预测的标签是：0, 实际标签是：0\n",
      "预测的标签是：0, 实际标签是：0\n",
      "预测的标签是：1, 实际标签是：1\n",
      "预测的标签是：1, 实际标签是：1\n",
      "预测的标签是：1, 实际标签是：1\n",
      "预测的标签是：1, 实际标签是：1\n",
      "预测的标签是：1, 实际标签是：1\n"
     ]
    }
   ],
   "source": [
    "# 评估\r\n",
    "model.evaluate(eval_data=DataReader(test_x, test_y, length), batch_size=32)\r\n",
    "\r\n",
    "# 预测\r\n",
    "true_y = test_y[100:105] + test_y[-110:-105]\r\n",
    "pred_y = model.predict(DataReader(test_x[100:105] + test_x[-110:-105], None, length), batch_size=1)\r\n",
    "\r\n",
    "for index, y in enumerate(pred_y[0]):\r\n",
    "    print(\"预测的标签是：%d, 实际标签是：%d\" % (np.argmax(y), true_y[index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
