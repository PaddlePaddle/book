{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **强化学习——Advantage Actor-Critic(A2C)**\n",
    "**作者：**[EastSmith](https://github.com/EastSmith)\n",
    "\n",
    "**日期：** 2021.06 \n",
    "\n",
    "**AI Studio项目**：[点击体验](https://aistudio.baidu.com/aistudio/projectdetail/1766508)\n",
    "## **一、介绍**\n",
    "###  **让我们回顾一下以前的知识， 您可能知道，目前有两种主要的RL方法类型：** \n",
    "\n",
    "* **基于值**：他们试图找到或近似最佳值函数，这是一个动作和一个值之间的映射。 值越高，动作越好。 最著名的算法是Q学习及其所有增强的方法， 例如Deep Q Networks，Double Dueling Q Networks等 \n",
    "* **基于策略的**：基于策略的算法（例如“ 策略梯度” 和REINFORCE）尝试直接找到最佳策略，而无需Q值作为中间步骤。\n",
    "\n",
    "当这两个算法流行以后，下一个显而易见的步骤是……尝试合并它们。 这就是演员——评论家的诞生方式。 演员评论家旨在利用基于价值和基于策略的优点，同时消除其弊端。 以及他们如何做到这一点？\n",
    "\n",
    "###  **主要思想是将模型分为两部分：一个用于基于状态计算动作，另一个用于估计动作的Q值。**\n",
    "\n",
    "演员将状态作为输入并输出最佳动作。 它实质上是通过控制代理的行为来学习最佳策略 （基于策略） 。 另一方面，评论家通过计算值函数评估动作 （基于值）来 。 这两个模型参加了一场比赛，随着时间的流逝，他们各自的角色都变得更好。 结果是，与单独使用两种方法相比，整个体系结构将学会更有效地玩游戏。 \n",
    "\n",
    "让两个模型相互交互（或竞争）的想法在机器学习领域越来越流行。 例如， 生成对抗网络（Generative Adversarial Networks） 或 变体自动编码器（Variational Autoencoders）\n",
    "\n",
    "\n",
    "###  **演员——评论家：**\n",
    "（可以参照的教程：[强化学习——Actor Critic Method-使用文档-PaddlePaddle深度学习平台](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/reinforcement_learning/Actor_Critic_Method/Actor_Critic_Method.html)）\n",
    "\n",
    "演员——评论家的一个很好的比喻是一个小男孩和他的母亲。 这个孩子（演员）不断尝试新事物并探索他周围的环境。 他吃自己的玩具，触摸热烤箱，用头撞在墙上（谁知道他为什么这样做）。 他的母亲（评论家）看着他，并批评或称赞他。 这个孩子听母亲讲给他的话，并调整自己的行为。 随着孩子的成长，他学会了什么动作是坏事还是好事，并且他实质上学会了玩称为生活的游戏。 这与演员评论家的工作方式完全相同。\n",
    "\n",
    "参与者演员可以是类似于**神经网络的函数逼近器**，其任务是针对给定状态产生最佳动作。 当然，它可以是全连接的神经网络，也可以是卷积或其他任何东西。 评论家是另一个函数逼近器，它接收参与者输入的环境和动作作为输入，将它们连接起来并输出评分值（Q值）。 让我提醒您几秒钟，Q值本质上是将来的最大奖励。\n",
    "\n",
    "这两个网络的训练是分别进行的，评论家使用梯度上升（找到全局最大值而不是最小值）来更新它们的权重。 随着时间的流逝，演员正在学会做出更好的动作（他开始学习策略），而评论家在评估这些动作方面也越来越好。 重要的是要注意，权重的更新发生在每个步骤（TD学习），而不是发生在事件的结尾，这与策略梯度相反。\n",
    "\n",
    "事实证明，演员评论家能够学习大型复杂的环境，并且已在很多著名的2d和3d游戏中使用，例如Doom，Super Mario等。\n",
    "\n",
    "###  **优势-演员-评论家 Advantage-Actor-Critic（A2C）**\n",
    "什么是优势？ Q值实际上可以分解为两部分：状态值函数V（s）和优势值A（s，a）：\n",
    "\n",
    "Q（s，a）= V（s）+ A（s，a）\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aa58a55591394f478cfbde40227812e1ab21bc088a7546c09e440b4b353cc761)\n",
    "\n",
    "优势函数能够评估在给定状态下与其他行为相比更好的行为，而众所周知，价值函数是评估在此状态下行为的良好程度。\n",
    "\n",
    "你猜这是怎么回事，对不对？ 与其让评论家学习Q值，不如让评论家学习Advantage值 。 这样，对行为的评估不仅基于行为的良好程度，而且还取决于行为可以改善的程度。 优势函数的优势是它减少了策略网络的数值差异并稳定了模型。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、环境配置**\n",
    "本教程基于Paddle 2.1 编写，如果您的环境不是本版本，请先参考官网[安装](https://www.paddlepaddle.org.cn/install/quick) Paddle 2.1 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import paddle.nn.functional as F\n",
    "from paddle.distribution import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from visualdl import LogWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、实施“优势-演员-评论家 Advantage-Actor-Critic（A2C）”算法**\n",
    "### **构建多个进程玩CartPole-v0**\n",
    "A2C会构建多个进程，包括多个并行的 worker，与独立的环境进行交互，收集独立的经验。详细代码在multiprocessing_env.py里。简单介绍一下创建多环境的过程：env = gym.make(env_name)只能创建一个线程，智能体只能和一个环境进行交互，而使用 SubprocVecEnv(envs)可以创建多个并行的环境，用num_envs定义并行环境的数量。需要注意的是如果创建的是多个并行的环境envs的话，那么envs.step()需要输入的是成组的动作，每个环境对应一组动作，相应的envs返回的next_state, reward等也是成组的。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5d443811448d4f13bf6a16b43bb0339e885b3bfee1b04d23be1665f925877c5d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This code is from openai baseline\n",
    "#https://github.com/openai/baselines/tree/master/baselines/common/vec_env\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x()\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            # ob, reward, done, info = env.step(1)\n",
    "\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class VecEnv(object):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a tuple of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a tuple of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    \n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "        \n",
    "class SubprocVecEnv(VecEnv):\n",
    "    def __init__(self, env_fns, spaces=None):\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.nenvs = nenvs\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "            self.closed = True\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.nenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = LogWriter(logdir=\"./log\") \n",
    "\n",
    "#from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 8\n",
    "env_name = \"CartPole-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "plt.ion()\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs) # 8 env\n",
    "\n",
    "env = gym.make(env_name) # a single env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **定义网络结构并开始训练**\n",
    "self.critic部分定义的是“评论家”，self.actor部分定义的是“演员”。“评论家”网络观察输入并“打分”，“演员”网络接收输入并给出行动的类别分布，这里用到了API——paddle.distribution.Categorical，后续调用sample(shape)生成指定维度的样本、调用entropy()返回类别分布的信息熵、调用log_prob(value)返回所选择类别的对数概率，其他用法可以查看飞桨API文档。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5ecb0c2d202d43e99cdbe378bc7ebb0fbb438e1e9b1b41469cf633e73c5e688b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Layer):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        nn.initializer.set_global_initializer(nn.initializer.XavierNormal(), nn.initializer.Constant(value=0.))\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "            nn.Softmax(axis=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        probs = self.actor(x)\n",
    "        dist  = Categorical(probs)\n",
    "        return dist, value\n",
    "\n",
    "\n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = paddle.to_tensor(state,dtype=\"float32\").unsqueeze(0)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample([1]).cpu().numpy()[0][0])        \n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def plot(frame_idx, rewards):\n",
    "    plt.plot(rewards,'b-')\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **实例化模型和定义优化器**\n",
    "hidden_size是网络的隐藏层的“神经元”数目，lr是优化器的学习率，咱使用经典的Adam优化器。num_steps是收集轨迹的步数，值设置的越大，更新网络前收集的轨迹越长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.n\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size = 256\n",
    "lr          = 1e-3\n",
    "num_steps   = 8\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "optimizer = optim.Adam(parameters=model.parameters(),learning_rate=lr)\n",
    "save_model_path = \"models/A2C_model.pdparams\"\n",
    "if os.path.exists(save_model_path):\n",
    "    model_state_dict  = paddle.load(save_model_path)\n",
    "    model.set_state_dict(model_state_dict )\n",
    "    print(' Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、开始循环训练过程：**\n",
    "收集经验—>计算损失—>反向传播\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 首先定义最大的训练帧数，并行的环境envs每执行一步step()算一帧。如果按照前面定义的\n",
    "# 是8组环境并行，那么envs就需要输入8组动作，同时会输出8组回报（reward）、下一\n",
    "# 观测状态（next_state）。\n",
    "\n",
    "max_frames   = 20000\n",
    "frame_idx    = 0\n",
    "test_rewards = []\n",
    "\n",
    "\n",
    "state = envs.reset()\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    # rollout trajectory\n",
    "    # 现在模型展开num_steps步的轨迹：模型会根据观测状态返回动作的分布、状态价值，然后\n",
    "    # 根据动作分布采样动作，接着环境step一步进入到下一个状态，并返回reward。\n",
    "    for _ in range(num_steps):\n",
    "        state = paddle.to_tensor(state,dtype=\"float32\")\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample([1]).squeeze(0)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(paddle.to_tensor(reward,dtype=\"float32\").unsqueeze(1))\n",
    "        masks.append(paddle.to_tensor(1 - done).unsqueeze(1))\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        Plot = False\n",
    "        # 程序每隔100帧会进行一次评估，评估的方式是运行2次test_env()并计算返回的\n",
    "        # total_reward的均值，这里用VisualDL记录它，文章的最后会展示模型运行效果。\n",
    "        if  frame_idx % 100 == 0:\n",
    "            test_rewards.append(np.mean([test_env() for _ in range(2)]))\n",
    "            writer.add_scalar(\"test_rewards\", value=test_rewards[-1], step=frame_idx)            \n",
    "            if Plot:\n",
    "                plot(frame_idx, test_rewards)\n",
    "            else:\n",
    "                print('frame {}. reward: {}'.format(frame_idx, test_rewards[-1]))\n",
    "\n",
    "    # 程序会记录展开轨迹的动作对数似然概率log_probs、模型估计价值values、回报rewards等，\n",
    "    # 并计算优势值advantage 。由于是多环境并行，可以用paddle.concat将这些值分别拼接起来，\n",
    "    # 随后计算出演员网络的损失actor_loss、评论家网络的损失critic_loss，在最终loss中有一项\n",
    "    # 是动作分布熵的均值，希望能增大网络的探索能力。        \n",
    "    next_state = paddle.to_tensor(next_state,dtype=\"float32\")\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_returns(next_value, rewards, masks)\n",
    "    \n",
    "    log_probs = paddle.concat(log_probs)\n",
    "    returns   = paddle.concat(returns).detach()\n",
    "    values    = paddle.concat(values)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss  = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "    # 用VisualDL记录训练的actor_loss、critic_loss以及合并后的loss。然后再反向传播，优化神\n",
    "    # 经网络的参数，开始下一轮的训练循环。\n",
    "    writer.add_scalar(\"actor_loss\", value=actor_loss, step=frame_idx)\n",
    "    writer.add_scalar(\"critic_loss\", value=critic_loss, step=frame_idx)\n",
    "    writer.add_scalar(\"loss\", value=loss, step=frame_idx)\n",
    "    ##动态学习率，每隔2000帧缩放一次\n",
    "    if frame_idx % 2000 ==0:\n",
    "        lr = 0.92*lr\n",
    "        optimizer.set_lr(lr)    \n",
    "\n",
    "    optimizer.clear_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    " \n",
    "if not os.path.exists(os.path.dirname(save_model_path)):\n",
    "            os.makedirs(os.path.dirname(save_model_path))\n",
    "# paddle.save(model.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **五、VisualDL里展示模型运行的效果**\n",
    "在gym的CartPole环境（env）里面，小车需要左右移动来保持杆子竖直。左移或者右移小车之后，env会返回一个“+1”的reward，如果杠子倾角过大或者小车超范围游戏就结束了。其中，在CartPole-v0环境里reward达到200也会结束游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/d1f795ede3fe434eb4b0e12f5e985a8c76072df13c944188bd20040f0665176c)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **六、总结和建议**\n",
    "* 深度强化学习中，很多基础算法都是单线程的，也就是一个 agent 去跟环境交互产生经验。基础版 Actor-Critic ，由于环境是固定不变的，agent 的动作又是连续的，这样收集到的经验就有很强的时序关联，而且在有限的时间内也只能探索到部分状态和动作空间。\n",
    "\n",
    "* 为了打破经验之间的耦合，可以采用Experiencre Replay的方法，让 agent 能够在后续的训练中访问到以前的历史经验，这就是 DQN 和 DDPG 这类基于值的（DDPG虽然也属于 Actor-Critic 架构，但本质上是 DQN 在连续空间中的扩展）算法所采用的方式。而对于基于策略类的算法，agent 收集的经验都是以 episode为单位的，跑完一个episode 后经验就要丢掉，更好的方式是采用多线程的并行架构，这样既能解决前面的问题，又能高效利用计算资源，提升训练效率。\n",
    "\n",
    "* Advantage Actor-Critic（A2C） 算法引入了并行架构，各个 worker 都会独立的跟自己的环境去交互，得到独立的采样经验，而这些经验之间也是相互独立的，这样就打破了经验之间的耦合，起到跟 Experiencre Replay 相当的效果。因此通常 A2C和A3C 是不需要使用 Replay Buffer 的，这种结构本身就可以替代了。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/88b967da1ba74e049b3ff28dd9083d1e527ba734dc064a798374f99199f84086)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Paddle2.1.1",
   "language": "python",
   "name": "paddle2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
