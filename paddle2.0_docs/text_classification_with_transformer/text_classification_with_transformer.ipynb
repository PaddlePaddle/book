{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 通过Transformer实现文本分类\n",
    "作者：[YinHang2515](https://github.com/YinHang2515)\n",
    "\n",
    "日期：2020年11月20日\n",
    "\n",
    "本示例教程演示如何使用Transformer模型在IMDB数据集上完成文本分类的任务。\n",
    "\n",
    "IMDB数据集是一个对电影评论标注为正向评论与负向评论的数据集，共有25000条文本数据作为训练集，25000条文本数据作为测试集。 该数据集的官方地址为： http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle as pd\n",
    "import paddle.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 处理数据集\n",
    "首先通过paddle内置的dataset完成数据集的导入，并构建字典和相应的reader\n",
    "\n",
    "然后通过padding的方式对同一个batch中长度不一致的数据进行补齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB word dict....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB word dict....\")\r\n",
    "word_dict = pd.dataset.imdb.word_dict()\r\n",
    "\r\n",
    "train_reader = pd.dataset.imdb.train(word_dict)\r\n",
    "test_reader = pd.dataset.imdb.test(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:0\n",
      "and:1\n",
      "a:2\n",
      "of:3\n",
      "to:4\n",
      "...\n",
      "virtual:5143\n",
      "warriors:5144\n",
      "widely:5145\n",
      "<unk>:5146\n",
      "<pad>:5147\n",
      "totally 5148 words\n"
     ]
    }
   ],
   "source": [
    "# 添加<pad>\r\n",
    "word_dict['<pad>'] = len(word_dict)\r\n",
    "\r\n",
    "for k in list(word_dict)[:5]:\r\n",
    "    print(\"{}:{}\".format(k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"...\")\r\n",
    "\r\n",
    "for k in list(word_dict)[-5:]:\r\n",
    "    print(\"{}:{}\".format(k if isinstance(k, str) else k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"totally {} words\".format(len(word_dict)))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_dict)  \r\n",
    "maxlen = 200  \r\n",
    "seq_len = 200\r\n",
    "batch_size = 32\r\n",
    "epochs = 2\r\n",
    "pad_id = word_dict['<pad>']\r\n",
    "embed_dim = 32  # Embedding size for each token\r\n",
    "num_heads = 2  # Number of attention heads\r\n",
    "feed_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
    "\r\n",
    "classes = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200)\n",
      "(25000, 1)\n",
      "(25000, 200)\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "#用padding的方式对齐数据\r\n",
    "def create_padded_dataset(reader):\r\n",
    "    padded_sents = []\r\n",
    "    labels = []\r\n",
    "    for batch_id, data in enumerate(reader):\r\n",
    "        sent, label = data\r\n",
    "        padded_sent = sent[:seq_len] + [pad_id] * (seq_len - len(sent))\r\n",
    "        padded_sents.append(padded_sent)\r\n",
    "        labels.append(label)\r\n",
    "    return np.array(padded_sents), np.expand_dims(np.array(labels), axis=1)\r\n",
    "\r\n",
    "train_sents, train_labels = create_padded_dataset(train_reader())\r\n",
    "test_sents, test_labels = create_padded_dataset(test_reader())\r\n",
    "\r\n",
    "print(train_sents.shape)\r\n",
    "print(train_labels.shape)\r\n",
    "print(test_sents.shape)\r\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 用Dataset 与 DataLoader 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "class IMDBDataset(pd.io.Dataset):\r\n",
    "    def __init__(self, sents, labels):\r\n",
    "\r\n",
    "        self.sents = sents\r\n",
    "        self.labels = labels\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "\r\n",
    "        data = self.sents[index]\r\n",
    "        label = self.labels[index]\r\n",
    "\r\n",
    "        return data, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \r\n",
    "        return len(self.sents)\r\n",
    "    \r\n",
    "train_dataset = IMDBDataset(train_sents, train_labels)\r\n",
    "test_dataset = IMDBDataset(test_sents, test_labels)\r\n",
    "\r\n",
    "train_loader = pd.io.DataLoader(train_dataset, places=pd.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "test_loader = pd.io.DataLoader(test_dataset, places=pd.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义多头自注意力机制 (Multi-head Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads=8):\r\n",
    "        super(MultiHeadSelfAttention, self).__init__()\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        if embed_dim % num_heads != 0:\r\n",
    "            raise ValueError(\r\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\r\n",
    "            )\r\n",
    "        self.projection_dim = embed_dim // num_heads\r\n",
    "        self.query_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.key_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.value_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim)\r\n",
    "\r\n",
    "    def attention(self, query, key, value):\r\n",
    "        score = pd.matmul(query, key, transpose_y=True)\r\n",
    "        dim_key = pd.cast(pd.shape(key)[-1], 'float32')\r\n",
    "        scaled_score = score / pd.sqrt(dim_key)\r\n",
    "        weights = nn.functional.softmax(scaled_score, axis=-1)\r\n",
    "        output = pd.matmul(weights, value)\r\n",
    "        return output, weights\r\n",
    "\r\n",
    "    def separate_heads(self, x, batch_size):\r\n",
    "        x = pd.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\r\n",
    "        return pd.transpose(x, perm=[0, 2, 1, 3])\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\r\n",
    "        batch_size = pd.shape(inputs)[0]\r\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        query = self.separate_heads(\r\n",
    "            query, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        key = self.separate_heads(\r\n",
    "            key, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        value = self.separate_heads(\r\n",
    "            value, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        attention, weights = self.attention(query, key, value)\r\n",
    "        attention = pd.transpose(\r\n",
    "            attention, perm=[0, 2, 1, 3]\r\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\r\n",
    "        concat_attention = pd.reshape(\r\n",
    "            attention, (batch_size, -1, self.embed_dim)\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        output = self.combine_heads(\r\n",
    "            concat_attention\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        return output\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义点式前馈网络（Point wise feed forward network）\n",
    "\n",
    "点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。\n",
    "\n",
    "这个网络不会改变向量的大小，只是做了一步提取特征的工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForwardNetwork(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, feed_dim):\r\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\r\n",
    "        self.linear1 = pd.fluid.dygraph.Linear(embed_dim, feed_dim, act='relu')\r\n",
    "        self.linear2 = nn.Linear(feed_dim, embed_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.linear2(out)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\r\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\r\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        maxlen = pd.shape(x)[-1]\r\n",
    "        positions = pd.arange(start=0, end=maxlen, step=1, dtype='int64')\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        return x + positions\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, feed_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\r\n",
    "        self.ffn = PointWiseFeedForwardNetwork(embed_dim, feed_dim)\r\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.dropout1 = nn.Dropout(rate)\r\n",
    "        self.dropout2 = nn.Dropout(rate)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        attn_output = self.att(inputs)\r\n",
    "        attn_output = self.dropout1(attn_output)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 组建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "        self.emb = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
    "        self.trs = TransformerBlock(embed_dim, num_heads, feed_dim)\r\n",
    "        self.drop1 = nn.Dropout(0.1)\r\n",
    "        self.relu = pd.fluid.dygraph.Linear(feed_dim, 20, act='relu')\r\n",
    "        self.drop2 = nn.Dropout(0.1)\r\n",
    "        self.soft = pd.fluid.dygraph.Linear(20, 2, act='softmax')\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x)\r\n",
    "        x = self.trs(x)\r\n",
    "        x = pd.mean(x, axis=1)\r\n",
    "        x = self.drop1(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        x = self.drop2(x)\r\n",
    "        x = self.soft(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "step 781/781 [==============================] - loss: 0.3791 - 101ms/step         \n",
      "Eval begin...\n",
      "step 781/781 [==============================] - loss: 0.3921 - 32ms/step         \n",
      "Eval samples: 24992\n",
      "Epoch 2/2\n",
      "step 781/781 [==============================] - loss: 0.5574 - 100ms/step        \n",
      "Eval begin...\n",
      "step 781/781 [==============================] - loss: 0.3562 - 33ms/step        \n",
      "Eval samples: 24992\n"
     ]
    }
   ],
   "source": [
    "#使用高层API进行训练\r\n",
    "model = pd.Model(MyNet()) # 用 Model封装 MyNet\r\n",
    "\r\n",
    "# 模型配置\r\n",
    "model.prepare(optimizer=pd.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()),\r\n",
    "              loss=nn.CrossEntropyLoss())\r\n",
    "\r\n",
    "# 模型训练\r\n",
    "model.fit(train_loader,\r\n",
    "          test_loader,\r\n",
    "          epochs=epochs,\r\n",
    "          batch_size=batch_size,\r\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到经过两轮的迭代训练，可以达到85%左右的准确率，当然你也可以通过调整参数、更改优化方式等等来进一步提升性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
