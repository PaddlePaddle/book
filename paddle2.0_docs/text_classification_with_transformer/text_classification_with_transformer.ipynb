{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 通过Transformer实现文本分类\n",
    "作者：[YinHang2515](https://github.com/YinHang2515)\n",
    "\n",
    "日期：2020年11月20日\n",
    "\n",
    "本示例教程演示如何使用Transformer模型在IMDB数据集上完成文本分类的任务。\n",
    "\n",
    "IMDB数据集是一个对电影评论标注为正向评论与负向评论的数据集，共有25000条文本数据作为训练集，25000条文本数据作为测试集。 该数据集的官方地址为： http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle as pd\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as func\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 处理数据集\n",
    "首先通过paddle内置的dataset完成数据集的导入，并构建字典和相应的reader\n",
    "\n",
    "然后通过padding的方式对同一个batch中长度不一致的数据进行补齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB word dict....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB word dict....\")\r\n",
    "word_dict = pd.dataset.imdb.word_dict()\r\n",
    "\r\n",
    "train_reader = pd.dataset.imdb.train(word_dict)\r\n",
    "test_reader = pd.dataset.imdb.test(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:0\n",
      "and:1\n",
      "a:2\n",
      "of:3\n",
      "to:4\n",
      "...\n",
      "virtual:5143\n",
      "warriors:5144\n",
      "widely:5145\n",
      "<unk>:5146\n",
      "<pad>:5147\n",
      "totally 5148 words\n"
     ]
    }
   ],
   "source": [
    "# 添加<pad>\r\n",
    "word_dict['<pad>'] = len(word_dict)\r\n",
    "\r\n",
    "for k in list(word_dict)[:5]:\r\n",
    "    print(\"{}:{}\".format(k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"...\")\r\n",
    "\r\n",
    "for k in list(word_dict)[-5:]:\r\n",
    "    print(\"{}:{}\".format(k if isinstance(k, str) else k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"totally {} words\".format(len(word_dict)))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_dict)  \r\n",
    "maxlen = 200  \r\n",
    "seq_len = 200\r\n",
    "batch_size = 128\r\n",
    "epochs = 2\r\n",
    "pad_id = word_dict['<pad>']\r\n",
    "embed_dim = 32  # Embedding size for each token\r\n",
    "num_heads = 2  # Number of attention heads\r\n",
    "feed_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
    "\r\n",
    "classes = ['positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200)\n",
      "(25000, 1)\n",
      "(25000, 200)\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "#用padding的方式对齐数据\r\n",
    "def create_padded_dataset(reader):\r\n",
    "    padded_sents = []\r\n",
    "    labels = []\r\n",
    "    for batch_id, data in enumerate(reader):\r\n",
    "        sent, label = data\r\n",
    "        padded_sent = sent[:seq_len] + [pad_id] * (seq_len - len(sent))\r\n",
    "        padded_sents.append(padded_sent)\r\n",
    "        labels.append(label)\r\n",
    "    return np.array(padded_sents), np.expand_dims(np.array(labels), axis=1)\r\n",
    "\r\n",
    "train_sents, train_labels = create_padded_dataset(train_reader())\r\n",
    "test_sents, test_labels = create_padded_dataset(test_reader())\r\n",
    "\r\n",
    "print(train_sents.shape)\r\n",
    "print(train_labels.shape)\r\n",
    "print(test_sents.shape)\r\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 用Dataset 与 DataLoader 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "class IMDBDataset(pd.io.Dataset):\r\n",
    "    def __init__(self, sents, labels):\r\n",
    "\r\n",
    "        self.sents = sents\r\n",
    "        self.labels = labels\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "\r\n",
    "        data = self.sents[index]\r\n",
    "        label = self.labels[index]\r\n",
    "\r\n",
    "        return data, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \r\n",
    "        return len(self.sents)\r\n",
    "    \r\n",
    "train_dataset = IMDBDataset(train_sents, train_labels)\r\n",
    "test_dataset = IMDBDataset(test_sents, test_labels)\r\n",
    "\r\n",
    "train_loader = pd.io.DataLoader(train_dataset, places=pd.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "test_loader = pd.io.DataLoader(test_dataset, places=pd.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义多头自注意力机制 (Multi-head Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads=8):\r\n",
    "        super(MultiHeadSelfAttention, self).__init__()\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        if embed_dim % num_heads != 0:\r\n",
    "            raise ValueError(\r\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\r\n",
    "            )\r\n",
    "        self.projection_dim = embed_dim // num_heads\r\n",
    "        self.query_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.key_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.value_dense = nn.Linear(embed_dim, embed_dim)\r\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim)\r\n",
    "\r\n",
    "    def attention(self, query, key, value):\r\n",
    "        score = pd.matmul(query, key, transpose_y=True)\r\n",
    "        dim_key = pd.cast(pd.shape(key)[-1], 'float32')\r\n",
    "        scaled_score = score / pd.sqrt(dim_key)\r\n",
    "        weights = func.softmax(scaled_score, axis=-1)\r\n",
    "        output = pd.matmul(weights, value)\r\n",
    "        return output, weights\r\n",
    "\r\n",
    "    def separate_heads(self, x, batch_size):\r\n",
    "        x = pd.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\r\n",
    "        return pd.transpose(x, perm=[0, 2, 1, 3])\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\r\n",
    "        batch_size = pd.shape(inputs)[0]\r\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        query = self.separate_heads(\r\n",
    "            query, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        key = self.separate_heads(\r\n",
    "            key, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        value = self.separate_heads(\r\n",
    "            value, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        attention, weights = self.attention(query, key, value)\r\n",
    "        attention = pd.transpose(\r\n",
    "            attention, perm=[0, 2, 1, 3]\r\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\r\n",
    "        concat_attention = pd.reshape(\r\n",
    "            attention, (batch_size, -1, self.embed_dim)\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        output = self.combine_heads(\r\n",
    "            concat_attention\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        return output\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义点式前馈网络（Point wise feed forward network）\n",
    "\n",
    "点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。\n",
    "\n",
    "这个网络不会改变向量的大小，只是做了一步提取特征的工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForwardNetwork(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, feed_dim):\r\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\r\n",
    "        self.linear1 = pd.nn.Linear(embed_dim, feed_dim)\r\n",
    "        self.linear2 = pd.nn.Linear(feed_dim, embed_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.linear2(out)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\r\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\r\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        maxlen = pd.shape(x)[-1]\r\n",
    "        positions = pd.arange(start=0, end=maxlen, step=1, dtype='int64')\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        return x + positions\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, feed_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\r\n",
    "        self.ffn = PointWiseFeedForwardNetwork(embed_dim, feed_dim)\r\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.dropout1 = nn.Dropout(rate)\r\n",
    "        self.dropout2 = nn.Dropout(rate)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        attn_output = self.att(inputs)\r\n",
    "        attn_output = self.dropout1(attn_output)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 组建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "        self.emb = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
    "        self.trs = TransformerBlock(embed_dim, num_heads, feed_dim)\r\n",
    "        self.drop1 = nn.Dropout(0.1)\r\n",
    "        self.linear1 = nn.Linear(feed_dim, 20)\r\n",
    "        self.drop2 = nn.Dropout(0.1)\r\n",
    "        self.linear2 = nn.Linear(20, 2)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x)\r\n",
    "        x = self.trs(x)\r\n",
    "        x = pd.mean(x, axis=1)\r\n",
    "        x = self.drop1(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = func.relu(x)\r\n",
    "        x = self.drop2(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        x = func.softmax(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/distributed/parallel.py:119: UserWarning: Currently not a parallel execution environment, `paddle.distributed.init_parallel_env` will not do anything.\n",
      "  \"Currently not a parallel execution environment, `paddle.distributed.init_parallel_env` will not do anything.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 195/195 [==============================] - loss: 0.4911 - 356ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 195/195 [==============================] - loss: 0.4804 - 110ms/step        \n",
      "Eval samples: 24960\n",
      "Epoch 2/2\n",
      "step 195/195 [==============================] - loss: 0.4327 - 351ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 195/195 [==============================] - loss: 0.4843 - 108ms/step        \n",
      "Eval samples: 24960\n"
     ]
    }
   ],
   "source": [
    "#使用高层API进行训练\r\n",
    "model = pd.Model(MyNet()) # 用 Model封装 MyNet\r\n",
    "\r\n",
    "# 模型配置\r\n",
    "model.prepare(optimizer=pd.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()),\r\n",
    "              loss=nn.CrossEntropyLoss())\r\n",
    "\r\n",
    "#模型训练\r\n",
    "model.fit(train_loader,\r\n",
    "          test_loader,\r\n",
    "          epochs=epochs,\r\n",
    "          batch_size=batch_size,\r\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 195/195 [==============================] - 110ms/step        \n",
      "Predict samples: 24960\n"
     ]
    }
   ],
   "source": [
    "#使用model.predict对测试集进行测试\r\n",
    "result = model.predict(test_loader, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1837 now the scifi channel original company has made some pretty crappy films house of the dead 2 all souls day etc but when you leave the job entirely to horror master and now director bruce campbell you get one of the best damn made for tv independent horror films ever made i normally hate these movies in my previous review house of the dead 2 i could not believe how horrible the film was but somehow i took a liking for this film a very good liking for this film the violence is good and so is the black comedy in the film and i recommend you get it a true bruce campbell masterpiece well since there is only a few more lines left i can say whatever i want about this movie \n",
      "Lable: positive\n",
      "Predict: negative\n",
      "#14350 they do each sequel is worst you who think that 2 or 3 need a 1 please watch this sequel youll be wondering with the first three parts then youll give a 10 to the first 8 to the second and 5 or 6 to the other thats because 4 really gets the big 1 from me it does \n",
      "Lable: negative\n",
      "Predict: negative\n",
      "#1032 i agree with this is a great movie here are some dialogue br sean connery to you see the man at the well how he draws the water when one the other it is so with the world at present you are full of power but youre it and is up the drops as they from your br br the english have paid very well in the past well youll not have your way with the americans president will have your head for this this president would try and take it himself he certainly would he is a man of and strong moral he does not women and children what kind of does he use a i have no knowledge of this you will br br brian keith the american bear is a of the american character strength intelligence a little blind and at times but beyond all doubt oh and one other goes with all previous \n",
      "Lable: positive\n",
      "Predict: positive\n",
      "#20055 the real star of the last of the airport films is that big the french created called the if you bear in mind that the whole film is dedicated to showing what that plane could do in the sky than the whole film kind of makes br but if youre expecting some serious drama here than by all means take some of the action the shows here when some nasty folks try to shoot her br susan plays a television news reporter who also happens to be the mistress of military robert one of just happens to bring her information on some of dirty business selling arms to folks not friendly to the usa when the source is killed in front of her and shes nearly done in by a hitman whom she escapes from of course she with the information and of course he it but right before the widow hands her the necessary br br but on the way to with a stop in paris and his \n",
      "Lable: negative\n",
      "Predict: negative\n",
      "#8924 throw hasnt dated at all its as funny now as when it was released a genuinely eccentric comedy that doesnt try too hard to be liked and is all the better for it full of memorable laugh out loud lines even small characters are well written and beautifully played like billy best friends girlfriend and a lovely cameo from rob as agent a little bit insane and a lot funny \n",
      "Lable: positive\n",
      "Predict: negative\n",
      "#16674 starts in los angeles with jim roger his wife alice as they drive along in the rain unfortunately jim the car his daughter leigh silver ends up dead while alice is turned into a bound by the death of his daughter jim starts visiting he then kills them because of voices in his head thats it br written produced directed by i hate as a film there are some films you occasionally see that move the goal as it were in regard to everything you watch some films are so brilliant that all others will be by it while others like for example are so bad that it sets a new cinematic low this is truly one of the worst films ive ever seen i am seriously surprised by the largely positive comments on the imdb although im not surprised the the low overall rating on the main page i not sure if i missed something but for a start has no plot it has no story a lot of it seems almost random there was nothing \n",
      "Lable: negative\n",
      "Predict: negative\n",
      "#19070 this movie probably seemed like a great idea in lets make a movie about one of the greatest and most controversial of the modern era and lets cast brian as coach bobby knight thats where this movie went terribly wrong why cast an actor who bears no of the man hes portraying and then why let this actor turn his character into not coach knight but brian in a red as i sat watching this movie on i didnt find myself believing this man was actually coach knight he didnt look like him talk like him act like him or even walk like him i could not get past this fact and i could not enjoy the movie when paul and robert were cast as the and the kid we didnt care if they were accurate historical models of their true characters because most of us had never even heard of these men until we saw the movie but with someone as visible in todays media as coach knight you have to do better when anthony was cast as it was the same \n",
      "Lable: negative\n",
      "Predict: positive\n",
      "#8422 i am a college student and i bought this movie at a used book store because it sounded really funny from the back description nothing would prepare me for what would be the funniest movie ever br it is especially good to watch if you are an like me because then you can see how silly these christian people are they seem to think like to live in really disgusting houses and do nothing but drink beer and be mean to the neighbors its a laugh br my buddies and i watch it every couple of weeks so that we may be entertained beyond our see this movie nowbr br lets hope rich makes some more of these movies to entertain us \n",
      "Lable: positive\n",
      "Predict: positive\n",
      "#11299 i think that new york times film critic elvis mitchell wrote the best one line review of in the mood for love when he said that it is with a romantic spirit thats been missing from the cinema forever how true those words are truly romantic films are so rare these days while films that include plenty of sex and nudity which are often portrayed in a and gratuitous manner so given this cinematic latest film feels like a much needed breath of fresh air in the mood for love is about the doomed romance between two neighbors mr played by tony and mrs chan played by maggie whose are having an affair as they try not to be like them but after hanging out with each other on lonely nights while their are away on care of a sick mother they fall in love and must resist the of going too br several are responsible for making in the mood for love a new classic among romantic in the best sense of that term first the specific period of the film \n",
      "Lable: positive\n",
      "Predict: positive\n",
      "#10607 this one gets better with each new look certainly one of paul best roles outstanding music score which was also outstanding on sound track so why no cd one the very early sound film releases by the way the original theatrical trailer for this is really great \n",
      "Lable: positive\n",
      "Predict: negative\n"
     ]
    }
   ],
   "source": [
    "#随机选择\r\n",
    "label_pred = np.array([])\r\n",
    "for label in result[0]:\r\n",
    "    label_pred = np.append(label_pred, label.reshape(-1))\r\n",
    "for i in range(10):\r\n",
    "    words = \"\"\r\n",
    "    idx = np.random.randint(int(len(label_pred)/2))\r\n",
    "    for k in test_sents[idx]:\r\n",
    "        word = list(word_dict)[k]\r\n",
    "        if not isinstance(word, str):\r\n",
    "            word = word.decode('ASCII')\r\n",
    "        if word != '<pad>' and word != '<unk>':\r\n",
    "            words += (word + \" \")\r\n",
    "    print(\"#\" + str(idx) + \" \" + words)\r\n",
    "    print(\"Lable: \" + classes[test_labels[idx][0]])\r\n",
    "    if label_pred[2 * idx] > label_pred[2 * idx + 1]:\r\n",
    "        print(\"Predict: \" + classes[0])\r\n",
    "    else:\r\n",
    "        print(\"Predict: \" + classes[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
