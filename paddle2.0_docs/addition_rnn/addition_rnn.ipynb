{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用序列到序列模型完成数字加法\n",
    "* 作者：[jm12138](https://github.com/jm12138)\n",
    "* 日期：2020.10.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简要介绍\n",
    "* 本示例教程介绍如何使用飞桨完成一个数字加法任务\n",
    "* 我们将会使用飞桨提供的LSTM的API，组建一个序列到序列模型\n",
    "* 并在随机生成的数据集上完成数字加法任务的模型训练与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "* 本示例教程基于飞桨2.0-rc版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle version: 2.0.0-rc0\n"
     ]
    }
   ],
   "source": [
    "# 导入项目运行所需的包\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "from visualdl import LogWriter\n",
    "\n",
    "# 打印Paddle版本\n",
    "print('paddle version: %s' % paddle.__version__)\n",
    "\n",
    "# 设置CPU为运行位置\n",
    "place = paddle.CPUPlace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集\n",
    "* 随机生成数据，并使用生成的数据构造数据集\n",
    "* 通过继承paddle.io.Dataset来完成数据集的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating datas..\n",
      "making the dataset...\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# 编码函数\n",
    "def encoder(text, LEN, label_dict):\n",
    "    # 文本转ID\n",
    "    ids = [label_dict[word] for word in text]\n",
    "    # 对长度进行补齐\n",
    "    ids += [label_dict[' ']]*(LEN-len(ids))\n",
    "    return ids\n",
    "\n",
    "# 单个数据生成函数\n",
    "def make_data(inputs, labels, DIGITS, label_dict):\n",
    "    MAXLEN = DIGITS + 1 + DIGITS\n",
    "    # 对输入输出文本进行ID编码\n",
    "    inputs = encoder(inputs, MAXLEN, label_dict)\n",
    "    labels = encoder(labels, DIGITS + 1, label_dict)\n",
    "    return inputs, labels\n",
    "\n",
    "# 批量数据生成函数\n",
    "def gen_datas(DATA_NUM, MAX_NUM, DIGITS, label_dict):\n",
    "    datas = []\n",
    "    while len(datas)<DATA_NUM:\n",
    "        # 随机取两个数\n",
    "        a = random.randint(0,MAX_NUM)\n",
    "        b = random.randint(0,MAX_NUM)\n",
    "        # 生成输入文本\n",
    "        inputs = '%d+%d' % (a, b)\n",
    "        # 生成输出文本\n",
    "        labels = str(eval(inputs))\n",
    "        # 生成单个数据\n",
    "        inputs, labels = [np.array(_).astype('int64') for _ in make_data(inputs, labels, DIGITS, label_dict)]\n",
    "        datas.append([inputs, labels])\n",
    "    return datas\n",
    "\n",
    "# 继承paddle.io.Dataset来构造数据集\n",
    "class Addition_Dataset(paddle.io.Dataset):\n",
    "    # 重写数据集初始化函数\n",
    "    def __init__(self, datas):\n",
    "        super(Addition_Dataset, self).__init__()\n",
    "        self.datas = datas\n",
    "    \n",
    "    # 重写生成样本的函数\n",
    "    def __getitem__(self, index):\n",
    "        data, label = [paddle.to_tensor(_) for _ in self.datas[index]]\n",
    "        return data, label\n",
    "\n",
    "    # 重写返回数据集大小的函数\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "print('generating datas..')\n",
    "\n",
    "# 定义字符表\n",
    "label_dict = {\n",
    "    '0': 0, '1': 1, '2': 2, '3': 3,\n",
    "    '4': 4, '5': 5, '6': 6, '7': 7,\n",
    "    '8': 8, '9': 9, '+': 10, ' ': 11\n",
    "}\n",
    "\n",
    "# 输入数字最大位数\n",
    "DIGITS = 2\n",
    "\n",
    "# 数据数量\n",
    "train_num = 5000\n",
    "dev_num = 500\n",
    "\n",
    "# 数据批大小\n",
    "batch_size = 32\n",
    "\n",
    "# 读取线程数\n",
    "num_workers = 8\n",
    "\n",
    "# 定义一些所需变量\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "MAX_NUM = 10**(DIGITS)-1\n",
    "\n",
    "# 生成数据\n",
    "train_datas = gen_datas(\n",
    "    train_num, \n",
    "    MAX_NUM,\n",
    "    DIGITS, \n",
    "    label_dict\n",
    ") \n",
    "dev_datas = gen_datas(\n",
    "    dev_num, \n",
    "    MAX_NUM,\n",
    "    DIGITS, \n",
    "    label_dict\n",
    ")\n",
    "\n",
    "# 实例化数据集\n",
    "train_dataset = Addition_Dataset(train_datas)\n",
    "dev_dataset = Addition_Dataset(dev_datas)\n",
    "\n",
    "print('making the dataset...')\n",
    "\n",
    "# 实例化数据读取器\n",
    "train_reader = paddle.io.DataLoader(\n",
    "    train_dataset,\n",
    "    places=place,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "dev_reader = paddle.io.DataLoader(\n",
    "    dev_dataset,\n",
    "    places=place,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  模型组网\n",
    "* 通过继承paddle.nn.Layer类来搭建模型\n",
    "* 本次介绍的模型是一个简单的基于LSTM的Seq2Seq模型\n",
    "* 一共有如下四个主要的网络层：\n",
    "\n",
    "  1. 嵌入层(Embedding)：将输入的文本序列转为嵌入向量\n",
    "  2. 编码层(LSTM)：将嵌入向量进行编码\n",
    "  3. 解码层(LSTM)：将编码向量进行解码\n",
    "  4. 全连接层(Linear)：对解码完成的向量进行线性映射\n",
    "* 损失函数为交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继承paddle.nn.Layer类\n",
    "class Addition_Model(nn.Layer):\n",
    "    # 重写初始化函数\n",
    "    # 参数：字符表长度、嵌入层大小、隐藏层大小、解码器层数、处理数字的最大位数\n",
    "    def __init__(self, char_len=12, embedding_size=128, hidden_size=128, num_layers=1, DIGITS=2):\n",
    "        super(Addition_Model, self).__init__()\n",
    "        # 初始化变量\n",
    "        self.DIGITS = DIGITS\n",
    "        self.MAXLEN = DIGITS + 1 + DIGITS\n",
    "        self.hidden_size = hidden_size\n",
    "        self.char_len = char_len\n",
    "\n",
    "        # 嵌入层\n",
    "        self.emb = nn.Embedding(\n",
    "            char_len, \n",
    "            embedding_size\n",
    "        )\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1\n",
    "        )\n",
    "        \n",
    "        # 解码器\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size, \n",
    "            char_len\n",
    "        )\n",
    "    \n",
    "    # 重写模型前向计算函数\n",
    "    # 参数：输入[None, MAXLEN]、标签[None, DIGITS + 1]\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # 嵌入层\n",
    "        out = self.emb(inputs)\n",
    "\n",
    "        # 编码器\n",
    "        out, (_, _) = self.encoder(out)\n",
    "\n",
    "        # 按时间步切分编码器输出\n",
    "        out = paddle.split(out, self.MAXLEN, axis=1)\n",
    "\n",
    "        # 取最后一个时间步的输出并复制 DIGITS + 1 次\n",
    "        out = paddle.expand(out[-1], [out[-1].shape[0], self.DIGITS + 1, self.hidden_size])\n",
    "\n",
    "        # 解码器\n",
    "        out, (_, _) = self.decoder(out)\n",
    "\n",
    "        # 全连接\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # 如果标签存在，则计算其损失和准确率\n",
    "        if labels is not None:\n",
    "            # 转置解码器输出\n",
    "            tmp = paddle.transpose(out, [0, 2, 1])\n",
    "\n",
    "            # 计算交叉熵损失\n",
    "            loss = nn.functional.cross_entropy(tmp, labels)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc = paddle.metric.accuracy(paddle.reshape(out, [-1, self.char_len]), paddle.reshape(labels, [-1, 1]))\n",
    "\n",
    "            # 返回损失和准确率\n",
    "            return loss, acc\n",
    "\n",
    "        # 返回输出\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与评估\n",
    "* 使用Adam作为优化器进行模型训练\n",
    "* 以模型准确率作为评价指标\n",
    "* 使用VisualDL对训练数据进行可视化\n",
    "* 训练过程中会同时进行模型评估和最佳模型的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 step: 0 loss:2.485033 acc:0.093750\n",
      "eval epoch:0 step: 0 loss:2.485033 acc:0.093750\n",
      "saving the best_model...\n",
      "train epoch:0 step: 20 loss:2.408553 acc:0.145833\n",
      "train epoch:0 step: 40 loss:2.235319 acc:0.395833\n",
      "train epoch:0 step: 60 loss:2.215429 acc:0.406250\n",
      "train epoch:0 step: 80 loss:2.252749 acc:0.354167\n",
      "train epoch:0 step: 100 loss:2.244046 acc:0.375000\n",
      "train epoch:0 step: 120 loss:2.253286 acc:0.364583\n",
      "train epoch:0 step: 140 loss:2.224211 acc:0.395833\n",
      "train epoch:1 step: 160 loss:2.254224 acc:0.364583\n",
      "train epoch:1 step: 180 loss:2.253024 acc:0.364583\n",
      "train epoch:1 step: 200 loss:2.253495 acc:0.364583\n",
      "train epoch:1 step: 220 loss:2.150470 acc:0.468750\n",
      "train epoch:1 step: 240 loss:2.262765 acc:0.354167\n",
      "train epoch:1 step: 260 loss:2.253251 acc:0.364583\n",
      "train epoch:1 step: 280 loss:2.214941 acc:0.406250\n",
      "train epoch:1 step: 300 loss:2.241283 acc:0.375000\n",
      "train epoch:2 step: 320 loss:2.252329 acc:0.364583\n",
      "train epoch:2 step: 340 loss:2.237661 acc:0.385417\n",
      "train epoch:2 step: 360 loss:2.224525 acc:0.395833\n",
      "train epoch:2 step: 380 loss:2.204742 acc:0.416667\n",
      "train epoch:2 step: 400 loss:2.242289 acc:0.375000\n",
      "train epoch:2 step: 420 loss:2.168498 acc:0.447917\n",
      "train epoch:2 step: 440 loss:2.226979 acc:0.395833\n",
      "train epoch:2 step: 460 loss:2.221338 acc:0.385417\n",
      "train epoch:3 step: 480 loss:2.218206 acc:0.395833\n",
      "train epoch:3 step: 500 loss:2.209448 acc:0.406250\n",
      "eval epoch:3 step: 500 loss:2.209448 acc:0.406250\n",
      "saving the best_model...\n",
      "train epoch:3 step: 520 loss:2.226512 acc:0.385417\n",
      "train epoch:3 step: 540 loss:2.168457 acc:0.458333\n",
      "train epoch:3 step: 560 loss:2.236423 acc:0.375000\n",
      "train epoch:3 step: 580 loss:2.210313 acc:0.427083\n",
      "train epoch:3 step: 600 loss:2.242670 acc:0.364583\n",
      "train epoch:3 step: 620 loss:2.188299 acc:0.427083\n",
      "train epoch:4 step: 640 loss:2.192388 acc:0.416667\n",
      "train epoch:4 step: 660 loss:2.200211 acc:0.416667\n",
      "train epoch:4 step: 680 loss:2.164731 acc:0.447917\n",
      "train epoch:4 step: 700 loss:2.215114 acc:0.395833\n",
      "train epoch:4 step: 720 loss:2.211918 acc:0.406250\n",
      "train epoch:4 step: 740 loss:2.195170 acc:0.437500\n",
      "train epoch:4 step: 760 loss:2.196378 acc:0.427083\n",
      "train epoch:4 step: 780 loss:2.193992 acc:0.395833\n",
      "train epoch:5 step: 800 loss:2.168856 acc:0.447917\n",
      "train epoch:5 step: 820 loss:2.196009 acc:0.427083\n",
      "train epoch:5 step: 840 loss:2.188383 acc:0.447917\n",
      "train epoch:5 step: 860 loss:2.186121 acc:0.447917\n",
      "train epoch:5 step: 880 loss:2.193837 acc:0.416667\n",
      "train epoch:5 step: 900 loss:2.210794 acc:0.406250\n",
      "train epoch:5 step: 920 loss:2.161280 acc:0.447917\n",
      "train epoch:5 step: 940 loss:2.139361 acc:0.479167\n",
      "train epoch:6 step: 960 loss:2.153265 acc:0.479167\n",
      "train epoch:6 step: 980 loss:2.196227 acc:0.406250\n",
      "train epoch:6 step: 1000 loss:2.171296 acc:0.406250\n",
      "eval epoch:6 step: 1000 loss:2.171296 acc:0.406250\n",
      "train epoch:6 step: 1020 loss:2.143297 acc:0.479167\n",
      "train epoch:6 step: 1040 loss:2.143037 acc:0.458333\n",
      "train epoch:6 step: 1060 loss:2.119567 acc:0.500000\n",
      "train epoch:6 step: 1080 loss:2.131720 acc:0.500000\n",
      "train epoch:7 step: 1100 loss:2.178119 acc:0.416667\n",
      "train epoch:7 step: 1120 loss:2.180813 acc:0.427083\n",
      "train epoch:7 step: 1140 loss:2.159398 acc:0.458333\n",
      "train epoch:7 step: 1160 loss:2.140251 acc:0.458333\n",
      "train epoch:7 step: 1180 loss:2.153191 acc:0.468750\n",
      "train epoch:7 step: 1200 loss:2.135339 acc:0.489583\n",
      "train epoch:7 step: 1220 loss:2.090904 acc:0.541667\n",
      "train epoch:7 step: 1240 loss:2.163815 acc:0.437500\n",
      "train epoch:8 step: 1260 loss:2.085285 acc:0.520833\n",
      "train epoch:8 step: 1280 loss:2.113217 acc:0.500000\n",
      "train epoch:8 step: 1300 loss:2.188943 acc:0.427083\n",
      "train epoch:8 step: 1320 loss:2.144432 acc:0.479167\n",
      "train epoch:8 step: 1340 loss:2.140976 acc:0.468750\n",
      "train epoch:8 step: 1360 loss:2.114273 acc:0.489583\n",
      "train epoch:8 step: 1380 loss:2.146623 acc:0.468750\n",
      "train epoch:8 step: 1400 loss:2.170307 acc:0.427083\n",
      "train epoch:9 step: 1420 loss:2.148923 acc:0.468750\n",
      "train epoch:9 step: 1440 loss:2.191637 acc:0.427083\n",
      "train epoch:9 step: 1460 loss:2.162540 acc:0.458333\n",
      "train epoch:9 step: 1480 loss:2.146085 acc:0.468750\n",
      "train epoch:9 step: 1500 loss:2.102295 acc:0.520833\n",
      "eval epoch:9 step: 1500 loss:2.102295 acc:0.520833\n",
      "saving the best_model...\n",
      "train epoch:9 step: 1520 loss:2.148785 acc:0.447917\n",
      "train epoch:9 step: 1540 loss:2.132374 acc:0.489583\n",
      "train epoch:9 step: 1560 loss:2.074304 acc:0.541667\n",
      "train epoch:10 step: 1580 loss:2.142809 acc:0.458333\n",
      "train epoch:10 step: 1600 loss:2.123583 acc:0.500000\n",
      "train epoch:10 step: 1620 loss:2.156823 acc:0.458333\n",
      "train epoch:10 step: 1640 loss:2.124027 acc:0.500000\n",
      "train epoch:10 step: 1660 loss:2.126479 acc:0.489583\n",
      "train epoch:10 step: 1680 loss:2.160826 acc:0.447917\n",
      "train epoch:10 step: 1700 loss:2.178597 acc:0.416667\n",
      "train epoch:10 step: 1720 loss:2.148957 acc:0.458333\n",
      "train epoch:11 step: 1740 loss:2.130795 acc:0.489583\n",
      "train epoch:11 step: 1760 loss:2.126959 acc:0.489583\n",
      "train epoch:11 step: 1780 loss:2.095742 acc:0.531250\n",
      "train epoch:11 step: 1800 loss:2.190416 acc:0.427083\n",
      "train epoch:11 step: 1820 loss:2.084354 acc:0.531250\n",
      "train epoch:11 step: 1840 loss:2.178923 acc:0.427083\n",
      "train epoch:11 step: 1860 loss:2.084016 acc:0.510417\n",
      "train epoch:11 step: 1880 loss:2.087030 acc:0.520833\n",
      "train epoch:12 step: 1900 loss:2.142104 acc:0.468750\n",
      "train epoch:12 step: 1920 loss:2.093829 acc:0.520833\n",
      "train epoch:12 step: 1940 loss:2.131518 acc:0.489583\n",
      "train epoch:12 step: 1960 loss:2.116108 acc:0.500000\n",
      "train epoch:12 step: 1980 loss:2.096988 acc:0.520833\n",
      "train epoch:12 step: 2000 loss:2.136149 acc:0.458333\n",
      "eval epoch:12 step: 2000 loss:2.136149 acc:0.458333\n",
      "train epoch:12 step: 2020 loss:2.126221 acc:0.489583\n",
      "train epoch:12 step: 2040 loss:2.215861 acc:0.375000\n",
      "train epoch:13 step: 2060 loss:2.176953 acc:0.437500\n",
      "train epoch:13 step: 2080 loss:2.143498 acc:0.468750\n",
      "train epoch:13 step: 2100 loss:2.133570 acc:0.489583\n",
      "train epoch:13 step: 2120 loss:2.131343 acc:0.479167\n",
      "train epoch:13 step: 2140 loss:2.079574 acc:0.552083\n",
      "train epoch:13 step: 2160 loss:2.158209 acc:0.468750\n",
      "train epoch:13 step: 2180 loss:2.110152 acc:0.520833\n",
      "train epoch:14 step: 2200 loss:2.083514 acc:0.541667\n",
      "train epoch:14 step: 2220 loss:2.123724 acc:0.479167\n",
      "train epoch:14 step: 2240 loss:2.167266 acc:0.447917\n",
      "train epoch:14 step: 2260 loss:2.130182 acc:0.468750\n",
      "train epoch:14 step: 2280 loss:2.143327 acc:0.468750\n",
      "train epoch:14 step: 2300 loss:2.138113 acc:0.468750\n",
      "train epoch:14 step: 2320 loss:2.105023 acc:0.531250\n",
      "train epoch:14 step: 2340 loss:2.098114 acc:0.531250\n",
      "train epoch:15 step: 2360 loss:2.116810 acc:0.510417\n",
      "train epoch:15 step: 2380 loss:2.024528 acc:0.583333\n",
      "train epoch:15 step: 2400 loss:2.035045 acc:0.583333\n",
      "train epoch:15 step: 2420 loss:2.111958 acc:0.510417\n",
      "train epoch:15 step: 2440 loss:2.131193 acc:0.468750\n",
      "train epoch:15 step: 2460 loss:2.109442 acc:0.489583\n",
      "train epoch:15 step: 2480 loss:2.037780 acc:0.604167\n",
      "train epoch:15 step: 2500 loss:2.116760 acc:0.500000\n",
      "eval epoch:15 step: 2500 loss:2.116760 acc:0.500000\n",
      "train epoch:16 step: 2520 loss:2.085177 acc:0.541667\n",
      "train epoch:16 step: 2540 loss:2.110000 acc:0.520833\n",
      "train epoch:16 step: 2560 loss:2.109299 acc:0.500000\n",
      "train epoch:16 step: 2580 loss:2.140175 acc:0.489583\n",
      "train epoch:16 step: 2600 loss:2.117856 acc:0.479167\n",
      "train epoch:16 step: 2620 loss:2.126268 acc:0.479167\n",
      "train epoch:16 step: 2640 loss:2.071561 acc:0.541667\n",
      "train epoch:16 step: 2660 loss:2.059859 acc:0.562500\n",
      "train epoch:17 step: 2680 loss:2.075985 acc:0.541667\n",
      "train epoch:17 step: 2700 loss:2.138288 acc:0.468750\n",
      "train epoch:17 step: 2720 loss:2.138012 acc:0.489583\n",
      "train epoch:17 step: 2740 loss:2.156902 acc:0.458333\n",
      "train epoch:17 step: 2760 loss:2.118933 acc:0.520833\n",
      "train epoch:17 step: 2780 loss:2.088427 acc:0.552083\n",
      "train epoch:17 step: 2800 loss:2.098581 acc:0.520833\n",
      "train epoch:17 step: 2820 loss:2.061374 acc:0.562500\n",
      "train epoch:18 step: 2840 loss:2.061162 acc:0.552083\n",
      "train epoch:18 step: 2860 loss:2.116271 acc:0.500000\n",
      "train epoch:18 step: 2880 loss:2.065627 acc:0.541667\n",
      "train epoch:18 step: 2900 loss:2.111847 acc:0.489583\n",
      "train epoch:18 step: 2920 loss:2.093766 acc:0.520833\n",
      "train epoch:18 step: 2940 loss:2.132539 acc:0.479167\n",
      "train epoch:18 step: 2960 loss:2.074923 acc:0.562500\n",
      "train epoch:18 step: 2980 loss:2.104811 acc:0.510417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:19 step: 3000 loss:2.088592 acc:0.510417\n",
      "eval epoch:19 step: 3000 loss:2.088592 acc:0.510417\n",
      "train epoch:19 step: 3020 loss:2.102115 acc:0.520833\n",
      "train epoch:19 step: 3040 loss:2.054089 acc:0.562500\n",
      "train epoch:19 step: 3060 loss:2.069692 acc:0.552083\n",
      "train epoch:19 step: 3080 loss:2.020173 acc:0.614583\n",
      "train epoch:19 step: 3100 loss:2.111103 acc:0.510417\n",
      "train epoch:19 step: 3120 loss:2.011185 acc:0.614583\n",
      "train epoch:20 step: 3140 loss:1.972947 acc:0.645833\n",
      "train epoch:20 step: 3160 loss:2.110248 acc:0.510417\n",
      "train epoch:20 step: 3180 loss:2.110843 acc:0.500000\n",
      "train epoch:20 step: 3200 loss:2.045410 acc:0.572917\n",
      "train epoch:20 step: 3220 loss:2.061454 acc:0.552083\n",
      "train epoch:20 step: 3240 loss:2.116978 acc:0.479167\n",
      "train epoch:20 step: 3260 loss:2.075301 acc:0.562500\n",
      "train epoch:20 step: 3280 loss:2.020166 acc:0.593750\n",
      "train epoch:21 step: 3300 loss:2.028885 acc:0.604167\n",
      "train epoch:21 step: 3320 loss:2.012455 acc:0.604167\n",
      "train epoch:21 step: 3340 loss:2.050273 acc:0.562500\n",
      "train epoch:21 step: 3360 loss:2.049257 acc:0.583333\n",
      "train epoch:21 step: 3380 loss:2.010973 acc:0.614583\n",
      "train epoch:21 step: 3400 loss:2.030514 acc:0.593750\n",
      "train epoch:21 step: 3420 loss:1.992300 acc:0.635417\n",
      "train epoch:21 step: 3440 loss:2.009651 acc:0.625000\n",
      "train epoch:22 step: 3460 loss:2.073640 acc:0.541667\n",
      "train epoch:22 step: 3480 loss:1.997681 acc:0.625000\n",
      "train epoch:22 step: 3500 loss:2.071093 acc:0.541667\n",
      "eval epoch:22 step: 3500 loss:2.071093 acc:0.541667\n",
      "saving the best_model...\n",
      "train epoch:22 step: 3520 loss:1.977061 acc:0.666667\n",
      "train epoch:22 step: 3540 loss:2.051390 acc:0.562500\n",
      "train epoch:22 step: 3560 loss:2.014240 acc:0.614583\n",
      "train epoch:22 step: 3580 loss:2.029595 acc:0.593750\n",
      "train epoch:22 step: 3600 loss:2.057017 acc:0.562500\n",
      "train epoch:23 step: 3620 loss:2.043925 acc:0.583333\n",
      "train epoch:23 step: 3640 loss:1.983295 acc:0.666667\n",
      "train epoch:23 step: 3660 loss:1.930016 acc:0.718750\n",
      "train epoch:23 step: 3680 loss:1.998225 acc:0.656250\n",
      "train epoch:23 step: 3700 loss:2.020724 acc:0.614583\n",
      "train epoch:23 step: 3720 loss:2.036568 acc:0.572917\n",
      "train epoch:23 step: 3740 loss:1.942185 acc:0.697917\n",
      "train epoch:23 step: 3760 loss:1.999302 acc:0.625000\n",
      "train epoch:24 step: 3780 loss:2.034012 acc:0.604167\n",
      "train epoch:24 step: 3800 loss:1.917741 acc:0.739583\n",
      "train epoch:24 step: 3820 loss:2.009123 acc:0.625000\n",
      "train epoch:24 step: 3840 loss:2.031808 acc:0.593750\n",
      "train epoch:24 step: 3860 loss:1.989426 acc:0.656250\n",
      "train epoch:24 step: 3880 loss:1.980893 acc:0.625000\n",
      "train epoch:24 step: 3900 loss:1.943371 acc:0.687500\n",
      "train epoch:24 step: 3920 loss:1.935834 acc:0.677083\n",
      "train epoch:25 step: 3940 loss:1.958081 acc:0.677083\n",
      "train epoch:25 step: 3960 loss:2.002775 acc:0.635417\n",
      "train epoch:25 step: 3980 loss:2.033413 acc:0.593750\n",
      "train epoch:25 step: 4000 loss:1.975006 acc:0.666667\n",
      "eval epoch:25 step: 4000 loss:1.975006 acc:0.666667\n",
      "saving the best_model...\n",
      "train epoch:25 step: 4020 loss:1.977628 acc:0.656250\n",
      "train epoch:25 step: 4040 loss:1.982264 acc:0.666667\n",
      "train epoch:25 step: 4060 loss:1.960987 acc:0.677083\n",
      "train epoch:25 step: 4080 loss:1.864528 acc:0.770833\n",
      "train epoch:26 step: 4100 loss:1.892222 acc:0.739583\n",
      "train epoch:26 step: 4120 loss:1.914104 acc:0.708333\n",
      "train epoch:26 step: 4140 loss:1.925230 acc:0.718750\n",
      "train epoch:26 step: 4160 loss:1.924310 acc:0.687500\n",
      "train epoch:26 step: 4180 loss:1.895999 acc:0.739583\n",
      "train epoch:26 step: 4200 loss:1.895360 acc:0.750000\n",
      "train epoch:26 step: 4220 loss:1.936628 acc:0.697917\n",
      "train epoch:27 step: 4240 loss:1.931340 acc:0.697917\n",
      "train epoch:27 step: 4260 loss:1.942701 acc:0.687500\n",
      "train epoch:27 step: 4280 loss:1.942334 acc:0.697917\n",
      "train epoch:27 step: 4300 loss:1.931857 acc:0.687500\n",
      "train epoch:27 step: 4320 loss:1.918606 acc:0.708333\n",
      "train epoch:27 step: 4340 loss:1.959075 acc:0.666667\n",
      "train epoch:27 step: 4360 loss:1.837479 acc:0.791667\n",
      "train epoch:27 step: 4380 loss:1.897484 acc:0.729167\n",
      "train epoch:28 step: 4400 loss:1.887688 acc:0.750000\n",
      "train epoch:28 step: 4420 loss:1.861869 acc:0.770833\n",
      "train epoch:28 step: 4440 loss:1.828221 acc:0.802083\n",
      "train epoch:28 step: 4460 loss:1.864297 acc:0.781250\n",
      "train epoch:28 step: 4480 loss:1.864595 acc:0.781250\n",
      "train epoch:28 step: 4500 loss:1.905885 acc:0.718750\n",
      "eval epoch:28 step: 4500 loss:1.905885 acc:0.718750\n",
      "saving the best_model...\n",
      "train epoch:28 step: 4520 loss:1.898291 acc:0.729167\n",
      "train epoch:28 step: 4540 loss:1.960801 acc:0.666667\n",
      "train epoch:29 step: 4560 loss:1.953930 acc:0.666667\n",
      "train epoch:29 step: 4580 loss:1.883116 acc:0.750000\n",
      "train epoch:29 step: 4600 loss:1.866855 acc:0.750000\n",
      "train epoch:29 step: 4620 loss:1.890665 acc:0.739583\n",
      "train epoch:29 step: 4640 loss:1.955193 acc:0.697917\n",
      "train epoch:29 step: 4660 loss:1.866565 acc:0.770833\n",
      "train epoch:29 step: 4680 loss:1.839826 acc:0.791667\n",
      "train epoch:29 step: 4700 loss:1.884906 acc:0.739583\n",
      "train epoch:30 step: 4720 loss:1.873335 acc:0.750000\n",
      "train epoch:30 step: 4740 loss:1.862446 acc:0.770833\n",
      "train epoch:30 step: 4760 loss:1.845827 acc:0.781250\n",
      "train epoch:30 step: 4780 loss:1.853939 acc:0.781250\n",
      "train epoch:30 step: 4800 loss:1.861476 acc:0.770833\n",
      "train epoch:30 step: 4820 loss:1.854194 acc:0.781250\n",
      "train epoch:30 step: 4840 loss:1.951349 acc:0.677083\n",
      "train epoch:30 step: 4860 loss:1.839390 acc:0.781250\n",
      "train epoch:31 step: 4880 loss:1.844819 acc:0.781250\n",
      "train epoch:31 step: 4900 loss:1.902362 acc:0.739583\n",
      "train epoch:31 step: 4920 loss:1.847222 acc:0.781250\n",
      "train epoch:31 step: 4940 loss:1.976317 acc:0.656250\n",
      "train epoch:31 step: 4960 loss:1.841331 acc:0.791667\n",
      "train epoch:31 step: 4980 loss:1.870673 acc:0.750000\n",
      "train epoch:31 step: 5000 loss:1.854230 acc:0.760417\n",
      "eval epoch:31 step: 5000 loss:1.854230 acc:0.760417\n",
      "saving the best_model...\n",
      "train epoch:31 step: 5020 loss:1.874280 acc:0.750000\n",
      "train epoch:32 step: 5040 loss:1.895703 acc:0.739583\n",
      "train epoch:32 step: 5060 loss:1.819288 acc:0.812500\n",
      "train epoch:32 step: 5080 loss:1.837046 acc:0.791667\n",
      "train epoch:32 step: 5100 loss:1.772820 acc:0.875000\n",
      "train epoch:32 step: 5120 loss:1.909229 acc:0.718750\n",
      "train epoch:32 step: 5140 loss:1.832272 acc:0.802083\n",
      "train epoch:32 step: 5160 loss:1.873804 acc:0.750000\n",
      "train epoch:32 step: 5180 loss:1.773164 acc:0.833333\n",
      "train epoch:33 step: 5200 loss:1.863856 acc:0.750000\n",
      "train epoch:33 step: 5220 loss:1.815222 acc:0.802083\n",
      "train epoch:33 step: 5240 loss:1.842568 acc:0.791667\n",
      "train epoch:33 step: 5260 loss:1.847339 acc:0.781250\n",
      "train epoch:33 step: 5280 loss:1.847980 acc:0.781250\n",
      "train epoch:33 step: 5300 loss:1.842945 acc:0.791667\n",
      "train epoch:33 step: 5320 loss:1.899461 acc:0.718750\n",
      "train epoch:34 step: 5340 loss:1.816696 acc:0.822917\n",
      "train epoch:34 step: 5360 loss:1.878047 acc:0.750000\n",
      "train epoch:34 step: 5380 loss:1.853166 acc:0.791667\n",
      "train epoch:34 step: 5400 loss:1.855311 acc:0.781250\n",
      "train epoch:34 step: 5420 loss:1.853879 acc:0.770833\n",
      "train epoch:34 step: 5440 loss:1.805161 acc:0.822917\n",
      "train epoch:34 step: 5460 loss:1.786386 acc:0.843750\n",
      "train epoch:34 step: 5480 loss:1.747292 acc:0.895833\n",
      "train epoch:35 step: 5500 loss:1.787597 acc:0.843750\n",
      "eval epoch:35 step: 5500 loss:1.787597 acc:0.843750\n",
      "saving the best_model...\n",
      "train epoch:35 step: 5520 loss:1.914655 acc:0.718750\n",
      "train epoch:35 step: 5540 loss:1.855982 acc:0.781250\n",
      "train epoch:35 step: 5560 loss:1.817497 acc:0.812500\n",
      "train epoch:35 step: 5580 loss:1.804701 acc:0.822917\n",
      "train epoch:35 step: 5600 loss:1.761003 acc:0.864583\n",
      "train epoch:35 step: 5620 loss:1.854451 acc:0.770833\n",
      "train epoch:35 step: 5640 loss:1.768195 acc:0.875000\n",
      "train epoch:36 step: 5660 loss:1.846791 acc:0.770833\n",
      "train epoch:36 step: 5680 loss:1.757262 acc:0.875000\n",
      "train epoch:36 step: 5700 loss:1.784731 acc:0.854167\n",
      "train epoch:36 step: 5720 loss:1.842951 acc:0.791667\n",
      "train epoch:36 step: 5740 loss:1.708320 acc:0.927083\n",
      "train epoch:36 step: 5760 loss:1.732998 acc:0.895833\n",
      "train epoch:36 step: 5780 loss:1.807146 acc:0.833333\n",
      "train epoch:36 step: 5800 loss:1.750504 acc:0.885417\n",
      "train epoch:37 step: 5820 loss:1.786636 acc:0.843750\n",
      "train epoch:37 step: 5840 loss:1.725746 acc:0.916667\n",
      "train epoch:37 step: 5860 loss:1.819281 acc:0.812500\n",
      "train epoch:37 step: 5880 loss:1.762005 acc:0.864583\n",
      "train epoch:37 step: 5900 loss:1.759803 acc:0.864583\n",
      "train epoch:37 step: 5920 loss:1.787134 acc:0.854167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:37 step: 5940 loss:1.838856 acc:0.781250\n",
      "train epoch:37 step: 5960 loss:1.774894 acc:0.854167\n",
      "train epoch:38 step: 5980 loss:1.713140 acc:0.916667\n",
      "train epoch:38 step: 6000 loss:1.735639 acc:0.885417\n",
      "eval epoch:38 step: 6000 loss:1.735639 acc:0.885417\n",
      "saving the best_model...\n",
      "train epoch:38 step: 6020 loss:1.829078 acc:0.791667\n",
      "train epoch:38 step: 6040 loss:1.742466 acc:0.885417\n",
      "train epoch:38 step: 6060 loss:1.792089 acc:0.833333\n",
      "train epoch:38 step: 6080 loss:1.728374 acc:0.895833\n",
      "train epoch:38 step: 6100 loss:1.739469 acc:0.885417\n",
      "train epoch:38 step: 6120 loss:1.727779 acc:0.906250\n",
      "train epoch:39 step: 6140 loss:1.767244 acc:0.864583\n",
      "train epoch:39 step: 6160 loss:1.806533 acc:0.822917\n",
      "train epoch:39 step: 6180 loss:1.800849 acc:0.812500\n",
      "train epoch:39 step: 6200 loss:1.792199 acc:0.833333\n",
      "train epoch:39 step: 6220 loss:1.751119 acc:0.875000\n",
      "train epoch:39 step: 6240 loss:1.767042 acc:0.864583\n",
      "train epoch:39 step: 6260 loss:1.811072 acc:0.802083\n",
      "train epoch:40 step: 6280 loss:1.763893 acc:0.854167\n",
      "train epoch:40 step: 6300 loss:1.756940 acc:0.875000\n",
      "train epoch:40 step: 6320 loss:1.720572 acc:0.916667\n",
      "train epoch:40 step: 6340 loss:1.753378 acc:0.875000\n",
      "train epoch:40 step: 6360 loss:1.734458 acc:0.895833\n",
      "train epoch:40 step: 6380 loss:1.704588 acc:0.927083\n",
      "train epoch:40 step: 6400 loss:1.758442 acc:0.875000\n",
      "train epoch:40 step: 6420 loss:1.703081 acc:0.927083\n",
      "train epoch:41 step: 6440 loss:1.721910 acc:0.906250\n",
      "train epoch:41 step: 6460 loss:1.725080 acc:0.906250\n",
      "train epoch:41 step: 6480 loss:1.776157 acc:0.843750\n",
      "train epoch:41 step: 6500 loss:1.699357 acc:0.937500\n",
      "eval epoch:41 step: 6500 loss:1.699357 acc:0.937500\n",
      "saving the best_model...\n",
      "train epoch:41 step: 6520 loss:1.689338 acc:0.947917\n",
      "train epoch:41 step: 6540 loss:1.711526 acc:0.927083\n",
      "train epoch:41 step: 6560 loss:1.710179 acc:0.916667\n",
      "train epoch:41 step: 6580 loss:1.776953 acc:0.864583\n",
      "train epoch:42 step: 6600 loss:1.680637 acc:0.947917\n",
      "train epoch:42 step: 6620 loss:1.703135 acc:0.927083\n",
      "train epoch:42 step: 6640 loss:1.678732 acc:0.947917\n",
      "train epoch:42 step: 6660 loss:1.699903 acc:0.927083\n",
      "train epoch:42 step: 6680 loss:1.756843 acc:0.864583\n",
      "train epoch:42 step: 6700 loss:1.688249 acc:0.937500\n",
      "train epoch:42 step: 6720 loss:1.727921 acc:0.906250\n",
      "train epoch:42 step: 6740 loss:1.717356 acc:0.906250\n",
      "train epoch:43 step: 6760 loss:1.676343 acc:0.947917\n",
      "train epoch:43 step: 6780 loss:1.706854 acc:0.916667\n",
      "train epoch:43 step: 6800 loss:1.698127 acc:0.927083\n",
      "train epoch:43 step: 6820 loss:1.734393 acc:0.885417\n",
      "train epoch:43 step: 6840 loss:1.706470 acc:0.916667\n",
      "train epoch:43 step: 6860 loss:1.771488 acc:0.854167\n",
      "train epoch:43 step: 6880 loss:1.685545 acc:0.937500\n",
      "train epoch:43 step: 6900 loss:1.695774 acc:0.927083\n",
      "train epoch:44 step: 6920 loss:1.698433 acc:0.927083\n",
      "train epoch:44 step: 6940 loss:1.710221 acc:0.916667\n",
      "train epoch:44 step: 6960 loss:1.678343 acc:0.937500\n",
      "train epoch:44 step: 6980 loss:1.721115 acc:0.895833\n",
      "train epoch:44 step: 7000 loss:1.744931 acc:0.885417\n",
      "eval epoch:44 step: 7000 loss:1.744931 acc:0.885417\n",
      "train epoch:44 step: 7020 loss:1.722362 acc:0.895833\n",
      "train epoch:44 step: 7040 loss:1.675185 acc:0.947917\n",
      "train epoch:44 step: 7060 loss:1.698189 acc:0.927083\n",
      "train epoch:45 step: 7080 loss:1.655320 acc:0.968750\n",
      "train epoch:45 step: 7100 loss:1.685136 acc:0.937500\n",
      "train epoch:45 step: 7120 loss:1.719491 acc:0.906250\n",
      "train epoch:45 step: 7140 loss:1.713243 acc:0.906250\n",
      "train epoch:45 step: 7160 loss:1.712076 acc:0.916667\n",
      "train epoch:45 step: 7180 loss:1.737097 acc:0.885417\n",
      "train epoch:45 step: 7200 loss:1.723296 acc:0.895833\n",
      "train epoch:45 step: 7220 loss:1.701303 acc:0.927083\n",
      "train epoch:46 step: 7240 loss:1.695802 acc:0.927083\n",
      "train epoch:46 step: 7260 loss:1.746888 acc:0.875000\n",
      "train epoch:46 step: 7280 loss:1.677187 acc:0.947917\n",
      "train epoch:46 step: 7300 loss:1.688021 acc:0.937500\n",
      "train epoch:46 step: 7320 loss:1.704053 acc:0.916667\n",
      "train epoch:46 step: 7340 loss:1.729342 acc:0.895833\n",
      "train epoch:46 step: 7360 loss:1.708441 acc:0.916667\n",
      "train epoch:47 step: 7380 loss:1.664373 acc:0.958333\n",
      "train epoch:47 step: 7400 loss:1.722467 acc:0.906250\n",
      "train epoch:47 step: 7420 loss:1.721721 acc:0.906250\n",
      "train epoch:47 step: 7440 loss:1.682008 acc:0.937500\n",
      "train epoch:47 step: 7460 loss:1.707411 acc:0.916667\n",
      "train epoch:47 step: 7480 loss:1.745040 acc:0.885417\n",
      "train epoch:47 step: 7500 loss:1.739508 acc:0.885417\n",
      "eval epoch:47 step: 7500 loss:1.739508 acc:0.885417\n",
      "train epoch:47 step: 7520 loss:1.686621 acc:0.937500\n",
      "train epoch:48 step: 7540 loss:1.690094 acc:0.937500\n",
      "train epoch:48 step: 7560 loss:1.757222 acc:0.864583\n",
      "train epoch:48 step: 7580 loss:1.702661 acc:0.916667\n",
      "train epoch:48 step: 7600 loss:1.692265 acc:0.937500\n",
      "train epoch:48 step: 7620 loss:1.706642 acc:0.916667\n",
      "train epoch:48 step: 7640 loss:1.667327 acc:0.958333\n",
      "train epoch:48 step: 7660 loss:1.728747 acc:0.895833\n",
      "train epoch:48 step: 7680 loss:1.708892 acc:0.916667\n",
      "train epoch:49 step: 7700 loss:1.684725 acc:0.937500\n",
      "train epoch:49 step: 7720 loss:1.662407 acc:0.958333\n",
      "train epoch:49 step: 7740 loss:1.743829 acc:0.875000\n",
      "train epoch:49 step: 7760 loss:1.675900 acc:0.947917\n",
      "train epoch:49 step: 7780 loss:1.742835 acc:0.875000\n",
      "train epoch:49 step: 7800 loss:1.686143 acc:0.937500\n",
      "train epoch:49 step: 7820 loss:1.682511 acc:0.947917\n",
      "train epoch:49 step: 7840 loss:1.772743 acc:0.854167\n",
      "train epoch:50 step: 7860 loss:1.713971 acc:0.906250\n",
      "train epoch:50 step: 7880 loss:1.674701 acc:0.947917\n",
      "train epoch:50 step: 7900 loss:1.731981 acc:0.885417\n",
      "train epoch:50 step: 7920 loss:1.750866 acc:0.864583\n",
      "train epoch:50 step: 7940 loss:1.720560 acc:0.895833\n",
      "train epoch:50 step: 7960 loss:1.718036 acc:0.906250\n",
      "train epoch:50 step: 7980 loss:1.743015 acc:0.875000\n",
      "train epoch:50 step: 8000 loss:1.699111 acc:0.927083\n",
      "eval epoch:50 step: 8000 loss:1.699111 acc:0.927083\n",
      "train epoch:51 step: 8020 loss:1.781316 acc:0.843750\n",
      "train epoch:51 step: 8040 loss:1.682717 acc:0.947917\n",
      "train epoch:51 step: 8060 loss:1.706998 acc:0.916667\n",
      "train epoch:51 step: 8080 loss:1.685619 acc:0.937500\n",
      "train epoch:51 step: 8100 loss:1.718269 acc:0.906250\n",
      "train epoch:51 step: 8120 loss:1.713018 acc:0.906250\n",
      "train epoch:51 step: 8140 loss:1.683339 acc:0.937500\n",
      "train epoch:51 step: 8160 loss:1.687933 acc:0.937500\n",
      "train epoch:52 step: 8180 loss:1.662568 acc:0.958333\n",
      "train epoch:52 step: 8200 loss:1.674335 acc:0.947917\n",
      "train epoch:52 step: 8220 loss:1.685917 acc:0.937500\n",
      "train epoch:52 step: 8240 loss:1.662190 acc:0.958333\n",
      "train epoch:52 step: 8260 loss:1.670617 acc:0.947917\n",
      "train epoch:52 step: 8280 loss:1.714937 acc:0.906250\n",
      "train epoch:52 step: 8300 loss:1.726413 acc:0.895833\n",
      "train epoch:52 step: 8320 loss:1.662920 acc:0.958333\n",
      "train epoch:53 step: 8340 loss:1.692341 acc:0.927083\n",
      "train epoch:53 step: 8360 loss:1.676841 acc:0.947917\n",
      "train epoch:53 step: 8380 loss:1.717515 acc:0.906250\n",
      "train epoch:53 step: 8400 loss:1.665146 acc:0.958333\n",
      "train epoch:53 step: 8420 loss:1.682372 acc:0.937500\n",
      "train epoch:53 step: 8440 loss:1.671950 acc:0.947917\n",
      "train epoch:53 step: 8460 loss:1.674090 acc:0.947917\n",
      "train epoch:54 step: 8480 loss:1.640743 acc:0.979167\n",
      "train epoch:54 step: 8500 loss:1.671646 acc:0.947917\n",
      "eval epoch:54 step: 8500 loss:1.671646 acc:0.947917\n",
      "saving the best_model...\n",
      "train epoch:54 step: 8520 loss:1.672210 acc:0.947917\n",
      "train epoch:54 step: 8540 loss:1.691804 acc:0.927083\n",
      "train epoch:54 step: 8560 loss:1.685773 acc:0.937500\n",
      "train epoch:54 step: 8580 loss:1.734488 acc:0.885417\n",
      "train epoch:54 step: 8600 loss:1.672238 acc:0.947917\n",
      "train epoch:54 step: 8620 loss:1.660150 acc:0.958333\n",
      "train epoch:55 step: 8640 loss:1.660998 acc:0.958333\n",
      "train epoch:55 step: 8660 loss:1.693785 acc:0.927083\n",
      "train epoch:55 step: 8680 loss:1.725034 acc:0.895833\n",
      "train epoch:55 step: 8700 loss:1.665881 acc:0.958333\n",
      "train epoch:55 step: 8720 loss:1.673984 acc:0.947917\n",
      "train epoch:55 step: 8740 loss:1.676748 acc:0.947917\n",
      "train epoch:55 step: 8760 loss:1.630332 acc:1.000000\n",
      "train epoch:55 step: 8780 loss:1.724629 acc:0.895833\n",
      "train epoch:56 step: 8800 loss:1.687040 acc:0.937500\n",
      "train epoch:56 step: 8820 loss:1.672550 acc:0.947917\n",
      "train epoch:56 step: 8840 loss:1.706945 acc:0.916667\n",
      "train epoch:56 step: 8860 loss:1.671512 acc:0.947917\n",
      "train epoch:56 step: 8880 loss:1.665781 acc:0.947917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:56 step: 8900 loss:1.664687 acc:0.958333\n",
      "train epoch:56 step: 8920 loss:1.716503 acc:0.906250\n",
      "train epoch:56 step: 8940 loss:1.715070 acc:0.906250\n",
      "train epoch:57 step: 8960 loss:1.725543 acc:0.895833\n",
      "train epoch:57 step: 8980 loss:1.709907 acc:0.916667\n",
      "train epoch:57 step: 9000 loss:1.704064 acc:0.916667\n",
      "eval epoch:57 step: 9000 loss:1.704064 acc:0.916667\n",
      "train epoch:57 step: 9020 loss:1.651235 acc:0.968750\n",
      "train epoch:57 step: 9040 loss:1.685288 acc:0.937500\n",
      "train epoch:57 step: 9060 loss:1.719383 acc:0.906250\n",
      "train epoch:57 step: 9080 loss:1.713328 acc:0.906250\n",
      "train epoch:57 step: 9100 loss:1.699857 acc:0.927083\n",
      "train epoch:58 step: 9120 loss:1.665797 acc:0.958333\n",
      "train epoch:58 step: 9140 loss:1.727639 acc:0.885417\n",
      "train epoch:58 step: 9160 loss:1.686109 acc:0.937500\n",
      "train epoch:58 step: 9180 loss:1.689479 acc:0.927083\n",
      "train epoch:58 step: 9200 loss:1.716828 acc:0.916667\n",
      "train epoch:58 step: 9220 loss:1.664574 acc:0.958333\n",
      "train epoch:58 step: 9240 loss:1.644176 acc:0.968750\n",
      "train epoch:58 step: 9260 loss:1.685727 acc:0.937500\n",
      "train epoch:59 step: 9280 loss:1.691627 acc:0.937500\n",
      "train epoch:59 step: 9300 loss:1.660759 acc:0.958333\n",
      "train epoch:59 step: 9320 loss:1.693995 acc:0.937500\n",
      "train epoch:59 step: 9340 loss:1.660668 acc:0.968750\n",
      "train epoch:59 step: 9360 loss:1.638177 acc:0.989583\n",
      "train epoch:59 step: 9380 loss:1.688485 acc:0.937500\n",
      "train epoch:59 step: 9400 loss:1.687237 acc:0.937500\n",
      "train epoch:60 step: 9420 loss:1.627050 acc:1.000000\n",
      "train epoch:60 step: 9440 loss:1.676215 acc:0.947917\n",
      "train epoch:60 step: 9460 loss:1.643111 acc:0.979167\n",
      "train epoch:60 step: 9480 loss:1.665295 acc:0.958333\n",
      "train epoch:60 step: 9500 loss:1.661926 acc:0.958333\n",
      "eval epoch:60 step: 9500 loss:1.661926 acc:0.958333\n",
      "saving the best_model...\n",
      "train epoch:60 step: 9520 loss:1.646451 acc:0.979167\n",
      "train epoch:60 step: 9540 loss:1.677394 acc:0.947917\n",
      "train epoch:60 step: 9560 loss:1.663986 acc:0.958333\n",
      "train epoch:61 step: 9580 loss:1.680244 acc:0.937500\n",
      "train epoch:61 step: 9600 loss:1.652907 acc:0.968750\n",
      "train epoch:61 step: 9620 loss:1.621558 acc:1.000000\n",
      "train epoch:61 step: 9640 loss:1.663688 acc:0.958333\n",
      "train epoch:61 step: 9660 loss:1.674750 acc:0.947917\n",
      "train epoch:61 step: 9680 loss:1.672523 acc:0.947917\n",
      "train epoch:61 step: 9700 loss:1.683020 acc:0.937500\n",
      "train epoch:61 step: 9720 loss:1.641549 acc:0.979167\n",
      "train epoch:62 step: 9740 loss:1.642323 acc:0.979167\n",
      "train epoch:62 step: 9760 loss:1.675545 acc:0.947917\n",
      "train epoch:62 step: 9780 loss:1.651190 acc:0.968750\n",
      "train epoch:62 step: 9800 loss:1.652454 acc:0.968750\n",
      "train epoch:62 step: 9820 loss:1.650330 acc:0.979167\n",
      "train epoch:62 step: 9840 loss:1.684278 acc:0.937500\n",
      "train epoch:62 step: 9860 loss:1.707782 acc:0.916667\n",
      "train epoch:62 step: 9880 loss:1.655692 acc:0.968750\n",
      "train epoch:63 step: 9900 loss:1.645842 acc:0.979167\n",
      "train epoch:63 step: 9920 loss:1.755362 acc:0.864583\n",
      "train epoch:63 step: 9940 loss:1.694230 acc:0.927083\n",
      "train epoch:63 step: 9960 loss:1.680562 acc:0.947917\n",
      "train epoch:63 step: 9980 loss:1.665169 acc:0.958333\n",
      "train epoch:63 step: 10000 loss:1.659734 acc:0.968750\n",
      "eval epoch:63 step: 10000 loss:1.659734 acc:0.968750\n",
      "saving the best_model...\n",
      "train epoch:63 step: 10020 loss:1.672481 acc:0.937500\n",
      "train epoch:63 step: 10040 loss:1.696283 acc:0.927083\n",
      "train epoch:64 step: 10060 loss:1.646010 acc:0.979167\n",
      "train epoch:64 step: 10080 loss:1.657201 acc:0.968750\n",
      "train epoch:64 step: 10100 loss:1.633065 acc:0.989583\n",
      "train epoch:64 step: 10120 loss:1.663837 acc:0.958333\n",
      "train epoch:64 step: 10140 loss:1.655299 acc:0.968750\n",
      "train epoch:64 step: 10160 loss:1.648464 acc:0.968750\n",
      "train epoch:64 step: 10180 loss:1.644418 acc:0.979167\n",
      "train epoch:64 step: 10200 loss:1.641523 acc:0.979167\n",
      "train epoch:65 step: 10220 loss:1.641957 acc:0.979167\n",
      "train epoch:65 step: 10240 loss:1.677983 acc:0.937500\n",
      "train epoch:65 step: 10260 loss:1.661706 acc:0.958333\n",
      "train epoch:65 step: 10280 loss:1.671750 acc:0.947917\n",
      "train epoch:65 step: 10300 loss:1.671739 acc:0.947917\n",
      "train epoch:65 step: 10320 loss:1.653263 acc:0.968750\n",
      "train epoch:65 step: 10340 loss:1.665279 acc:0.958333\n",
      "train epoch:65 step: 10360 loss:1.650931 acc:0.968750\n",
      "train epoch:66 step: 10380 loss:1.622165 acc:1.000000\n",
      "train epoch:66 step: 10400 loss:1.655997 acc:0.968750\n",
      "train epoch:66 step: 10420 loss:1.652243 acc:0.968750\n",
      "train epoch:66 step: 10440 loss:1.620242 acc:1.000000\n",
      "train epoch:66 step: 10460 loss:1.662298 acc:0.958333\n",
      "train epoch:66 step: 10480 loss:1.690640 acc:0.927083\n",
      "train epoch:66 step: 10500 loss:1.630896 acc:0.989583\n",
      "eval epoch:66 step: 10500 loss:1.630896 acc:0.989583\n",
      "saving the best_model...\n",
      "train epoch:67 step: 10520 loss:1.671875 acc:0.947917\n",
      "train epoch:67 step: 10540 loss:1.662153 acc:0.958333\n",
      "train epoch:67 step: 10560 loss:1.633473 acc:0.989583\n",
      "train epoch:67 step: 10580 loss:1.664607 acc:0.958333\n",
      "train epoch:67 step: 10600 loss:1.707079 acc:0.916667\n",
      "train epoch:67 step: 10620 loss:1.658511 acc:0.958333\n",
      "train epoch:67 step: 10640 loss:1.672405 acc:0.947917\n",
      "train epoch:67 step: 10660 loss:1.664855 acc:0.958333\n",
      "train epoch:68 step: 10680 loss:1.641132 acc:0.979167\n",
      "train epoch:68 step: 10700 loss:1.625341 acc:1.000000\n",
      "train epoch:68 step: 10720 loss:1.640517 acc:0.979167\n",
      "train epoch:68 step: 10740 loss:1.640747 acc:0.979167\n",
      "train epoch:68 step: 10760 loss:1.640318 acc:0.979167\n",
      "train epoch:68 step: 10780 loss:1.653298 acc:0.968750\n",
      "train epoch:68 step: 10800 loss:1.693775 acc:0.927083\n",
      "train epoch:68 step: 10820 loss:1.641052 acc:0.979167\n",
      "train epoch:69 step: 10840 loss:1.631477 acc:0.989583\n",
      "train epoch:69 step: 10860 loss:1.693244 acc:0.927083\n",
      "train epoch:69 step: 10880 loss:1.650689 acc:0.968750\n",
      "train epoch:69 step: 10900 loss:1.662910 acc:0.958333\n",
      "train epoch:69 step: 10920 loss:1.640877 acc:0.979167\n",
      "train epoch:69 step: 10940 loss:1.661625 acc:0.958333\n",
      "train epoch:69 step: 10960 loss:1.619946 acc:1.000000\n",
      "train epoch:69 step: 10980 loss:1.650783 acc:0.968750\n",
      "train epoch:70 step: 11000 loss:1.692350 acc:0.927083\n",
      "eval epoch:70 step: 11000 loss:1.692350 acc:0.927083\n",
      "train epoch:70 step: 11020 loss:1.674029 acc:0.947917\n",
      "train epoch:70 step: 11040 loss:1.675419 acc:0.947917\n",
      "train epoch:70 step: 11060 loss:1.664384 acc:0.958333\n",
      "train epoch:70 step: 11080 loss:1.662385 acc:0.958333\n",
      "train epoch:70 step: 11100 loss:1.650933 acc:0.979167\n",
      "train epoch:70 step: 11120 loss:1.672246 acc:0.947917\n",
      "train epoch:70 step: 11140 loss:1.689597 acc:0.927083\n",
      "train epoch:71 step: 11160 loss:1.665406 acc:0.958333\n",
      "train epoch:71 step: 11180 loss:1.682075 acc:0.937500\n",
      "train epoch:71 step: 11200 loss:1.680730 acc:0.947917\n",
      "train epoch:71 step: 11220 loss:1.641763 acc:0.979167\n",
      "train epoch:71 step: 11240 loss:1.643787 acc:0.979167\n",
      "train epoch:71 step: 11260 loss:1.641637 acc:0.979167\n",
      "train epoch:71 step: 11280 loss:1.691685 acc:0.927083\n",
      "train epoch:71 step: 11300 loss:1.641302 acc:0.979167\n",
      "train epoch:72 step: 11320 loss:1.650521 acc:0.968750\n",
      "train epoch:72 step: 11340 loss:1.640395 acc:0.979167\n",
      "train epoch:72 step: 11360 loss:1.651480 acc:0.968750\n",
      "train epoch:72 step: 11380 loss:1.671575 acc:0.947917\n",
      "train epoch:72 step: 11400 loss:1.643651 acc:0.979167\n",
      "train epoch:72 step: 11420 loss:1.654799 acc:0.968750\n",
      "train epoch:72 step: 11440 loss:1.669794 acc:0.947917\n",
      "train epoch:72 step: 11460 loss:1.653653 acc:0.958333\n",
      "train epoch:73 step: 11480 loss:1.648937 acc:0.979167\n",
      "train epoch:73 step: 11500 loss:1.671931 acc:0.947917\n",
      "eval epoch:73 step: 11500 loss:1.671931 acc:0.947917\n",
      "train epoch:73 step: 11520 loss:1.728330 acc:0.895833\n",
      "train epoch:73 step: 11540 loss:1.705629 acc:0.916667\n",
      "train epoch:73 step: 11560 loss:1.689243 acc:0.937500\n",
      "train epoch:73 step: 11580 loss:1.670142 acc:0.947917\n",
      "train epoch:73 step: 11600 loss:1.676121 acc:0.947917\n",
      "train epoch:74 step: 11620 loss:1.660069 acc:0.968750\n",
      "train epoch:74 step: 11640 loss:1.661476 acc:0.958333\n",
      "train epoch:74 step: 11660 loss:1.674201 acc:0.947917\n",
      "train epoch:74 step: 11680 loss:1.662988 acc:0.958333\n",
      "train epoch:74 step: 11700 loss:1.652873 acc:0.968750\n",
      "train epoch:74 step: 11720 loss:1.651821 acc:0.968750\n",
      "train epoch:74 step: 11740 loss:1.641352 acc:0.979167\n",
      "train epoch:74 step: 11760 loss:1.645972 acc:0.979167\n",
      "train epoch:75 step: 11780 loss:1.690593 acc:0.927083\n",
      "train epoch:75 step: 11800 loss:1.644697 acc:0.979167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:75 step: 11820 loss:1.659688 acc:0.968750\n",
      "train epoch:75 step: 11840 loss:1.663348 acc:0.958333\n",
      "train epoch:75 step: 11860 loss:1.682247 acc:0.937500\n",
      "train epoch:75 step: 11880 loss:1.641298 acc:0.979167\n",
      "train epoch:75 step: 11900 loss:1.689550 acc:0.927083\n",
      "train epoch:75 step: 11920 loss:1.689580 acc:0.927083\n",
      "train epoch:76 step: 11940 loss:1.649229 acc:0.979167\n",
      "train epoch:76 step: 11960 loss:1.633778 acc:0.989583\n",
      "train epoch:76 step: 11980 loss:1.697053 acc:0.927083\n",
      "train epoch:76 step: 12000 loss:1.660124 acc:0.958333\n",
      "eval epoch:76 step: 12000 loss:1.660124 acc:0.958333\n",
      "train epoch:76 step: 12020 loss:1.690063 acc:0.927083\n",
      "train epoch:76 step: 12040 loss:1.663207 acc:0.958333\n",
      "train epoch:76 step: 12060 loss:1.651356 acc:0.968750\n",
      "train epoch:76 step: 12080 loss:1.671807 acc:0.947917\n",
      "train epoch:77 step: 12100 loss:1.621480 acc:1.000000\n",
      "train epoch:77 step: 12120 loss:1.640068 acc:0.979167\n",
      "train epoch:77 step: 12140 loss:1.669735 acc:0.947917\n",
      "train epoch:77 step: 12160 loss:1.651429 acc:0.968750\n",
      "train epoch:77 step: 12180 loss:1.671438 acc:0.947917\n",
      "train epoch:77 step: 12200 loss:1.681912 acc:0.937500\n",
      "train epoch:77 step: 12220 loss:1.650569 acc:0.968750\n",
      "train epoch:77 step: 12240 loss:1.661033 acc:0.958333\n",
      "train epoch:78 step: 12260 loss:1.650580 acc:0.968750\n",
      "train epoch:78 step: 12280 loss:1.660455 acc:0.958333\n",
      "train epoch:78 step: 12300 loss:1.744037 acc:0.875000\n",
      "train epoch:78 step: 12320 loss:1.639937 acc:0.979167\n",
      "train epoch:78 step: 12340 loss:1.671018 acc:0.947917\n",
      "train epoch:78 step: 12360 loss:1.619204 acc:1.000000\n",
      "train epoch:78 step: 12380 loss:1.650297 acc:0.968750\n",
      "train epoch:78 step: 12400 loss:1.639815 acc:0.979167\n",
      "train epoch:79 step: 12420 loss:1.649987 acc:0.968750\n",
      "train epoch:79 step: 12440 loss:1.640068 acc:0.979167\n",
      "train epoch:79 step: 12460 loss:1.660709 acc:0.958333\n",
      "train epoch:79 step: 12480 loss:1.650330 acc:0.968750\n",
      "train epoch:79 step: 12500 loss:1.681580 acc:0.937500\n",
      "eval epoch:79 step: 12500 loss:1.681580 acc:0.937500\n",
      "train epoch:79 step: 12520 loss:1.650266 acc:0.968750\n",
      "train epoch:79 step: 12540 loss:1.629550 acc:0.989583\n",
      "train epoch:80 step: 12560 loss:1.670960 acc:0.947917\n",
      "train epoch:80 step: 12580 loss:1.660662 acc:0.958333\n",
      "train epoch:80 step: 12600 loss:1.629004 acc:0.989583\n",
      "train epoch:80 step: 12620 loss:1.660707 acc:0.958333\n",
      "train epoch:80 step: 12640 loss:1.671081 acc:0.947917\n",
      "train epoch:80 step: 12660 loss:1.670709 acc:0.947917\n",
      "train epoch:80 step: 12680 loss:1.671971 acc:0.947917\n",
      "train epoch:80 step: 12700 loss:1.660681 acc:0.958333\n",
      "train epoch:81 step: 12720 loss:1.639618 acc:0.979167\n",
      "train epoch:81 step: 12740 loss:1.691927 acc:0.927083\n",
      "train epoch:81 step: 12760 loss:1.650342 acc:0.968750\n",
      "train epoch:81 step: 12780 loss:1.671097 acc:0.947917\n",
      "train epoch:81 step: 12800 loss:1.681403 acc:0.937500\n",
      "train epoch:81 step: 12820 loss:1.660630 acc:0.958333\n",
      "train epoch:81 step: 12840 loss:1.660573 acc:0.958333\n",
      "train epoch:81 step: 12860 loss:1.629455 acc:0.989583\n",
      "train epoch:82 step: 12880 loss:1.669348 acc:0.947917\n",
      "train epoch:82 step: 12900 loss:1.650258 acc:0.968750\n",
      "train epoch:82 step: 12920 loss:1.691697 acc:0.927083\n",
      "train epoch:82 step: 12940 loss:1.650533 acc:0.968750\n",
      "train epoch:82 step: 12960 loss:1.650169 acc:0.968750\n",
      "train epoch:82 step: 12980 loss:1.640656 acc:0.979167\n",
      "train epoch:82 step: 13000 loss:1.660755 acc:0.958333\n",
      "eval epoch:82 step: 13000 loss:1.660755 acc:0.958333\n",
      "train epoch:82 step: 13020 loss:1.660608 acc:0.958333\n",
      "train epoch:83 step: 13040 loss:1.650291 acc:0.968750\n",
      "train epoch:83 step: 13060 loss:1.639777 acc:0.979167\n",
      "train epoch:83 step: 13080 loss:1.670951 acc:0.947917\n",
      "train epoch:83 step: 13100 loss:1.704286 acc:0.916667\n",
      "train epoch:83 step: 13120 loss:1.668504 acc:0.958333\n",
      "train epoch:83 step: 13140 loss:1.654268 acc:0.968750\n",
      "train epoch:83 step: 13160 loss:1.681631 acc:0.947917\n",
      "train epoch:83 step: 13180 loss:1.735847 acc:0.885417\n",
      "train epoch:84 step: 13200 loss:1.766571 acc:0.864583\n",
      "train epoch:84 step: 13220 loss:1.755720 acc:0.864583\n",
      "train epoch:84 step: 13240 loss:1.693625 acc:0.937500\n",
      "train epoch:84 step: 13260 loss:1.696257 acc:0.927083\n",
      "train epoch:84 step: 13280 loss:1.709067 acc:0.906250\n",
      "train epoch:84 step: 13300 loss:1.660899 acc:0.958333\n",
      "train epoch:84 step: 13320 loss:1.627684 acc:0.989583\n",
      "train epoch:84 step: 13340 loss:1.651978 acc:0.968750\n",
      "train epoch:85 step: 13360 loss:1.688012 acc:0.927083\n",
      "train epoch:85 step: 13380 loss:1.672359 acc:0.947917\n",
      "train epoch:85 step: 13400 loss:1.669870 acc:0.958333\n",
      "train epoch:85 step: 13420 loss:1.694387 acc:0.927083\n",
      "train epoch:85 step: 13440 loss:1.660880 acc:0.958333\n",
      "train epoch:85 step: 13460 loss:1.662939 acc:0.958333\n",
      "train epoch:85 step: 13480 loss:1.655427 acc:0.968750\n",
      "train epoch:85 step: 13500 loss:1.665995 acc:0.947917\n",
      "eval epoch:85 step: 13500 loss:1.665995 acc:0.947917\n",
      "train epoch:86 step: 13520 loss:1.706960 acc:0.916667\n",
      "train epoch:86 step: 13540 loss:1.661991 acc:0.958333\n",
      "train epoch:86 step: 13560 loss:1.650805 acc:0.968750\n",
      "train epoch:86 step: 13580 loss:1.708166 acc:0.916667\n",
      "train epoch:86 step: 13600 loss:1.652923 acc:0.968750\n",
      "train epoch:86 step: 13620 loss:1.671117 acc:0.947917\n",
      "train epoch:86 step: 13640 loss:1.640745 acc:0.979167\n",
      "train epoch:87 step: 13660 loss:1.651495 acc:0.968750\n",
      "train epoch:87 step: 13680 loss:1.677415 acc:0.947917\n",
      "train epoch:87 step: 13700 loss:1.686822 acc:0.937500\n",
      "train epoch:87 step: 13720 loss:1.662189 acc:0.958333\n",
      "train epoch:87 step: 13740 loss:1.660906 acc:0.958333\n",
      "train epoch:87 step: 13760 loss:1.638703 acc:0.979167\n",
      "train epoch:87 step: 13780 loss:1.630411 acc:0.989583\n",
      "train epoch:87 step: 13800 loss:1.695093 acc:0.927083\n",
      "train epoch:88 step: 13820 loss:1.667390 acc:0.958333\n",
      "train epoch:88 step: 13840 loss:1.739085 acc:0.885417\n",
      "train epoch:88 step: 13860 loss:1.687005 acc:0.937500\n",
      "train epoch:88 step: 13880 loss:1.652871 acc:0.968750\n",
      "train epoch:88 step: 13900 loss:1.684011 acc:0.937500\n",
      "train epoch:88 step: 13920 loss:1.685186 acc:0.937500\n",
      "train epoch:88 step: 13940 loss:1.665892 acc:0.958333\n",
      "train epoch:88 step: 13960 loss:1.662694 acc:0.958333\n",
      "train epoch:89 step: 13980 loss:1.659109 acc:0.958333\n",
      "train epoch:89 step: 14000 loss:1.640299 acc:0.979167\n",
      "eval epoch:89 step: 14000 loss:1.640299 acc:0.979167\n",
      "train epoch:89 step: 14020 loss:1.631090 acc:0.989583\n",
      "train epoch:89 step: 14040 loss:1.645123 acc:0.979167\n",
      "train epoch:89 step: 14060 loss:1.655656 acc:0.958333\n",
      "train epoch:89 step: 14080 loss:1.671554 acc:0.947917\n",
      "train epoch:89 step: 14100 loss:1.640792 acc:0.979167\n",
      "train epoch:89 step: 14120 loss:1.672864 acc:0.947917\n",
      "train epoch:90 step: 14140 loss:1.676685 acc:0.937500\n",
      "train epoch:90 step: 14160 loss:1.660870 acc:0.958333\n",
      "train epoch:90 step: 14180 loss:1.681775 acc:0.937500\n",
      "train epoch:90 step: 14200 loss:1.650248 acc:0.968750\n",
      "train epoch:90 step: 14220 loss:1.660678 acc:0.958333\n",
      "train epoch:90 step: 14240 loss:1.650312 acc:0.968750\n",
      "train epoch:90 step: 14260 loss:1.643529 acc:0.979167\n",
      "train epoch:90 step: 14280 loss:1.661717 acc:0.958333\n",
      "train epoch:91 step: 14300 loss:1.639702 acc:0.979167\n",
      "train epoch:91 step: 14320 loss:1.681823 acc:0.937500\n",
      "train epoch:91 step: 14340 loss:1.671482 acc:0.947917\n",
      "train epoch:91 step: 14360 loss:1.650649 acc:0.968750\n",
      "train epoch:91 step: 14380 loss:1.660038 acc:0.958333\n",
      "train epoch:91 step: 14400 loss:1.671111 acc:0.947917\n",
      "train epoch:91 step: 14420 loss:1.661101 acc:0.958333\n",
      "train epoch:91 step: 14440 loss:1.652809 acc:0.968750\n",
      "train epoch:92 step: 14460 loss:1.639894 acc:0.979167\n",
      "train epoch:92 step: 14480 loss:1.639986 acc:0.979167\n",
      "train epoch:92 step: 14500 loss:1.681533 acc:0.937500\n",
      "eval epoch:92 step: 14500 loss:1.681533 acc:0.937500\n",
      "train epoch:92 step: 14520 loss:1.629519 acc:0.989583\n",
      "train epoch:92 step: 14540 loss:1.671236 acc:0.947917\n",
      "train epoch:92 step: 14560 loss:1.681943 acc:0.937500\n",
      "train epoch:92 step: 14580 loss:1.651436 acc:0.968750\n",
      "train epoch:92 step: 14600 loss:1.661168 acc:0.958333\n",
      "train epoch:93 step: 14620 loss:1.640145 acc:0.979167\n",
      "train epoch:93 step: 14640 loss:1.662786 acc:0.958333\n",
      "train epoch:93 step: 14660 loss:1.651040 acc:0.968750\n",
      "train epoch:93 step: 14680 loss:1.660586 acc:0.958333\n",
      "train epoch:93 step: 14700 loss:1.694048 acc:0.927083\n",
      "train epoch:93 step: 14720 loss:1.671629 acc:0.947917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:93 step: 14740 loss:1.619510 acc:1.000000\n",
      "train epoch:94 step: 14760 loss:1.629374 acc:0.989583\n",
      "train epoch:94 step: 14780 loss:1.681427 acc:0.937500\n",
      "train epoch:94 step: 14800 loss:1.629266 acc:0.989583\n",
      "train epoch:94 step: 14820 loss:1.649955 acc:0.968750\n",
      "train epoch:94 step: 14840 loss:1.650119 acc:0.968750\n",
      "train epoch:94 step: 14860 loss:1.660808 acc:0.958333\n",
      "train epoch:94 step: 14880 loss:1.631774 acc:0.989583\n",
      "train epoch:94 step: 14900 loss:1.652067 acc:0.968750\n",
      "train epoch:95 step: 14920 loss:1.641314 acc:0.979167\n",
      "train epoch:95 step: 14940 loss:1.618897 acc:1.000000\n",
      "train epoch:95 step: 14960 loss:1.686483 acc:0.937500\n",
      "train epoch:95 step: 14980 loss:1.661042 acc:0.958333\n",
      "train epoch:95 step: 15000 loss:1.681458 acc:0.937500\n",
      "eval epoch:95 step: 15000 loss:1.681458 acc:0.937500\n",
      "train epoch:95 step: 15020 loss:1.639850 acc:0.979167\n",
      "train epoch:95 step: 15040 loss:1.629479 acc:0.989583\n",
      "train epoch:95 step: 15060 loss:1.664315 acc:0.958333\n",
      "train epoch:96 step: 15080 loss:1.651141 acc:0.968750\n",
      "train epoch:96 step: 15100 loss:1.650502 acc:0.968750\n",
      "train epoch:96 step: 15120 loss:1.650134 acc:0.968750\n",
      "train epoch:96 step: 15140 loss:1.662865 acc:0.958333\n",
      "train epoch:96 step: 15160 loss:1.671691 acc:0.947917\n",
      "train epoch:96 step: 15180 loss:1.671108 acc:0.947917\n",
      "train epoch:96 step: 15200 loss:1.697799 acc:0.916667\n",
      "train epoch:96 step: 15220 loss:1.640722 acc:0.979167\n",
      "train epoch:97 step: 15240 loss:1.653933 acc:0.968750\n",
      "train epoch:97 step: 15260 loss:1.640806 acc:0.979167\n",
      "train epoch:97 step: 15280 loss:1.651504 acc:0.968750\n",
      "train epoch:97 step: 15300 loss:1.687556 acc:0.937500\n",
      "train epoch:97 step: 15320 loss:1.679619 acc:0.937500\n",
      "train epoch:97 step: 15340 loss:1.650310 acc:0.958333\n",
      "train epoch:97 step: 15360 loss:1.656501 acc:0.968750\n",
      "train epoch:97 step: 15380 loss:1.659984 acc:0.958333\n",
      "train epoch:98 step: 15400 loss:1.651608 acc:0.968750\n",
      "train epoch:98 step: 15420 loss:1.651711 acc:0.968750\n",
      "train epoch:98 step: 15440 loss:1.644721 acc:0.979167\n",
      "train epoch:98 step: 15460 loss:1.643446 acc:0.979167\n",
      "train epoch:98 step: 15480 loss:1.646584 acc:0.979167\n",
      "train epoch:98 step: 15500 loss:1.672586 acc:0.947917\n",
      "eval epoch:98 step: 15500 loss:1.672586 acc:0.947917\n",
      "train epoch:98 step: 15520 loss:1.639123 acc:0.979167\n",
      "train epoch:98 step: 15540 loss:1.684344 acc:0.937500\n",
      "train epoch:99 step: 15560 loss:1.654390 acc:0.968750\n",
      "train epoch:99 step: 15580 loss:1.708344 acc:0.906250\n",
      "train epoch:99 step: 15600 loss:1.683800 acc:0.937500\n",
      "train epoch:99 step: 15620 loss:1.653290 acc:0.968750\n",
      "train epoch:99 step: 15640 loss:1.687016 acc:0.937500\n",
      "train epoch:99 step: 15660 loss:1.661176 acc:0.958333\n",
      "train epoch:99 step: 15680 loss:1.671942 acc:0.947917\n",
      "train epoch:100 step: 15700 loss:1.671684 acc:0.947917\n",
      "train epoch:100 step: 15720 loss:1.643749 acc:0.979167\n",
      "train epoch:100 step: 15740 loss:1.652981 acc:0.968750\n",
      "train epoch:100 step: 15760 loss:1.633682 acc:0.989583\n",
      "train epoch:100 step: 15780 loss:1.641235 acc:0.979167\n",
      "train epoch:100 step: 15800 loss:1.684095 acc:0.937500\n",
      "train epoch:100 step: 15820 loss:1.677884 acc:0.937500\n",
      "train epoch:100 step: 15840 loss:1.670887 acc:0.947917\n",
      "train epoch:101 step: 15860 loss:1.647007 acc:0.968750\n",
      "train epoch:101 step: 15880 loss:1.681369 acc:0.937500\n",
      "train epoch:101 step: 15900 loss:1.639841 acc:0.979167\n",
      "train epoch:101 step: 15920 loss:1.661855 acc:0.958333\n",
      "train epoch:101 step: 15940 loss:1.668936 acc:0.947917\n",
      "train epoch:101 step: 15960 loss:1.660324 acc:0.958333\n",
      "train epoch:101 step: 15980 loss:1.664249 acc:0.958333\n",
      "train epoch:101 step: 16000 loss:1.641908 acc:0.979167\n",
      "eval epoch:101 step: 16000 loss:1.641908 acc:0.979167\n",
      "train epoch:102 step: 16020 loss:1.681588 acc:0.937500\n",
      "train epoch:102 step: 16040 loss:1.671034 acc:0.947917\n",
      "train epoch:102 step: 16060 loss:1.639893 acc:0.979167\n",
      "train epoch:102 step: 16080 loss:1.660577 acc:0.958333\n",
      "train epoch:102 step: 16100 loss:1.650443 acc:0.968750\n",
      "train epoch:102 step: 16120 loss:1.660545 acc:0.958333\n",
      "train epoch:102 step: 16140 loss:1.671062 acc:0.947917\n",
      "train epoch:102 step: 16160 loss:1.629369 acc:0.989583\n",
      "train epoch:103 step: 16180 loss:1.641394 acc:0.979167\n",
      "train epoch:103 step: 16200 loss:1.650054 acc:0.968750\n",
      "train epoch:103 step: 16220 loss:1.660877 acc:0.958333\n",
      "train epoch:103 step: 16240 loss:1.640003 acc:0.979167\n",
      "train epoch:103 step: 16260 loss:1.629352 acc:0.989583\n",
      "train epoch:103 step: 16280 loss:1.629823 acc:0.989583\n",
      "train epoch:103 step: 16300 loss:1.650155 acc:0.968750\n",
      "train epoch:103 step: 16320 loss:1.660593 acc:0.958333\n",
      "train epoch:104 step: 16340 loss:1.639864 acc:0.979167\n",
      "train epoch:104 step: 16360 loss:1.619020 acc:1.000000\n",
      "train epoch:104 step: 16380 loss:1.650135 acc:0.968750\n",
      "train epoch:104 step: 16400 loss:1.639702 acc:0.979167\n",
      "train epoch:104 step: 16420 loss:1.629229 acc:0.989583\n",
      "train epoch:104 step: 16440 loss:1.671011 acc:0.947917\n",
      "train epoch:104 step: 16460 loss:1.629328 acc:0.989583\n",
      "train epoch:104 step: 16480 loss:1.671091 acc:0.947917\n",
      "train epoch:105 step: 16500 loss:1.711818 acc:0.906250\n",
      "eval epoch:105 step: 16500 loss:1.711818 acc:0.906250\n",
      "train epoch:105 step: 16520 loss:1.645477 acc:0.968750\n",
      "train epoch:105 step: 16540 loss:1.673209 acc:0.947917\n",
      "train epoch:105 step: 16560 loss:1.660840 acc:0.958333\n",
      "train epoch:105 step: 16580 loss:1.642944 acc:0.979167\n",
      "train epoch:105 step: 16600 loss:1.650367 acc:0.968750\n",
      "train epoch:105 step: 16620 loss:1.630330 acc:0.989583\n",
      "train epoch:105 step: 16640 loss:1.640171 acc:0.979167\n",
      "train epoch:106 step: 16660 loss:1.650542 acc:0.968750\n",
      "train epoch:106 step: 16680 loss:1.673377 acc:0.947917\n",
      "train epoch:106 step: 16700 loss:1.643271 acc:0.979167\n",
      "train epoch:106 step: 16720 loss:1.653545 acc:0.968750\n",
      "train epoch:106 step: 16740 loss:1.652314 acc:0.968750\n",
      "train epoch:106 step: 16760 loss:1.683234 acc:0.937500\n",
      "train epoch:106 step: 16780 loss:1.652202 acc:0.968750\n",
      "train epoch:107 step: 16800 loss:1.630094 acc:0.989583\n",
      "train epoch:107 step: 16820 loss:1.649809 acc:0.968750\n",
      "train epoch:107 step: 16840 loss:1.693906 acc:0.927083\n",
      "train epoch:107 step: 16860 loss:1.681924 acc:0.937500\n",
      "train epoch:107 step: 16880 loss:1.632048 acc:0.989583\n",
      "train epoch:107 step: 16900 loss:1.640082 acc:0.979167\n",
      "train epoch:107 step: 16920 loss:1.639869 acc:0.979167\n",
      "train epoch:107 step: 16940 loss:1.687421 acc:0.927083\n",
      "train epoch:108 step: 16960 loss:1.682777 acc:0.937500\n",
      "train epoch:108 step: 16980 loss:1.650631 acc:0.968750\n",
      "train epoch:108 step: 17000 loss:1.639908 acc:0.979167\n",
      "eval epoch:108 step: 17000 loss:1.639908 acc:0.979167\n",
      "train epoch:108 step: 17020 loss:1.650354 acc:0.968750\n",
      "train epoch:108 step: 17040 loss:1.644802 acc:0.979167\n",
      "train epoch:108 step: 17060 loss:1.680959 acc:0.937500\n",
      "train epoch:108 step: 17080 loss:1.662337 acc:0.958333\n",
      "train epoch:108 step: 17100 loss:1.672084 acc:0.947917\n",
      "train epoch:109 step: 17120 loss:1.635774 acc:0.979167\n",
      "train epoch:109 step: 17140 loss:1.656083 acc:0.968750\n",
      "train epoch:109 step: 17160 loss:1.619439 acc:1.000000\n",
      "train epoch:109 step: 17180 loss:1.648614 acc:0.968750\n",
      "train epoch:109 step: 17200 loss:1.652114 acc:0.968750\n",
      "train epoch:109 step: 17220 loss:1.640297 acc:0.979167\n",
      "train epoch:109 step: 17240 loss:1.653118 acc:0.968750\n",
      "train epoch:109 step: 17260 loss:1.664437 acc:0.958333\n",
      "train epoch:110 step: 17280 loss:1.657776 acc:0.958333\n",
      "train epoch:110 step: 17300 loss:1.664853 acc:0.958333\n",
      "train epoch:110 step: 17320 loss:1.659905 acc:0.958333\n",
      "train epoch:110 step: 17340 loss:1.705887 acc:0.916667\n",
      "train epoch:110 step: 17360 loss:1.689667 acc:0.927083\n",
      "train epoch:110 step: 17380 loss:1.682890 acc:0.937500\n",
      "train epoch:110 step: 17400 loss:1.654974 acc:0.958333\n",
      "train epoch:110 step: 17420 loss:1.663586 acc:0.958333\n",
      "train epoch:111 step: 17440 loss:1.687097 acc:0.927083\n",
      "train epoch:111 step: 17460 loss:1.661132 acc:0.958333\n",
      "train epoch:111 step: 17480 loss:1.652520 acc:0.968750\n",
      "train epoch:111 step: 17500 loss:1.661211 acc:0.958333\n",
      "eval epoch:111 step: 17500 loss:1.661211 acc:0.958333\n",
      "train epoch:111 step: 17520 loss:1.650353 acc:0.968750\n",
      "train epoch:111 step: 17540 loss:1.678587 acc:0.947917\n",
      "train epoch:111 step: 17560 loss:1.671518 acc:0.947917\n",
      "train epoch:111 step: 17580 loss:1.630109 acc:0.989583\n",
      "train epoch:112 step: 17600 loss:1.692622 acc:0.927083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:112 step: 17620 loss:1.651245 acc:0.968750\n",
      "train epoch:112 step: 17640 loss:1.636205 acc:0.979167\n",
      "train epoch:112 step: 17660 loss:1.670953 acc:0.947917\n",
      "train epoch:112 step: 17680 loss:1.702054 acc:0.916667\n",
      "train epoch:112 step: 17700 loss:1.683097 acc:0.937500\n",
      "train epoch:112 step: 17720 loss:1.681103 acc:0.937500\n",
      "train epoch:112 step: 17740 loss:1.660480 acc:0.958333\n",
      "train epoch:113 step: 17760 loss:1.650308 acc:0.968750\n",
      "train epoch:113 step: 17780 loss:1.629344 acc:0.989583\n",
      "train epoch:113 step: 17800 loss:1.660249 acc:0.958333\n",
      "train epoch:113 step: 17820 loss:1.650166 acc:0.968750\n",
      "train epoch:113 step: 17840 loss:1.650404 acc:0.968750\n",
      "train epoch:113 step: 17860 loss:1.649828 acc:0.968750\n",
      "train epoch:113 step: 17880 loss:1.670738 acc:0.947917\n",
      "train epoch:114 step: 17900 loss:1.650108 acc:0.968750\n",
      "train epoch:114 step: 17920 loss:1.629312 acc:0.989583\n",
      "train epoch:114 step: 17940 loss:1.660625 acc:0.958333\n",
      "train epoch:114 step: 17960 loss:1.680867 acc:0.937500\n",
      "train epoch:114 step: 17980 loss:1.639377 acc:0.979167\n",
      "train epoch:114 step: 18000 loss:1.639692 acc:0.979167\n",
      "eval epoch:114 step: 18000 loss:1.639692 acc:0.979167\n",
      "train epoch:114 step: 18020 loss:1.639688 acc:0.979167\n",
      "train epoch:114 step: 18040 loss:1.629269 acc:0.989583\n",
      "train epoch:115 step: 18060 loss:1.691713 acc:0.927083\n",
      "train epoch:115 step: 18080 loss:1.670939 acc:0.947917\n",
      "train epoch:115 step: 18100 loss:1.670921 acc:0.947917\n",
      "train epoch:115 step: 18120 loss:1.639356 acc:0.979167\n",
      "train epoch:115 step: 18140 loss:1.639661 acc:0.979167\n",
      "train epoch:115 step: 18160 loss:1.639652 acc:0.979167\n",
      "train epoch:115 step: 18180 loss:1.639682 acc:0.979167\n",
      "train epoch:115 step: 18200 loss:1.660501 acc:0.958333\n",
      "train epoch:116 step: 18220 loss:1.670901 acc:0.947917\n",
      "train epoch:116 step: 18240 loss:1.639659 acc:0.979167\n",
      "train epoch:116 step: 18260 loss:1.649656 acc:0.968750\n",
      "train epoch:116 step: 18280 loss:1.639647 acc:0.979167\n",
      "train epoch:116 step: 18300 loss:1.670703 acc:0.947917\n",
      "train epoch:116 step: 18320 loss:1.629218 acc:0.989583\n",
      "train epoch:116 step: 18340 loss:1.670870 acc:0.947917\n",
      "train epoch:116 step: 18360 loss:1.691782 acc:0.927083\n",
      "train epoch:117 step: 18380 loss:1.670867 acc:0.947917\n",
      "train epoch:117 step: 18400 loss:1.670925 acc:0.947917\n",
      "train epoch:117 step: 18420 loss:1.650077 acc:0.968750\n",
      "train epoch:117 step: 18440 loss:1.681291 acc:0.937500\n",
      "train epoch:117 step: 18460 loss:1.639364 acc:0.979167\n",
      "train epoch:117 step: 18480 loss:1.650073 acc:0.968750\n",
      "train epoch:117 step: 18500 loss:1.660158 acc:0.958333\n",
      "eval epoch:117 step: 18500 loss:1.660158 acc:0.958333\n",
      "train epoch:117 step: 18520 loss:1.629210 acc:0.989583\n",
      "train epoch:118 step: 18540 loss:1.618835 acc:1.000000\n",
      "train epoch:118 step: 18560 loss:1.660496 acc:0.958333\n",
      "train epoch:118 step: 18580 loss:1.639637 acc:0.979167\n",
      "train epoch:118 step: 18600 loss:1.629212 acc:0.989583\n",
      "train epoch:118 step: 18620 loss:1.639663 acc:0.979167\n",
      "train epoch:118 step: 18640 loss:1.618867 acc:1.000000\n",
      "train epoch:118 step: 18660 loss:1.670262 acc:0.947917\n",
      "train epoch:118 step: 18680 loss:1.639651 acc:0.979167\n",
      "train epoch:119 step: 18700 loss:1.691761 acc:0.927083\n",
      "train epoch:119 step: 18720 loss:1.650072 acc:0.968750\n",
      "train epoch:119 step: 18740 loss:1.650074 acc:0.968750\n",
      "train epoch:119 step: 18760 loss:1.670901 acc:0.947917\n",
      "train epoch:119 step: 18780 loss:1.660134 acc:0.958333\n",
      "train epoch:119 step: 18800 loss:1.660325 acc:0.958333\n",
      "train epoch:119 step: 18820 loss:1.660521 acc:0.958333\n",
      "train epoch:120 step: 18840 loss:1.660511 acc:0.958333\n",
      "train epoch:120 step: 18860 loss:1.629196 acc:0.989583\n",
      "train epoch:120 step: 18880 loss:1.691528 acc:0.927083\n",
      "train epoch:120 step: 18900 loss:1.661005 acc:0.958333\n",
      "train epoch:120 step: 18920 loss:1.629209 acc:0.989583\n",
      "train epoch:120 step: 18940 loss:1.650060 acc:0.968750\n",
      "train epoch:120 step: 18960 loss:1.670926 acc:0.947917\n",
      "train epoch:120 step: 18980 loss:1.660122 acc:0.958333\n",
      "train epoch:121 step: 19000 loss:1.639687 acc:0.979167\n",
      "eval epoch:121 step: 19000 loss:1.639687 acc:0.979167\n",
      "train epoch:121 step: 19020 loss:1.639641 acc:0.979167\n",
      "train epoch:121 step: 19040 loss:1.681249 acc:0.937500\n",
      "train epoch:121 step: 19060 loss:1.650118 acc:0.968750\n",
      "train epoch:121 step: 19080 loss:1.680961 acc:0.937500\n",
      "train epoch:121 step: 19100 loss:1.629240 acc:0.989583\n",
      "train epoch:121 step: 19120 loss:1.660245 acc:0.958333\n",
      "train epoch:121 step: 19140 loss:1.618786 acc:1.000000\n",
      "train epoch:122 step: 19160 loss:1.670861 acc:0.947917\n",
      "train epoch:122 step: 19180 loss:1.670879 acc:0.947917\n",
      "train epoch:122 step: 19200 loss:1.639557 acc:0.979167\n",
      "train epoch:122 step: 19220 loss:1.670927 acc:0.947917\n",
      "train epoch:122 step: 19240 loss:1.670875 acc:0.947917\n",
      "train epoch:122 step: 19260 loss:1.618819 acc:1.000000\n",
      "train epoch:122 step: 19280 loss:1.629235 acc:0.989583\n",
      "train epoch:122 step: 19300 loss:1.650039 acc:0.968750\n",
      "train epoch:123 step: 19320 loss:1.649802 acc:0.968750\n",
      "train epoch:123 step: 19340 loss:1.702114 acc:0.916667\n",
      "train epoch:123 step: 19360 loss:1.629214 acc:0.989583\n",
      "train epoch:123 step: 19380 loss:1.670702 acc:0.947917\n",
      "train epoch:123 step: 19400 loss:1.639314 acc:0.979167\n",
      "train epoch:123 step: 19420 loss:1.650027 acc:0.968750\n",
      "train epoch:123 step: 19440 loss:1.629182 acc:0.989583\n",
      "train epoch:123 step: 19460 loss:1.680957 acc:0.937500\n",
      "train epoch:124 step: 19480 loss:1.649529 acc:0.968750\n",
      "train epoch:124 step: 19500 loss:1.639578 acc:0.979167\n",
      "eval epoch:124 step: 19500 loss:1.639578 acc:0.979167\n",
      "train epoch:124 step: 19520 loss:1.639617 acc:0.979167\n",
      "train epoch:124 step: 19540 loss:1.639627 acc:0.979167\n",
      "train epoch:124 step: 19560 loss:1.639302 acc:0.979167\n",
      "train epoch:124 step: 19580 loss:1.691739 acc:0.927083\n",
      "train epoch:124 step: 19600 loss:1.639619 acc:0.979167\n",
      "train epoch:124 step: 19620 loss:1.639620 acc:0.979167\n",
      "train epoch:125 step: 19640 loss:1.649889 acc:0.968750\n",
      "train epoch:125 step: 19660 loss:1.639418 acc:0.979167\n",
      "train epoch:125 step: 19680 loss:1.670741 acc:0.947917\n",
      "train epoch:125 step: 19700 loss:1.660434 acc:0.958333\n",
      "train epoch:125 step: 19720 loss:1.639602 acc:0.979167\n",
      "train epoch:125 step: 19740 loss:1.660104 acc:0.958333\n",
      "train epoch:125 step: 19760 loss:1.650034 acc:0.968750\n",
      "train epoch:125 step: 19780 loss:1.629200 acc:0.989583\n",
      "train epoch:126 step: 19800 loss:1.639393 acc:0.979167\n",
      "train epoch:126 step: 19820 loss:1.659508 acc:0.958333\n",
      "train epoch:126 step: 19840 loss:1.660440 acc:0.958333\n",
      "train epoch:126 step: 19860 loss:1.618769 acc:1.000000\n",
      "train epoch:126 step: 19880 loss:1.650112 acc:0.968750\n",
      "train epoch:126 step: 19900 loss:1.649722 acc:0.968750\n",
      "train epoch:126 step: 19920 loss:1.639623 acc:0.979167\n",
      "train epoch:127 step: 19940 loss:1.639627 acc:0.979167\n",
      "train epoch:127 step: 19960 loss:1.649660 acc:0.968750\n",
      "train epoch:127 step: 19980 loss:1.680606 acc:0.937500\n",
      "train epoch:127 step: 20000 loss:1.639617 acc:0.979167\n",
      "eval epoch:127 step: 20000 loss:1.639617 acc:0.979167\n",
      "train epoch:127 step: 20020 loss:1.649765 acc:0.968750\n",
      "train epoch:127 step: 20040 loss:1.670567 acc:0.947917\n",
      "train epoch:127 step: 20060 loss:1.639612 acc:0.979167\n",
      "train epoch:127 step: 20080 loss:1.639789 acc:0.979167\n",
      "train epoch:128 step: 20100 loss:1.649711 acc:0.968750\n",
      "train epoch:128 step: 20120 loss:1.660438 acc:0.958333\n",
      "train epoch:128 step: 20140 loss:1.670871 acc:0.947917\n",
      "train epoch:128 step: 20160 loss:1.639444 acc:0.979167\n",
      "train epoch:128 step: 20180 loss:1.639669 acc:0.979167\n",
      "train epoch:128 step: 20200 loss:1.639630 acc:0.979167\n",
      "train epoch:128 step: 20220 loss:1.670900 acc:0.947917\n",
      "train epoch:128 step: 20240 loss:1.680867 acc:0.937500\n",
      "train epoch:129 step: 20260 loss:1.629198 acc:0.989583\n",
      "train epoch:129 step: 20280 loss:1.639624 acc:0.979167\n",
      "train epoch:129 step: 20300 loss:1.670878 acc:0.947917\n",
      "train epoch:129 step: 20320 loss:1.629210 acc:0.989583\n",
      "train epoch:129 step: 20340 loss:1.659975 acc:0.958333\n",
      "train epoch:129 step: 20360 loss:1.629224 acc:0.989583\n",
      "train epoch:129 step: 20380 loss:1.639625 acc:0.979167\n",
      "train epoch:129 step: 20400 loss:1.670850 acc:0.947917\n",
      "train epoch:130 step: 20420 loss:1.649372 acc:0.968750\n",
      "train epoch:130 step: 20440 loss:1.639317 acc:0.979167\n",
      "train epoch:130 step: 20460 loss:1.618780 acc:1.000000\n",
      "train epoch:130 step: 20480 loss:1.618781 acc:1.000000\n",
      "train epoch:130 step: 20500 loss:1.639301 acc:0.979167\n",
      "eval epoch:130 step: 20500 loss:1.639301 acc:0.979167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:130 step: 20520 loss:1.649656 acc:0.968750\n",
      "train epoch:130 step: 20540 loss:1.650017 acc:0.968750\n",
      "train epoch:130 step: 20560 loss:1.680748 acc:0.937500\n",
      "train epoch:131 step: 20580 loss:1.660443 acc:0.958333\n",
      "train epoch:131 step: 20600 loss:1.681270 acc:0.937500\n",
      "train epoch:131 step: 20620 loss:1.650050 acc:0.968750\n",
      "train epoch:131 step: 20640 loss:1.629179 acc:0.989583\n",
      "train epoch:131 step: 20660 loss:1.629203 acc:0.989583\n",
      "train epoch:131 step: 20680 loss:1.680635 acc:0.937500\n",
      "train epoch:131 step: 20700 loss:1.649745 acc:0.968750\n",
      "train epoch:131 step: 20720 loss:1.649654 acc:0.968750\n",
      "train epoch:132 step: 20740 loss:1.639623 acc:0.979167\n",
      "train epoch:132 step: 20760 loss:1.649382 acc:0.968750\n",
      "train epoch:132 step: 20780 loss:1.670857 acc:0.947917\n",
      "train epoch:132 step: 20800 loss:1.649679 acc:0.968750\n",
      "train epoch:132 step: 20820 loss:1.680942 acc:0.937500\n",
      "train epoch:132 step: 20840 loss:1.639284 acc:0.979167\n",
      "train epoch:132 step: 20860 loss:1.639615 acc:0.979167\n",
      "train epoch:132 step: 20880 loss:1.618755 acc:1.000000\n",
      "train epoch:133 step: 20900 loss:1.659770 acc:0.958333\n",
      "train epoch:133 step: 20920 loss:1.670192 acc:0.947917\n",
      "train epoch:133 step: 20940 loss:1.639351 acc:0.979167\n",
      "train epoch:133 step: 20960 loss:1.660069 acc:0.958333\n",
      "train epoch:133 step: 20980 loss:1.639601 acc:0.979167\n",
      "train epoch:133 step: 21000 loss:1.680628 acc:0.937500\n",
      "eval epoch:133 step: 21000 loss:1.680628 acc:0.937500\n",
      "train epoch:133 step: 21020 loss:1.660128 acc:0.958333\n",
      "train epoch:134 step: 21040 loss:1.639263 acc:0.979167\n",
      "train epoch:134 step: 21060 loss:1.691685 acc:0.927083\n",
      "train epoch:134 step: 21080 loss:1.628882 acc:0.989583\n",
      "train epoch:134 step: 21100 loss:1.649686 acc:0.968750\n",
      "train epoch:134 step: 21120 loss:1.650017 acc:0.968750\n",
      "train epoch:134 step: 21140 loss:1.629193 acc:0.989583\n",
      "train epoch:134 step: 21160 loss:1.629184 acc:0.989583\n",
      "train epoch:134 step: 21180 loss:1.650024 acc:0.968750\n",
      "train epoch:135 step: 21200 loss:1.639266 acc:0.979167\n",
      "train epoch:135 step: 21220 loss:1.659802 acc:0.958333\n",
      "train epoch:135 step: 21240 loss:1.639591 acc:0.979167\n",
      "train epoch:135 step: 21260 loss:1.639711 acc:0.979167\n",
      "train epoch:135 step: 21280 loss:1.670846 acc:0.947917\n",
      "train epoch:135 step: 21300 loss:1.639632 acc:0.979167\n",
      "train epoch:135 step: 21320 loss:1.649713 acc:0.968750\n",
      "train epoch:135 step: 21340 loss:1.670216 acc:0.947917\n",
      "train epoch:136 step: 21360 loss:1.639598 acc:0.979167\n",
      "train epoch:136 step: 21380 loss:1.680986 acc:0.937500\n",
      "train epoch:136 step: 21400 loss:1.660439 acc:0.958333\n",
      "train epoch:136 step: 21420 loss:1.650014 acc:0.968750\n",
      "train epoch:136 step: 21440 loss:1.639599 acc:0.979167\n",
      "train epoch:136 step: 21460 loss:1.670539 acc:0.947917\n",
      "train epoch:136 step: 21480 loss:1.639644 acc:0.979167\n",
      "train epoch:136 step: 21500 loss:1.639614 acc:0.979167\n",
      "eval epoch:136 step: 21500 loss:1.639614 acc:0.979167\n",
      "train epoch:137 step: 21520 loss:1.660428 acc:0.958333\n",
      "train epoch:137 step: 21540 loss:1.649281 acc:0.968750\n",
      "train epoch:137 step: 21560 loss:1.639266 acc:0.979167\n",
      "train epoch:137 step: 21580 loss:1.660418 acc:0.958333\n",
      "train epoch:137 step: 21600 loss:1.649677 acc:0.968750\n",
      "train epoch:137 step: 21620 loss:1.660129 acc:0.958333\n",
      "train epoch:137 step: 21640 loss:1.629172 acc:0.989583\n",
      "train epoch:137 step: 21660 loss:1.629174 acc:0.989583\n",
      "train epoch:138 step: 21680 loss:1.660130 acc:0.958333\n",
      "train epoch:138 step: 21700 loss:1.639274 acc:0.979167\n",
      "train epoch:138 step: 21720 loss:1.629177 acc:0.989583\n",
      "train epoch:138 step: 21740 loss:1.639601 acc:0.979167\n",
      "train epoch:138 step: 21760 loss:1.660432 acc:0.958333\n",
      "train epoch:138 step: 21780 loss:1.639271 acc:0.979167\n",
      "train epoch:138 step: 21800 loss:1.639593 acc:0.979167\n",
      "train epoch:138 step: 21820 loss:1.670598 acc:0.947917\n",
      "train epoch:139 step: 21840 loss:1.650016 acc:0.968750\n",
      "train epoch:139 step: 21860 loss:1.639599 acc:0.979167\n",
      "train epoch:139 step: 21880 loss:1.691671 acc:0.927083\n",
      "train epoch:139 step: 21900 loss:1.650011 acc:0.968750\n",
      "train epoch:139 step: 21920 loss:1.639358 acc:0.979167\n",
      "train epoch:139 step: 21940 loss:1.650003 acc:0.968750\n",
      "train epoch:139 step: 21960 loss:1.670855 acc:0.947917\n",
      "train epoch:140 step: 21980 loss:1.639583 acc:0.979167\n",
      "train epoch:140 step: 22000 loss:1.670516 acc:0.947917\n",
      "eval epoch:140 step: 22000 loss:1.670516 acc:0.947917\n",
      "train epoch:140 step: 22020 loss:1.670532 acc:0.947917\n",
      "train epoch:140 step: 22040 loss:1.649696 acc:0.968750\n",
      "train epoch:140 step: 22060 loss:1.639596 acc:0.979167\n",
      "train epoch:140 step: 22080 loss:1.649918 acc:0.968750\n",
      "train epoch:140 step: 22100 loss:1.670455 acc:0.947917\n",
      "train epoch:140 step: 22120 loss:1.618760 acc:1.000000\n",
      "train epoch:141 step: 22140 loss:1.650012 acc:0.968750\n",
      "train epoch:141 step: 22160 loss:1.650180 acc:0.968750\n",
      "train epoch:141 step: 22180 loss:1.639579 acc:0.979167\n",
      "train epoch:141 step: 22200 loss:1.650014 acc:0.968750\n",
      "train epoch:141 step: 22220 loss:1.649999 acc:0.968750\n",
      "train epoch:141 step: 22240 loss:1.670830 acc:0.947917\n",
      "train epoch:141 step: 22260 loss:1.650011 acc:0.968750\n",
      "train epoch:141 step: 22280 loss:1.629193 acc:0.989583\n",
      "train epoch:142 step: 22300 loss:1.670534 acc:0.947917\n",
      "train epoch:142 step: 22320 loss:1.650007 acc:0.968750\n",
      "train epoch:142 step: 22340 loss:1.629171 acc:0.989583\n",
      "train epoch:142 step: 22360 loss:1.639292 acc:0.979167\n",
      "train epoch:142 step: 22380 loss:1.639281 acc:0.979167\n",
      "train epoch:142 step: 22400 loss:1.650001 acc:0.968750\n",
      "train epoch:142 step: 22420 loss:1.639594 acc:0.979167\n",
      "train epoch:142 step: 22440 loss:1.650047 acc:0.968750\n",
      "train epoch:143 step: 22460 loss:1.680937 acc:0.937500\n",
      "train epoch:143 step: 22480 loss:1.649932 acc:0.968750\n",
      "train epoch:143 step: 22500 loss:1.660418 acc:0.958333\n",
      "eval epoch:143 step: 22500 loss:1.660418 acc:0.958333\n",
      "train epoch:143 step: 22520 loss:1.649717 acc:0.968750\n",
      "train epoch:143 step: 22540 loss:1.649591 acc:0.968750\n",
      "train epoch:143 step: 22560 loss:1.650004 acc:0.968750\n",
      "train epoch:143 step: 22580 loss:1.639591 acc:0.979167\n",
      "train epoch:143 step: 22600 loss:1.629169 acc:0.989583\n",
      "train epoch:144 step: 22620 loss:1.639703 acc:0.979167\n",
      "train epoch:144 step: 22640 loss:1.629170 acc:0.989583\n",
      "train epoch:144 step: 22660 loss:1.629188 acc:0.989583\n",
      "train epoch:144 step: 22680 loss:1.628848 acc:0.989583\n",
      "train epoch:144 step: 22700 loss:1.650010 acc:0.968750\n",
      "train epoch:144 step: 22720 loss:1.650269 acc:0.968750\n",
      "train epoch:144 step: 22740 loss:1.760630 acc:0.854167\n",
      "train epoch:144 step: 22760 loss:1.933173 acc:0.687500\n",
      "train epoch:145 step: 22780 loss:1.872345 acc:0.739583\n",
      "train epoch:145 step: 22800 loss:1.866794 acc:0.760417\n",
      "train epoch:145 step: 22820 loss:1.787548 acc:0.843750\n",
      "train epoch:145 step: 22840 loss:1.831931 acc:0.791667\n",
      "train epoch:145 step: 22860 loss:1.829332 acc:0.781250\n",
      "train epoch:145 step: 22880 loss:1.730837 acc:0.895833\n",
      "train epoch:145 step: 22900 loss:1.719308 acc:0.895833\n",
      "train epoch:145 step: 22920 loss:1.665512 acc:0.958333\n",
      "train epoch:146 step: 22940 loss:1.664138 acc:0.958333\n",
      "train epoch:146 step: 22960 loss:1.730603 acc:0.885417\n",
      "train epoch:146 step: 22980 loss:1.637626 acc:0.989583\n",
      "train epoch:146 step: 23000 loss:1.650650 acc:0.968750\n",
      "eval epoch:146 step: 23000 loss:1.650650 acc:0.968750\n",
      "train epoch:146 step: 23020 loss:1.648986 acc:0.968750\n",
      "train epoch:146 step: 23040 loss:1.669065 acc:0.947917\n",
      "train epoch:146 step: 23060 loss:1.645290 acc:0.979167\n",
      "train epoch:147 step: 23080 loss:1.642034 acc:0.979167\n",
      "train epoch:147 step: 23100 loss:1.650214 acc:0.968750\n",
      "train epoch:147 step: 23120 loss:1.640190 acc:0.979167\n",
      "train epoch:147 step: 23140 loss:1.668045 acc:0.947917\n",
      "train epoch:147 step: 23160 loss:1.673161 acc:0.947917\n",
      "train epoch:147 step: 23180 loss:1.641072 acc:0.979167\n",
      "train epoch:147 step: 23200 loss:1.679857 acc:0.937500\n",
      "train epoch:147 step: 23220 loss:1.640096 acc:0.979167\n",
      "train epoch:148 step: 23240 loss:1.700779 acc:0.916667\n",
      "train epoch:148 step: 23260 loss:1.650873 acc:0.968750\n",
      "train epoch:148 step: 23280 loss:1.660781 acc:0.958333\n",
      "train epoch:148 step: 23300 loss:1.660534 acc:0.958333\n",
      "train epoch:148 step: 23320 loss:1.670915 acc:0.947917\n",
      "train epoch:148 step: 23340 loss:1.649887 acc:0.968750\n",
      "train epoch:148 step: 23360 loss:1.639661 acc:0.979167\n",
      "train epoch:148 step: 23380 loss:1.639605 acc:0.979167\n",
      "train epoch:149 step: 23400 loss:1.650262 acc:0.968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:149 step: 23420 loss:1.650252 acc:0.968750\n",
      "train epoch:149 step: 23440 loss:1.660205 acc:0.958333\n",
      "train epoch:149 step: 23460 loss:1.647797 acc:0.968750\n",
      "train epoch:149 step: 23480 loss:1.619283 acc:1.000000\n",
      "train epoch:149 step: 23500 loss:1.681245 acc:0.937500\n",
      "eval epoch:149 step: 23500 loss:1.681245 acc:0.937500\n",
      "train epoch:149 step: 23520 loss:1.660530 acc:0.958333\n",
      "train epoch:149 step: 23540 loss:1.649601 acc:0.968750\n",
      "train epoch:150 step: 23560 loss:1.639684 acc:0.979167\n",
      "train epoch:150 step: 23580 loss:1.670917 acc:0.947917\n",
      "train epoch:150 step: 23600 loss:1.660176 acc:0.958333\n",
      "train epoch:150 step: 23620 loss:1.660618 acc:0.958333\n",
      "train epoch:150 step: 23640 loss:1.681171 acc:0.937500\n",
      "train epoch:150 step: 23660 loss:1.640629 acc:0.979167\n",
      "train epoch:150 step: 23680 loss:1.649564 acc:0.968750\n",
      "train epoch:150 step: 23700 loss:1.661741 acc:0.958333\n",
      "train epoch:151 step: 23720 loss:1.639711 acc:0.979167\n",
      "train epoch:151 step: 23740 loss:1.639324 acc:0.979167\n",
      "train epoch:151 step: 23760 loss:1.681023 acc:0.937500\n",
      "train epoch:151 step: 23780 loss:1.670739 acc:0.947917\n",
      "train epoch:151 step: 23800 loss:1.618902 acc:1.000000\n",
      "train epoch:151 step: 23820 loss:1.650275 acc:0.968750\n",
      "train epoch:151 step: 23840 loss:1.639875 acc:0.979167\n",
      "train epoch:151 step: 23860 loss:1.618871 acc:1.000000\n",
      "train epoch:152 step: 23880 loss:1.631070 acc:0.989583\n",
      "train epoch:152 step: 23900 loss:1.659765 acc:0.958333\n",
      "train epoch:152 step: 23920 loss:1.671245 acc:0.947917\n",
      "train epoch:152 step: 23940 loss:1.629241 acc:0.989583\n",
      "train epoch:152 step: 23960 loss:1.670940 acc:0.947917\n",
      "train epoch:152 step: 23980 loss:1.650611 acc:0.968750\n",
      "train epoch:152 step: 24000 loss:1.650189 acc:0.968750\n",
      "eval epoch:152 step: 24000 loss:1.650189 acc:0.968750\n",
      "train epoch:152 step: 24020 loss:1.618842 acc:1.000000\n",
      "train epoch:153 step: 24040 loss:1.660542 acc:0.958333\n",
      "train epoch:153 step: 24060 loss:1.701611 acc:0.916667\n",
      "train epoch:153 step: 24080 loss:1.681013 acc:0.937500\n",
      "train epoch:153 step: 24100 loss:1.670919 acc:0.947917\n",
      "train epoch:153 step: 24120 loss:1.639657 acc:0.979167\n",
      "train epoch:153 step: 24140 loss:1.649642 acc:0.968750\n",
      "train epoch:153 step: 24160 loss:1.639665 acc:0.979167\n",
      "train epoch:154 step: 24180 loss:1.670942 acc:0.947917\n",
      "train epoch:154 step: 24200 loss:1.639636 acc:0.979167\n",
      "train epoch:154 step: 24220 loss:1.639614 acc:0.979167\n",
      "train epoch:154 step: 24240 loss:1.670919 acc:0.947917\n",
      "train epoch:154 step: 24260 loss:1.650046 acc:0.968750\n",
      "train epoch:154 step: 24280 loss:1.629218 acc:0.989583\n",
      "train epoch:154 step: 24300 loss:1.671031 acc:0.947917\n",
      "train epoch:154 step: 24320 loss:1.681252 acc:0.937500\n",
      "train epoch:155 step: 24340 loss:1.629225 acc:0.989583\n",
      "train epoch:155 step: 24360 loss:1.639629 acc:0.979167\n",
      "train epoch:155 step: 24380 loss:1.650056 acc:0.968750\n",
      "train epoch:155 step: 24400 loss:1.680687 acc:0.937500\n",
      "train epoch:155 step: 24420 loss:1.670876 acc:0.947917\n",
      "train epoch:155 step: 24440 loss:1.670592 acc:0.947917\n",
      "train epoch:155 step: 24460 loss:1.639378 acc:0.979167\n",
      "train epoch:155 step: 24480 loss:1.659989 acc:0.958333\n",
      "train epoch:156 step: 24500 loss:1.660426 acc:0.958333\n",
      "eval epoch:156 step: 24500 loss:1.660426 acc:0.958333\n",
      "train epoch:156 step: 24520 loss:1.649397 acc:0.968750\n",
      "train epoch:156 step: 24540 loss:1.639274 acc:0.979167\n",
      "train epoch:156 step: 24560 loss:1.670573 acc:0.947917\n",
      "train epoch:156 step: 24580 loss:1.618793 acc:1.000000\n",
      "train epoch:156 step: 24600 loss:1.639290 acc:0.979167\n",
      "train epoch:156 step: 24620 loss:1.629207 acc:0.989583\n",
      "train epoch:156 step: 24640 loss:1.681273 acc:0.937500\n",
      "train epoch:157 step: 24660 loss:1.629205 acc:0.989583\n",
      "train epoch:157 step: 24680 loss:1.650028 acc:0.968750\n",
      "train epoch:157 step: 24700 loss:1.670599 acc:0.947917\n",
      "train epoch:157 step: 24720 loss:1.639635 acc:0.979167\n",
      "train epoch:157 step: 24740 loss:1.629208 acc:0.989583\n",
      "train epoch:157 step: 24760 loss:1.670553 acc:0.947917\n",
      "train epoch:157 step: 24780 loss:1.639214 acc:0.979167\n",
      "train epoch:157 step: 24800 loss:1.629203 acc:0.989583\n",
      "train epoch:158 step: 24820 loss:1.650030 acc:0.968750\n",
      "train epoch:158 step: 24840 loss:1.670560 acc:0.947917\n",
      "train epoch:158 step: 24860 loss:1.670884 acc:0.947917\n",
      "train epoch:158 step: 24880 loss:1.650042 acc:0.968750\n",
      "train epoch:158 step: 24900 loss:1.649667 acc:0.968750\n",
      "train epoch:158 step: 24920 loss:1.629250 acc:0.989583\n",
      "train epoch:158 step: 24940 loss:1.650025 acc:0.968750\n",
      "train epoch:158 step: 24960 loss:1.629232 acc:0.989583\n",
      "train epoch:159 step: 24980 loss:1.629195 acc:0.989583\n",
      "train epoch:159 step: 25000 loss:1.650032 acc:0.968750\n",
      "eval epoch:159 step: 25000 loss:1.650032 acc:0.968750\n",
      "train epoch:159 step: 25020 loss:1.670528 acc:0.947917\n",
      "train epoch:159 step: 25040 loss:1.639599 acc:0.979167\n",
      "train epoch:159 step: 25060 loss:1.701747 acc:0.916667\n",
      "train epoch:159 step: 25080 loss:1.639319 acc:0.979167\n",
      "train epoch:159 step: 25100 loss:1.650024 acc:0.968750\n",
      "train epoch:160 step: 25120 loss:1.649730 acc:0.968750\n",
      "train epoch:160 step: 25140 loss:1.691334 acc:0.927083\n",
      "train epoch:160 step: 25160 loss:1.659812 acc:0.958333\n",
      "train epoch:160 step: 25180 loss:1.650046 acc:0.968750\n",
      "train epoch:160 step: 25200 loss:1.639615 acc:0.979167\n",
      "train epoch:160 step: 25220 loss:1.639629 acc:0.979167\n",
      "train epoch:160 step: 25240 loss:1.639645 acc:0.979167\n",
      "train epoch:160 step: 25260 loss:1.639596 acc:0.979167\n",
      "train epoch:161 step: 25280 loss:1.629202 acc:0.989583\n",
      "train epoch:161 step: 25300 loss:1.660429 acc:0.958333\n",
      "train epoch:161 step: 25320 loss:1.660436 acc:0.958333\n",
      "train epoch:161 step: 25340 loss:1.650029 acc:0.968750\n",
      "train epoch:161 step: 25360 loss:1.650043 acc:0.968750\n",
      "train epoch:161 step: 25380 loss:1.629196 acc:0.989583\n",
      "train epoch:161 step: 25400 loss:1.629184 acc:0.989583\n",
      "train epoch:161 step: 25420 loss:1.660468 acc:0.958333\n",
      "train epoch:162 step: 25440 loss:1.629191 acc:0.989583\n",
      "train epoch:162 step: 25460 loss:1.629197 acc:0.989583\n",
      "train epoch:162 step: 25480 loss:1.628943 acc:0.989583\n",
      "train epoch:162 step: 25500 loss:1.639652 acc:0.979167\n",
      "eval epoch:162 step: 25500 loss:1.639652 acc:0.979167\n",
      "train epoch:162 step: 25520 loss:1.629187 acc:0.989583\n",
      "train epoch:162 step: 25540 loss:1.639286 acc:0.979167\n",
      "train epoch:162 step: 25560 loss:1.670501 acc:0.947917\n",
      "train epoch:162 step: 25580 loss:1.660116 acc:0.958333\n",
      "train epoch:163 step: 25600 loss:1.660437 acc:0.958333\n",
      "train epoch:163 step: 25620 loss:1.670541 acc:0.947917\n",
      "train epoch:163 step: 25640 loss:1.650032 acc:0.968750\n",
      "train epoch:163 step: 25660 loss:1.660049 acc:0.958333\n",
      "train epoch:163 step: 25680 loss:1.628793 acc:0.989583\n",
      "train epoch:163 step: 25700 loss:1.629172 acc:0.989583\n",
      "train epoch:163 step: 25720 loss:1.650022 acc:0.968750\n",
      "train epoch:163 step: 25740 loss:1.618795 acc:1.000000\n",
      "train epoch:164 step: 25760 loss:1.649355 acc:0.968750\n",
      "train epoch:164 step: 25780 loss:1.639607 acc:0.979167\n",
      "train epoch:164 step: 25800 loss:1.691690 acc:0.927083\n",
      "train epoch:164 step: 25820 loss:1.650009 acc:0.968750\n",
      "train epoch:164 step: 25840 loss:1.650052 acc:0.968750\n",
      "train epoch:164 step: 25860 loss:1.670456 acc:0.947917\n",
      "train epoch:164 step: 25880 loss:1.639617 acc:0.979167\n",
      "train epoch:164 step: 25900 loss:1.639600 acc:0.979167\n",
      "train epoch:165 step: 25920 loss:1.660441 acc:0.958333\n",
      "train epoch:165 step: 25940 loss:1.659788 acc:0.958333\n",
      "train epoch:165 step: 25960 loss:1.639275 acc:0.979167\n",
      "train epoch:165 step: 25980 loss:1.629191 acc:0.989583\n",
      "train epoch:165 step: 26000 loss:1.618807 acc:1.000000\n",
      "eval epoch:165 step: 26000 loss:1.618807 acc:1.000000\n",
      "saving the best_model...\n",
      "train epoch:165 step: 26020 loss:1.639593 acc:0.979167\n",
      "train epoch:165 step: 26040 loss:1.691359 acc:0.927083\n",
      "train epoch:165 step: 26060 loss:1.629192 acc:0.989583\n",
      "train epoch:166 step: 26080 loss:1.629185 acc:0.989583\n",
      "train epoch:166 step: 26100 loss:1.639591 acc:0.979167\n",
      "train epoch:166 step: 26120 loss:1.649944 acc:0.968750\n",
      "train epoch:166 step: 26140 loss:1.660433 acc:0.958333\n",
      "train epoch:166 step: 26160 loss:1.659795 acc:0.958333\n",
      "train epoch:166 step: 26180 loss:1.650030 acc:0.968750\n",
      "train epoch:166 step: 26200 loss:1.660425 acc:0.958333\n",
      "train epoch:167 step: 26220 loss:1.649606 acc:0.968750\n",
      "train epoch:167 step: 26240 loss:1.670511 acc:0.947917\n",
      "train epoch:167 step: 26260 loss:1.670843 acc:0.947917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:167 step: 26280 loss:1.670848 acc:0.947917\n",
      "train epoch:167 step: 26300 loss:1.639598 acc:0.979167\n",
      "train epoch:167 step: 26320 loss:1.639609 acc:0.979167\n",
      "train epoch:167 step: 26340 loss:1.660442 acc:0.958333\n",
      "train epoch:167 step: 26360 loss:1.681290 acc:0.937500\n",
      "train epoch:168 step: 26380 loss:1.649615 acc:0.968750\n",
      "train epoch:168 step: 26400 loss:1.681271 acc:0.937500\n",
      "train epoch:168 step: 26420 loss:1.670542 acc:0.947917\n",
      "train epoch:168 step: 26440 loss:1.691326 acc:0.927083\n",
      "train epoch:168 step: 26460 loss:1.629228 acc:0.989583\n",
      "train epoch:168 step: 26480 loss:1.649628 acc:0.968750\n",
      "train epoch:168 step: 26500 loss:1.618818 acc:1.000000\n",
      "eval epoch:168 step: 26500 loss:1.618818 acc:1.000000\n",
      "train epoch:168 step: 26520 loss:1.618778 acc:1.000000\n",
      "train epoch:169 step: 26540 loss:1.650016 acc:0.968750\n",
      "train epoch:169 step: 26560 loss:1.660429 acc:0.958333\n",
      "train epoch:169 step: 26580 loss:1.639184 acc:0.979167\n",
      "train epoch:169 step: 26600 loss:1.639188 acc:0.979167\n",
      "train epoch:169 step: 26620 loss:1.691033 acc:0.927083\n",
      "train epoch:169 step: 26640 loss:1.690568 acc:0.927083\n",
      "train epoch:169 step: 26660 loss:1.629188 acc:0.989583\n",
      "train epoch:169 step: 26680 loss:1.639274 acc:0.979167\n",
      "train epoch:170 step: 26700 loss:1.649692 acc:0.968750\n",
      "train epoch:170 step: 26720 loss:1.670629 acc:0.947917\n",
      "train epoch:170 step: 26740 loss:1.649825 acc:0.968750\n",
      "train epoch:170 step: 26760 loss:1.660417 acc:0.958333\n",
      "train epoch:170 step: 26780 loss:1.681283 acc:0.937500\n",
      "train epoch:170 step: 26800 loss:1.649998 acc:0.968750\n",
      "train epoch:170 step: 26820 loss:1.618761 acc:1.000000\n",
      "train epoch:170 step: 26840 loss:1.670847 acc:0.947917\n",
      "train epoch:171 step: 26860 loss:1.649683 acc:0.968750\n",
      "train epoch:171 step: 26880 loss:1.660414 acc:0.958333\n",
      "train epoch:171 step: 26900 loss:1.660421 acc:0.958333\n",
      "train epoch:171 step: 26920 loss:1.649684 acc:0.968750\n",
      "train epoch:171 step: 26940 loss:1.639589 acc:0.979167\n",
      "train epoch:171 step: 26960 loss:1.659615 acc:0.958333\n",
      "train epoch:171 step: 26980 loss:1.659784 acc:0.958333\n",
      "train epoch:171 step: 27000 loss:1.650016 acc:0.968750\n",
      "eval epoch:171 step: 27000 loss:1.650016 acc:0.968750\n",
      "train epoch:172 step: 27020 loss:1.650005 acc:0.968750\n",
      "train epoch:172 step: 27040 loss:1.639583 acc:0.979167\n",
      "train epoch:172 step: 27060 loss:1.670828 acc:0.947917\n",
      "train epoch:172 step: 27080 loss:1.670848 acc:0.947917\n",
      "train epoch:172 step: 27100 loss:1.649644 acc:0.968750\n",
      "train epoch:172 step: 27120 loss:1.660432 acc:0.958333\n",
      "train epoch:172 step: 27140 loss:1.711742 acc:0.906250\n",
      "train epoch:172 step: 27160 loss:1.618752 acc:1.000000\n",
      "train epoch:173 step: 27180 loss:1.670113 acc:0.947917\n",
      "train epoch:173 step: 27200 loss:1.660074 acc:0.958333\n",
      "train epoch:173 step: 27220 loss:1.639299 acc:0.979167\n",
      "train epoch:173 step: 27240 loss:1.650000 acc:0.968750\n",
      "train epoch:173 step: 27260 loss:1.649674 acc:0.968750\n",
      "train epoch:173 step: 27280 loss:1.629189 acc:0.989583\n",
      "train epoch:173 step: 27300 loss:1.629179 acc:0.989583\n",
      "train epoch:174 step: 27320 loss:1.660103 acc:0.958333\n",
      "train epoch:174 step: 27340 loss:1.680122 acc:0.937500\n",
      "train epoch:174 step: 27360 loss:1.639593 acc:0.979167\n",
      "train epoch:174 step: 27380 loss:1.639721 acc:0.979167\n",
      "train epoch:174 step: 27400 loss:1.638776 acc:0.979167\n",
      "train epoch:174 step: 27420 loss:1.628874 acc:0.989583\n",
      "train epoch:174 step: 27440 loss:1.660092 acc:0.958333\n",
      "train epoch:174 step: 27460 loss:1.681251 acc:0.937500\n",
      "train epoch:175 step: 27480 loss:1.639581 acc:0.979167\n",
      "train epoch:175 step: 27500 loss:1.670524 acc:0.947917\n",
      "eval epoch:175 step: 27500 loss:1.670524 acc:0.947917\n",
      "train epoch:175 step: 27520 loss:1.670131 acc:0.947917\n",
      "train epoch:175 step: 27540 loss:1.629176 acc:0.989583\n",
      "train epoch:175 step: 27560 loss:1.681256 acc:0.937500\n",
      "train epoch:175 step: 27580 loss:1.639306 acc:0.979167\n",
      "train epoch:175 step: 27600 loss:1.649608 acc:0.968750\n",
      "train epoch:175 step: 27620 loss:1.639296 acc:0.979167\n",
      "train epoch:176 step: 27640 loss:1.670457 acc:0.947917\n",
      "train epoch:176 step: 27660 loss:1.649649 acc:0.968750\n",
      "train epoch:176 step: 27680 loss:1.639592 acc:0.979167\n",
      "train epoch:176 step: 27700 loss:1.670832 acc:0.947917\n",
      "train epoch:176 step: 27720 loss:1.660097 acc:0.958333\n",
      "train epoch:176 step: 27740 loss:1.670009 acc:0.947917\n",
      "train epoch:176 step: 27760 loss:1.660429 acc:0.958333\n",
      "train epoch:176 step: 27780 loss:1.649686 acc:0.968750\n",
      "train epoch:177 step: 27800 loss:1.660007 acc:0.958333\n",
      "train epoch:177 step: 27820 loss:1.650003 acc:0.968750\n",
      "train epoch:177 step: 27840 loss:1.670157 acc:0.947917\n",
      "train epoch:177 step: 27860 loss:1.660005 acc:0.958333\n",
      "train epoch:177 step: 27880 loss:1.660418 acc:0.958333\n",
      "train epoch:177 step: 27900 loss:1.629162 acc:0.989583\n",
      "train epoch:177 step: 27920 loss:1.629164 acc:0.989583\n",
      "train epoch:177 step: 27940 loss:1.629169 acc:0.989583\n",
      "train epoch:178 step: 27960 loss:1.629166 acc:0.989583\n",
      "train epoch:178 step: 27980 loss:1.660130 acc:0.958333\n",
      "train epoch:178 step: 28000 loss:1.650009 acc:0.968750\n",
      "eval epoch:178 step: 28000 loss:1.650009 acc:0.968750\n",
      "train epoch:178 step: 28020 loss:1.629159 acc:0.989583\n",
      "train epoch:178 step: 28040 loss:1.660416 acc:0.958333\n",
      "train epoch:178 step: 28060 loss:1.629162 acc:0.989583\n",
      "train epoch:178 step: 28080 loss:1.650000 acc:0.968750\n",
      "train epoch:178 step: 28100 loss:1.680617 acc:0.937500\n",
      "train epoch:179 step: 28120 loss:1.660425 acc:0.958333\n",
      "train epoch:179 step: 28140 loss:1.639590 acc:0.979167\n",
      "train epoch:179 step: 28160 loss:1.629166 acc:0.989583\n",
      "train epoch:179 step: 28180 loss:1.712496 acc:0.906250\n",
      "train epoch:179 step: 28200 loss:1.650000 acc:0.968750\n",
      "train epoch:179 step: 28220 loss:1.639357 acc:0.979167\n",
      "train epoch:179 step: 28240 loss:1.649999 acc:0.968750\n",
      "train epoch:180 step: 28260 loss:1.670834 acc:0.947917\n",
      "train epoch:180 step: 28280 loss:1.670033 acc:0.947917\n",
      "train epoch:180 step: 28300 loss:1.639589 acc:0.979167\n",
      "train epoch:180 step: 28320 loss:1.680487 acc:0.937500\n",
      "train epoch:180 step: 28340 loss:1.618748 acc:1.000000\n",
      "train epoch:180 step: 28360 loss:1.628843 acc:0.989583\n",
      "train epoch:180 step: 28380 loss:1.639273 acc:0.979167\n",
      "train epoch:180 step: 28400 loss:1.701533 acc:0.916667\n",
      "train epoch:181 step: 28420 loss:1.660419 acc:0.958333\n",
      "train epoch:181 step: 28440 loss:1.649688 acc:0.968750\n",
      "train epoch:181 step: 28460 loss:1.650003 acc:0.968750\n",
      "train epoch:181 step: 28480 loss:1.629163 acc:0.989583\n",
      "train epoch:181 step: 28500 loss:1.660015 acc:0.958333\n",
      "eval epoch:181 step: 28500 loss:1.660015 acc:0.958333\n",
      "train epoch:181 step: 28520 loss:1.701472 acc:0.916667\n",
      "train epoch:181 step: 28540 loss:1.639272 acc:0.979167\n",
      "train epoch:181 step: 28560 loss:1.649678 acc:0.968750\n",
      "train epoch:182 step: 28580 loss:1.670852 acc:0.947917\n",
      "train epoch:182 step: 28600 loss:1.629166 acc:0.989583\n",
      "train epoch:182 step: 28620 loss:1.629159 acc:0.989583\n",
      "train epoch:182 step: 28640 loss:1.760150 acc:0.864583\n",
      "train epoch:182 step: 28660 loss:1.844508 acc:0.791667\n",
      "train epoch:182 step: 28680 loss:1.860332 acc:0.760417\n",
      "train epoch:182 step: 28700 loss:1.802793 acc:0.822917\n",
      "train epoch:182 step: 28720 loss:1.843553 acc:0.781250\n",
      "train epoch:183 step: 28740 loss:1.781001 acc:0.833333\n",
      "train epoch:183 step: 28760 loss:1.785535 acc:0.833333\n",
      "train epoch:183 step: 28780 loss:1.746223 acc:0.885417\n",
      "train epoch:183 step: 28800 loss:1.737861 acc:0.885417\n",
      "train epoch:183 step: 28820 loss:1.722004 acc:0.916667\n",
      "train epoch:183 step: 28840 loss:1.734704 acc:0.885417\n",
      "train epoch:183 step: 28860 loss:1.700105 acc:0.916667\n",
      "train epoch:183 step: 28880 loss:1.696291 acc:0.927083\n",
      "train epoch:184 step: 28900 loss:1.664730 acc:0.947917\n",
      "train epoch:184 step: 28920 loss:1.652892 acc:0.968750\n",
      "train epoch:184 step: 28940 loss:1.668884 acc:0.947917\n",
      "train epoch:184 step: 28960 loss:1.680453 acc:0.937500\n",
      "train epoch:184 step: 28980 loss:1.704879 acc:0.906250\n",
      "train epoch:184 step: 29000 loss:1.661658 acc:0.958333\n",
      "eval epoch:184 step: 29000 loss:1.661658 acc:0.958333\n",
      "train epoch:184 step: 29020 loss:1.667837 acc:0.947917\n",
      "train epoch:184 step: 29040 loss:1.707200 acc:0.916667\n",
      "train epoch:185 step: 29060 loss:1.673104 acc:0.947917\n",
      "train epoch:185 step: 29080 loss:1.624981 acc:0.989583\n",
      "train epoch:185 step: 29100 loss:1.652378 acc:0.968750\n",
      "train epoch:185 step: 29120 loss:1.674026 acc:0.947917\n",
      "train epoch:185 step: 29140 loss:1.671331 acc:0.947917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:185 step: 29160 loss:1.681282 acc:0.937500\n",
      "train epoch:185 step: 29180 loss:1.653351 acc:0.968750\n",
      "train epoch:185 step: 29200 loss:1.656091 acc:0.958333\n",
      "train epoch:186 step: 29220 loss:1.662224 acc:0.958333\n",
      "train epoch:186 step: 29240 loss:1.650155 acc:0.968750\n",
      "train epoch:186 step: 29260 loss:1.660690 acc:0.958333\n",
      "train epoch:186 step: 29280 loss:1.670634 acc:0.947917\n",
      "train epoch:186 step: 29300 loss:1.663186 acc:0.958333\n",
      "train epoch:186 step: 29320 loss:1.642247 acc:0.979167\n",
      "train epoch:186 step: 29340 loss:1.641601 acc:0.979167\n",
      "train epoch:187 step: 29360 loss:1.660100 acc:0.958333\n",
      "train epoch:187 step: 29380 loss:1.640132 acc:0.979167\n",
      "train epoch:187 step: 29400 loss:1.671129 acc:0.947917\n",
      "train epoch:187 step: 29420 loss:1.620439 acc:1.000000\n",
      "train epoch:187 step: 29440 loss:1.689038 acc:0.927083\n",
      "train epoch:187 step: 29460 loss:1.694560 acc:0.927083\n",
      "train epoch:187 step: 29480 loss:1.639368 acc:0.979167\n",
      "train epoch:187 step: 29500 loss:1.687535 acc:0.927083\n",
      "eval epoch:187 step: 29500 loss:1.687535 acc:0.927083\n",
      "train epoch:188 step: 29520 loss:1.661716 acc:0.958333\n",
      "train epoch:188 step: 29540 loss:1.658791 acc:0.968750\n",
      "train epoch:188 step: 29560 loss:1.691382 acc:0.927083\n",
      "train epoch:188 step: 29580 loss:1.741029 acc:0.875000\n",
      "train epoch:188 step: 29600 loss:1.660353 acc:0.958333\n",
      "train epoch:188 step: 29620 loss:1.680706 acc:0.937500\n",
      "train epoch:188 step: 29640 loss:1.660344 acc:0.958333\n",
      "train epoch:188 step: 29660 loss:1.691733 acc:0.927083\n",
      "train epoch:189 step: 29680 loss:1.639761 acc:0.979167\n",
      "train epoch:189 step: 29700 loss:1.670606 acc:0.947917\n",
      "train epoch:189 step: 29720 loss:1.639667 acc:0.979167\n",
      "train epoch:189 step: 29740 loss:1.651226 acc:0.968750\n",
      "train epoch:189 step: 29760 loss:1.639650 acc:0.979167\n",
      "train epoch:189 step: 29780 loss:1.677174 acc:0.937500\n",
      "train epoch:189 step: 29800 loss:1.620021 acc:1.000000\n",
      "train epoch:189 step: 29820 loss:1.660552 acc:0.958333\n",
      "train epoch:190 step: 29840 loss:1.650139 acc:0.968750\n",
      "train epoch:190 step: 29860 loss:1.649905 acc:0.968750\n",
      "train epoch:190 step: 29880 loss:1.691733 acc:0.927083\n",
      "train epoch:190 step: 29900 loss:1.670948 acc:0.947917\n",
      "train epoch:190 step: 29920 loss:1.629271 acc:0.989583\n",
      "train epoch:190 step: 29940 loss:1.660132 acc:0.958333\n",
      "train epoch:190 step: 29960 loss:1.629271 acc:0.989583\n",
      "train epoch:190 step: 29980 loss:1.639217 acc:0.979167\n",
      "train epoch:191 step: 30000 loss:1.629213 acc:0.989583\n",
      "eval epoch:191 step: 30000 loss:1.629213 acc:0.989583\n",
      "train epoch:191 step: 30020 loss:1.660215 acc:0.958333\n",
      "train epoch:191 step: 30040 loss:1.639645 acc:0.979167\n",
      "train epoch:191 step: 30060 loss:1.660444 acc:0.958333\n",
      "train epoch:191 step: 30080 loss:1.639646 acc:0.979167\n",
      "train epoch:191 step: 30100 loss:1.660467 acc:0.958333\n",
      "train epoch:191 step: 30120 loss:1.650042 acc:0.968750\n",
      "train epoch:191 step: 30140 loss:1.670163 acc:0.947917\n",
      "train epoch:192 step: 30160 loss:1.660461 acc:0.958333\n",
      "train epoch:192 step: 30180 loss:1.670467 acc:0.947917\n",
      "train epoch:192 step: 30200 loss:1.618829 acc:1.000000\n",
      "train epoch:192 step: 30220 loss:1.660397 acc:0.958333\n",
      "train epoch:192 step: 30240 loss:1.660447 acc:0.958333\n",
      "train epoch:192 step: 30260 loss:1.670874 acc:0.947917\n",
      "train epoch:192 step: 30280 loss:1.629243 acc:0.989583\n",
      "train epoch:192 step: 30300 loss:1.618762 acc:1.000000\n",
      "train epoch:193 step: 30320 loss:1.629221 acc:0.989583\n",
      "train epoch:193 step: 30340 loss:1.659479 acc:0.958333\n",
      "train epoch:193 step: 30360 loss:1.639619 acc:0.979167\n",
      "train epoch:193 step: 30380 loss:1.660046 acc:0.958333\n",
      "train epoch:193 step: 30400 loss:1.650020 acc:0.968750\n",
      "train epoch:193 step: 30420 loss:1.629176 acc:0.989583\n",
      "train epoch:193 step: 30440 loss:1.650013 acc:0.968750\n",
      "train epoch:194 step: 30460 loss:1.639622 acc:0.979167\n",
      "train epoch:194 step: 30480 loss:1.701341 acc:0.916667\n",
      "train epoch:194 step: 30500 loss:1.639619 acc:0.979167\n",
      "eval epoch:194 step: 30500 loss:1.639619 acc:0.979167\n",
      "train epoch:194 step: 30520 loss:1.629198 acc:0.989583\n",
      "train epoch:194 step: 30540 loss:1.649743 acc:0.968750\n",
      "train epoch:194 step: 30560 loss:1.639627 acc:0.979167\n",
      "train epoch:194 step: 30580 loss:1.649716 acc:0.968750\n",
      "train epoch:194 step: 30600 loss:1.670452 acc:0.947917\n",
      "train epoch:195 step: 30620 loss:1.660185 acc:0.958333\n",
      "train epoch:195 step: 30640 loss:1.650051 acc:0.968750\n",
      "train epoch:195 step: 30660 loss:1.650020 acc:0.968750\n",
      "train epoch:195 step: 30680 loss:1.660076 acc:0.958333\n",
      "train epoch:195 step: 30700 loss:1.680943 acc:0.937500\n",
      "train epoch:195 step: 30720 loss:1.691260 acc:0.927083\n",
      "train epoch:195 step: 30740 loss:1.629167 acc:0.989583\n",
      "train epoch:195 step: 30760 loss:1.691695 acc:0.927083\n",
      "train epoch:196 step: 30780 loss:1.639605 acc:0.979167\n",
      "train epoch:196 step: 30800 loss:1.701563 acc:0.916667\n",
      "train epoch:196 step: 30820 loss:1.639608 acc:0.979167\n",
      "train epoch:196 step: 30840 loss:1.650028 acc:0.968750\n",
      "train epoch:196 step: 30860 loss:1.650019 acc:0.968750\n",
      "train epoch:196 step: 30880 loss:1.629196 acc:0.989583\n",
      "train epoch:196 step: 30900 loss:1.629194 acc:0.989583\n",
      "train epoch:196 step: 30920 loss:1.670842 acc:0.947917\n",
      "train epoch:197 step: 30940 loss:1.680635 acc:0.937500\n",
      "train epoch:197 step: 30960 loss:1.660445 acc:0.958333\n",
      "train epoch:197 step: 30980 loss:1.701692 acc:0.916667\n",
      "train epoch:197 step: 31000 loss:1.639600 acc:0.979167\n",
      "eval epoch:197 step: 31000 loss:1.639600 acc:0.979167\n",
      "train epoch:197 step: 31020 loss:1.670866 acc:0.947917\n",
      "train epoch:197 step: 31040 loss:1.679885 acc:0.937500\n",
      "train epoch:197 step: 31060 loss:1.681153 acc:0.937500\n",
      "train epoch:197 step: 31080 loss:1.629197 acc:0.989583\n",
      "train epoch:198 step: 31100 loss:1.661031 acc:0.958333\n",
      "train epoch:198 step: 31120 loss:1.660828 acc:0.958333\n",
      "train epoch:198 step: 31140 loss:1.671399 acc:0.947917\n",
      "train epoch:198 step: 31160 loss:1.769823 acc:0.843750\n",
      "train epoch:198 step: 31180 loss:1.712068 acc:0.906250\n",
      "train epoch:198 step: 31200 loss:1.658398 acc:0.958333\n",
      "train epoch:198 step: 31220 loss:1.700884 acc:0.916667\n",
      "train epoch:198 step: 31240 loss:1.727787 acc:0.895833\n",
      "train epoch:199 step: 31260 loss:1.715477 acc:0.906250\n",
      "train epoch:199 step: 31280 loss:1.683247 acc:0.937500\n",
      "train epoch:199 step: 31300 loss:1.709645 acc:0.906250\n",
      "train epoch:199 step: 31320 loss:1.662523 acc:0.958333\n",
      "train epoch:199 step: 31340 loss:1.646050 acc:0.979167\n",
      "train epoch:199 step: 31360 loss:1.681068 acc:0.947917\n",
      "train epoch:199 step: 31380 loss:1.668127 acc:0.947917\n"
     ]
    }
   ],
   "source": [
    "# 初始化log写入器\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "\n",
    "# 模型参数设置\n",
    "embedding_size = 128\n",
    "hidden_size=128\n",
    "num_layers=1\n",
    "\n",
    "# 训练参数设置\n",
    "epoch_num = 200\n",
    "learning_rate = 0.001\n",
    "log_iter = 20\n",
    "eval_iter = 500\n",
    "\n",
    "# 定义一些所需变量\n",
    "global_step = 0\n",
    "log_step = 0\n",
    "max_acc = 0\n",
    "\n",
    "# 实例化模型\n",
    "model = Addition_Model(\n",
    "    char_len=len(label_dict), \n",
    "    embedding_size=embedding_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_layers=num_layers, \n",
    "    DIGITS=DIGITS)\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 设置优化器，学习率，并且把模型参数给优化器\n",
    "opt = paddle.optimizer.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    parameters=model.parameters()\n",
    ")\n",
    "\n",
    "# 启动训练，循环epoch_num个轮次\n",
    "for epoch in range(epoch_num):\n",
    "    # 遍历数据集读取数据\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        # 读取数据\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 模型前向计算\n",
    "        loss, acc = model(inputs, labels=labels)\n",
    "\n",
    "        # 打印训练数据\n",
    "        if global_step%log_iter==0:\n",
    "            print('train epoch:%d step: %d loss:%f acc:%f' % (epoch, global_step, loss.numpy(), acc.numpy()))\n",
    "            log_writer.add_scalar(tag=\"train/loss\", step=log_step, value=loss.numpy())\n",
    "            log_writer.add_scalar(tag=\"train/acc\", step=log_step, value=acc.numpy())\n",
    "            log_step+=1\n",
    "\n",
    "        # 模型验证\n",
    "        if global_step%eval_iter==0:\n",
    "            model.eval()\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for data in dev_reader():\n",
    "                loss, acc = model(inputs, labels=labels)\n",
    "                losses.append(loss.numpy())\n",
    "                accs.append(acc.numpy())\n",
    "            avg_loss = np.concatenate(losses).mean()\n",
    "            avg_acc = np.concatenate(accs).mean()\n",
    "            print('eval epoch:%d step: %d loss:%f acc:%f' % (epoch, global_step, avg_loss, avg_acc))\n",
    "            log_writer.add_scalar(tag=\"dev/loss\", step=log_step, value=avg_loss)\n",
    "            log_writer.add_scalar(tag=\"dev/acc\", step=log_step, value=avg_acc)\n",
    "\n",
    "            # 保存最佳模型\n",
    "            if avg_acc>max_acc:\n",
    "                max_acc = avg_acc\n",
    "                print('saving the best_model...')\n",
    "                paddle.save(model.state_dict(), 'best_model')\n",
    "            model.train()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 使用优化器进行参数优化\n",
    "        opt.step()\n",
    "\n",
    "        # 清除梯度\n",
    "        opt.clear_grad()\n",
    "\n",
    "        # 全局步数加一\n",
    "        global_step += 1\n",
    "\n",
    "# 保存最终模型\n",
    "paddle.save(model.state_dict(),'final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试\n",
    "* 使用保存的最佳模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model answer: 12+40=52 \n",
      "the true answer: 12+40=52\n"
     ]
    }
   ],
   "source": [
    "# 反转字符表\n",
    "label_dict_adv = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# 输入计算题目\n",
    "input_text = '12+40'\n",
    "\n",
    "# 编码输入为ID\n",
    "inputs = encoder(input_text, MAXLEN, label_dict)\n",
    "\n",
    "# 转换输入为向量形式\n",
    "inputs = np.array(inputs).reshape(-1, MAXLEN)\n",
    "inputs = paddle.to_tensor(inputs)\n",
    "\n",
    "# 加载模型\n",
    "params_dict= paddle.load('best_model')\n",
    "model.set_dict(params_dict)\n",
    "\n",
    "# 设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 模型推理\n",
    "out = model(inputs)\n",
    "\n",
    "# 结果转换\n",
    "result = ''.join([label_dict_adv[_] for _ in np.argmax(out.numpy(), -1).reshape(-1)])\n",
    "\n",
    "# 打印结果\n",
    "print('the model answer: %s=%s' % (input_text, result))\n",
    "print('the true answer: %s=%s' % (input_text, eval(input_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "* 你还可以通过变换网络结构，调整数据集，尝试不同的参数的方式来进一步提升本示例当中的数字加法的效果\n",
    "* 同时，也可以尝试在其他的类似的任务中用飞桨来完成实际的实践"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
