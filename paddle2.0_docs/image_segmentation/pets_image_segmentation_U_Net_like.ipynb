{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueGUN2EQeScw"
   },
   "source": [
    "# 基于U型语义分割模型实现的宠物图像分割\n",
    "\n",
    "本示例教程当前是基于2.0-beta版本Paddle做的案例实现，未来会随着2.0的系列版本发布进行升级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.简要介绍\n",
    "\n",
    "在计算机视觉领域，图像分割指的是将数字图像细分为多个图像子区域的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。图像分割的领域非常多，无人车、地块检测、表计识别等等。\n",
    "\n",
    "本示例简要介绍如何通过飞桨开源框架，实现图像分割。这里我们是采用了一个在图像分割领域比较熟知的U-Net网络结构，是一个基于FCN做改进后的一个深度学习网络，包含下采样（编码器，特征提取）和上采样（解码器，分辨率还原）两个阶段，因模型结构比较像U型而命名为U-Net。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.环境设置\n",
    "\n",
    "导入一些比较基础常用的模块，确认自己的飞桨版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "import paddle\n",
    "from paddle.nn import functional as F\n",
    "\n",
    "paddle.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMC2xLAxeScx"
   },
   "source": [
    "## 3.数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0KiJ_5N936Y"
   },
   "source": [
    "### 3.1 数据集下载\n",
    "\n",
    "本案例使用Oxford-IIIT Pet数据集，官网：https://www.robots.ox.ac.uk/~vgg/data/pets 。\n",
    "\n",
    "数据集统计如下：\n",
    "\n",
    "![alt 数据集统计信息](https://www.robots.ox.ac.uk/~vgg/data/pets/breed_count.jpg)\n",
    "\n",
    "数据集包含两个压缩文件：\n",
    "\n",
    "1. 原图：https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "2. 分割图像：https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "xJd9y-u9eScy",
    "outputId": "3985783f-7166-4afa-f511-16427b3e2a71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -O http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!curl -O http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5cP2CBz-Mra"
   },
   "source": [
    "### 3.2 数据集概览\n",
    "\n",
    "首先我们先看看下载到磁盘上的文件结构是什么样，来了解一下我们的数据集。\n",
    "\n",
    "1. 首先看一下images.tar.gz这个压缩包，该文件解压后得到一个images目录，这个目录比较简单，里面直接放的是用类名和序号命名好的图片文件，每个图片是对应的宠物照片。\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── samoyed_7.jpg\n",
    "├── ......\n",
    "└── samoyed_81.jpg\n",
    "```\n",
    "\n",
    "2. 然后我们在看下annotations.tar.gz，文件解压后的目录里面包含以下内容，目录中的README文件将每个目录和文件做了比较详细的介绍，我们可以通过README来查看每个目录文件的说明。\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── README\n",
    "├── list.txt\n",
    "├── test.txt\n",
    "├── trainval.txt\n",
    "├── trimaps\n",
    "│    ├── Abyssinian_1.png\n",
    "│    ├── Abyssinian_10.png\n",
    "│    ├── ......\n",
    "│    └── yorkshire_terrier_99.png\n",
    "└── xmls\n",
    "      ├── Abyssinian_1.xml\n",
    "      ├── Abyssinian_10.xml\n",
    "      ├── ......\n",
    "      └── yorkshire_terrier_190.xml\n",
    "```\n",
    "\n",
    "本次我们主要使用到images和annotations/trimaps两个目录，即原图和三元图像文件，前者作为训练的输入数据，后者是对应的标签数据。\n",
    "\n",
    "我们来看看这个数据集给我们提供了多少个训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tqB7YQ4leSc4",
    "outputId": "8872356c-ef32-4c94-defb-66250a00890a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images_path = \"images/\"\n",
    "label_images_path = \"annotations/trimaps/\"\n",
    "\n",
    "print(\"用于训练的图片样本数量:\", len([os.path.join(train_images_path, image_name) \n",
    "          for image_name in os.listdir(train_images_path) \n",
    "          if image_name.endswith('.jpg')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 数据集类定义\n",
    "\n",
    "飞桨（PaddlePaddle）数据集加载方案是统一使用Dataset（数据集定义） + DataLoader（多进程数据集加载）。\n",
    "\n",
    "首先我们先进行数据集的定义，数据集定义主要是实现一个新的Dataset类，继承父类paddle.io.Dataset，并实现父类中以下两个抽象方法，`__getitem__`和`__len__`：\n",
    "\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        \n",
    "    # 每次迭代时返回数据和对应的标签\n",
    "    def __getitem__(self, idx):\n",
    "        return x, y\n",
    "\n",
    "    # 返回整个数据集的总数\n",
    "    def __len__(self):\n",
    "        return count(samples)\n",
    "```\n",
    "\n",
    "在数据集内部可以结合图像数据预处理相关API进行图像的预处理（改变大小、反转、调整格式等）。\n",
    "\n",
    "由于加载进来的图像不一定都符合自己的需求，举个例子，已下载的这些图片里面就会有RGBA格式的图片，这个时候图片就不符合我们所需3通道的需求，我们需要进行图片的格式转换，那么这里我们直接实现了一个通用的图片读取接口，确保读取出来的图片都是满足我们的需求。\n",
    "\n",
    "另外图片加载出来的默认shape是HWC，这个时候要看看是否满足后面训练的需要，如果Layer的默认格式和这个不是符合的情况下，需要看下Layer有没有参数可以进行格式调整。不过如果layer较多的话，还是直接调整原数据Shape比较好，否则每个layer都要做参数设置，如果有遗漏就会导致训练出错，那么在本案例中是直接对数据源的shape做了统一调整，从HWC转换成了CHW，因为飞桨的卷积等API的默认输入格式为CHW，这样处理方便后续模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from paddle.io import Dataset\n",
    "from paddle.vision.transforms import transforms\n",
    "\n",
    "\n",
    "class ImgTranspose(object):\n",
    "    \"\"\"\n",
    "    图像预处理工具，用于将Mask图像进行升维(160, 160) => (160, 160, 1)，\n",
    "    并对图像的维度进行转换从HWC变为CHW\n",
    "    \"\"\"\n",
    "    def __init__(self, fmt):\n",
    "        self.format = fmt\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=2)\n",
    "            \n",
    "        return img.transpose(self.format)\n",
    "\n",
    "class PetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    数据集定义\n",
    "    \"\"\"\n",
    "    def __init__(self, image_path, label_path, mode='train'):\n",
    "        \"\"\"\n",
    "        构造函数\n",
    "        \"\"\"\n",
    "        self.image_size = (160, 160)\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.mode = mode.lower()\n",
    "        self.eval_image_num = 1000\n",
    "        \n",
    "        assert self.mode in ['train', 'test'], \\\n",
    "            \"mode should be 'train' or 'test', but got {}\".format(self.mode)\n",
    "        \n",
    "        self._parse_dataset()\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            ImgTranspose((2, 0, 1))\n",
    "        ])\n",
    "        \n",
    "    def _sort_images(self, image_dir, image_type):\n",
    "        \"\"\"\n",
    "        对文件夹内的图像进行按照文件名排序\n",
    "        \"\"\"\n",
    "        files = []\n",
    "\n",
    "        for image_name in os.listdir(image_dir):\n",
    "            if image_name.endswith('.{}'.format(image_type)) \\\n",
    "                    and not image_name.startswith('.'):\n",
    "                files.append(os.path.join(image_dir, image_name))\n",
    "\n",
    "        return sorted(files)\n",
    "        \n",
    "    def _parse_dataset(self):\n",
    "        \"\"\"\n",
    "        由于所有文件都是散落在文件夹中，在训练时我们需要使用的是数据集和标签对应的数据关系，\n",
    "        所以我们第一步是对原始的数据集进行整理，得到数据集和标签两个数组，分别一一对应。\n",
    "        这样可以在使用的时候能够很方便的找到原始数据和标签的对应关系，否则对于原有的文件夹图片数据无法直接应用。\n",
    "        在这里是用了一个非常简单的方法，按照文件名称进行排序。\n",
    "        因为刚好数据和标签的文件名是按照这个逻辑制作的，名字都一样，只有扩展名不一样。\n",
    "        \"\"\"\n",
    "        temp_train_images = self._sort_images(self.image_path, 'jpg')\n",
    "        temp_label_images = self._sort_images(self.label_path, 'png')\n",
    "\n",
    "        random.Random(1337).shuffle(temp_train_images)\n",
    "        random.Random(1337).shuffle(temp_label_images)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.train_images = temp_train_images[:-self.eval_image_num]\n",
    "            self.label_images = temp_label_images[:-self.eval_image_num]\n",
    "        else:\n",
    "            self.train_images = temp_train_images[-self.eval_image_num:]\n",
    "            self.label_images = temp_label_images[-self.eval_image_num:]\n",
    "    \n",
    "    def _load_img(self, path, color_mode='rgb'):\n",
    "        \"\"\"\n",
    "        统一的图像处理接口封装，用于规整图像大小和通道\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            img = PilImage.open(io.BytesIO(f.read()))\n",
    "            if color_mode == 'grayscale':\n",
    "                # if image is not already an 8-bit, 16-bit or 32-bit grayscale image\n",
    "                # convert it to an 8-bit grayscale image.\n",
    "                if img.mode not in ('L', 'I;16', 'I'):\n",
    "                    img = img.convert('L')\n",
    "            elif color_mode == 'rgba':\n",
    "                if img.mode != 'RGBA':\n",
    "                    img = img.convert('RGBA')\n",
    "            elif color_mode == 'rgb':\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "            else:\n",
    "                raise ValueError('color_mode must be \"grayscale\", \"rgb\", or \"rgba\"')\n",
    "\n",
    "            if self.image_size is not None:\n",
    "                if img.size != self.image_size:\n",
    "                    img = img.resize(self.image_size, PilImage.NEAREST)\n",
    "\n",
    "            return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回 image, label\n",
    "        \"\"\"\n",
    "        # 花了比较多的时间在数据处理这里，需要处理成模型能适配的格式，踩了一些坑（比如有不是RGB格式的）\n",
    "        # 有图片会出现通道数和期望不符的情况，需要进行相关考虑\n",
    "\n",
    "        # 加载原始图像\n",
    "        train_image = self._load_img(self.train_images[idx])\n",
    "        x = np.array(train_image, dtype='float32')\n",
    "\n",
    "        # 对图像进行预处理，统一大小，转换维度格式（HWC => CHW）\n",
    "        x = self.transforms(x)\n",
    "        \n",
    "        # 加载Label图像\n",
    "        label_image = self._load_img(self.label_images[idx], color_mode=\"grayscale\")  \n",
    "        y = np.array(label_image, dtype='uint8')  \n",
    "\n",
    "        # 图像预处理\n",
    "        # Label图像是二维的数组(size, size)，升维到(size, size, 1)后才能用于最后loss计算\n",
    "        y = self.transforms(y)\n",
    "        \n",
    "        # 返回img, label，转换为需要的格式\n",
    "        return x, y.astype('int64')\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集总数\n",
    "        \"\"\"\n",
    "        return len(self.train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYxTHfbBESSG"
   },
   "source": [
    "### 3.4 PetDataSet数据集抽样展示\n",
    "\n",
    "实现好Dataset数据集后，我们来测试一下数据集是否符合预期，因为Dataset是一个可以被迭代的Class，我们通过for循环从里面读取数据来用matplotlib进行展示，这里要注意的是对于分割的标签文件因为是1通道的灰度图片，需要在使用imshow接口时注意下传参cmap='gray'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "MTO-C5qFDnPn",
    "outputId": "0937ed5e-1216-4773-9b54-16db8fe7b457"
   },
   "outputs": [],
   "source": [
    "# 训练数据集\n",
    "train_dataset = PetDataset(train_images_path, label_images_path, mode='train')\n",
    "\n",
    "# 验证数据集\n",
    "val_dataset = PetDataset(train_images_path, label_images_path, mode='test')\n",
    "\n",
    "# 抽样一个数据\n",
    "image, label = train_dataset[0]\n",
    "\n",
    "# 进行图片的展示\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,2,1), \n",
    "plt.title('Train Image')\n",
    "plt.imshow(image.transpose((1, 2, 0)).astype('uint8'))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2), \n",
    "plt.title('Label')\n",
    "plt.imshow(np.squeeze(label, axis=0).astype('uint8'), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9JyZz3ZEnQ1"
   },
   "source": [
    "## 4.模型组网\n",
    "\n",
    "U-Net是一个U型网络结构，可以看做两个大的阶段，图像先经过Encoder编码器进行下采样得到高级语义特征图，再经过Decoder解码器上采样将特征图恢复到原图片的分辨率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi-ouGZL--BN"
   },
   "source": [
    "### 4.1 定义SeparableConv2d接口\n",
    "\n",
    "我们为了减少卷积操作中的训练参数来提升性能，是继承paddle.nn.Layer自定义了一个SeparableConv2d Layer类，整个过程是把`filter_size * filter_size * num_filters`的Conv2d操作拆解为两个子Conv2d，先对输入数据的每个通道使用`filter_size * filter_size * 1`的卷积核进行计算，输入输出通道数目相同，之后在使用`1 * 1 * num_filters`的卷积核计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0c-FikH-A4qP"
   },
   "outputs": [],
   "source": [
    "class SeparableConv2d(paddle.nn.Layer):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 dilation=1, \n",
    "                 groups=None, \n",
    "                 weight_attr=None, \n",
    "                 bias_attr=None, \n",
    "                 data_format=\"NCHW\"):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        # 第一次卷积操作没有偏置参数\n",
    "        self.conv_1 = paddle.nn.Conv2d(in_channels, \n",
    "                                       in_channels, \n",
    "                                       kernel_size, \n",
    "                                       stride=stride,\n",
    "                                       padding=padding,\n",
    "                                       dilation=dilation,\n",
    "                                       groups=in_channels, \n",
    "                                       weight_attr=weight_attr, \n",
    "                                       bias_attr=False,  \n",
    "                                       data_format=data_format)\n",
    "        self.pointwise = paddle.nn.Conv2d(in_channels, \n",
    "                                          out_channels, \n",
    "                                          1, \n",
    "                                          stride=1, \n",
    "                                          padding=0, \n",
    "                                          dilation=1, \n",
    "                                          groups=1, \n",
    "                                          weight_attr=weight_attr, \n",
    "                                          data_format=data_format)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        y = self.conv_1(inputs)\n",
    "        y = self.pointwise(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNyzlqQmBEEi"
   },
   "source": [
    "### 4.2 定义Encoder编码器\n",
    "\n",
    "我们将网络结构中的Encoder下采样过程进行了一个Layer封装，方便后续调用，减少代码编写，下采样是有一个模型逐渐向下画曲线的一个过程，这个过程中是不断的重复一个单元结构将通道数不断增加，形状不断缩小，并且引入残差网络结构，我们将这些都抽象出来进行统一封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpUi9VUeGmXp"
   },
   "outputs": [],
   "source": [
    "class Encoder(paddle.nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.relu = paddle.nn.ReLU()\n",
    "        self.separable_conv_01 = SeparableConv2d(in_channels, \n",
    "                                                 out_channels, \n",
    "                                                 kernel_size=3, \n",
    "                                                 padding='same')\n",
    "        self.bn = paddle.nn.BatchNorm2d(out_channels)\n",
    "        self.separable_conv_02 = SeparableConv2d(out_channels, \n",
    "                                                 out_channels, \n",
    "                                                 kernel_size=3, \n",
    "                                                 padding='same')\n",
    "        self.pool = paddle.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.residual_conv = paddle.nn.Conv2d(in_channels, \n",
    "                                              out_channels, \n",
    "                                              kernel_size=1, \n",
    "                                              stride=2, \n",
    "                                              padding='same')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        previous_block_activation = inputs\n",
    "        \n",
    "        y = self.relu(inputs)\n",
    "        y = self.separable_conv_01(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.separable_conv_02(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.pool(y)\n",
    "        \n",
    "        residual = self.residual_conv(previous_block_activation)\n",
    "        y = paddle.add(y, residual)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPBRD42WGmuH"
   },
   "source": [
    "### 4.3 定义Decoder解码器\n",
    "\n",
    "在通道数达到最大得到高级语义特征图后，网络结构会开始进行decode操作，进行上采样，通道数逐渐减小，对应图片尺寸逐步增加，直至恢复到原图像大小，那么这个过程里面也是通过不断的重复相同结构的残差网络完成，我们也是为了减少代码编写，将这个过程定义一个Layer来放到模型组网中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltVurq8OGvK7"
   },
   "outputs": [],
   "source": [
    "class Decoder(paddle.nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.relu = paddle.nn.ReLU()\n",
    "        self.conv_transpose_01 = paddle.nn.ConvTranspose2d(in_channels, \n",
    "                                                           out_channels, \n",
    "                                                           kernel_size=3, \n",
    "                                                           padding='same')\n",
    "        self.conv_transpose_02 = paddle.nn.ConvTranspose2d(out_channels, \n",
    "                                                           out_channels, \n",
    "                                                           kernel_size=3, \n",
    "                                                           padding='same')\n",
    "        self.bn = paddle.nn.BatchNorm2d(out_channels)\n",
    "        self.upsample = paddle.nn.UpSample(scale_factor=2.0)\n",
    "        self.residual_conv = paddle.nn.Conv2d(in_channels, \n",
    "                                              out_channels, \n",
    "                                              kernel_size=1, \n",
    "                                              padding='same')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        previous_block_activation = inputs\n",
    "\n",
    "        y = self.relu(inputs)\n",
    "        y = self.conv_transpose_01(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv_transpose_02(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.upsample(y)\n",
    "        \n",
    "        residual = self.upsample(previous_block_activation)\n",
    "        residual = self.residual_conv(residual)\n",
    "        \n",
    "        y = paddle.add(y, residual)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLKLj2FMGvdc"
   },
   "source": [
    "### 4.4 训练模型组网\n",
    "\n",
    "按照U型网络结构格式进行整体的网络结构搭建，三次下采样，四次上采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "an1YFILpG4Xy"
   },
   "outputs": [],
   "source": [
    "class PetModel(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PetModel, self).__init__()\n",
    "\n",
    "        self.conv_1 = paddle.nn.Conv2d(3, 32, \n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding='same')\n",
    "        self.bn = paddle.nn.BatchNorm2d(32)\n",
    "        self.relu = paddle.nn.ReLU()\n",
    "\n",
    "        in_channels = 32\n",
    "        self.encoders = []\n",
    "        self.encoder_list = [64, 128, 256]\n",
    "        self.decoder_list = [256, 128, 64, 32]\n",
    "\n",
    "        # 根据下采样个数和配置循环定义子Layer，避免重复写一样的程序\n",
    "        for out_channels in self.encoder_list:\n",
    "            block = self.add_sublayer('encoder_%s'.format(out_channels),\n",
    "                                      Encoder(in_channels, out_channels))\n",
    "            self.encoders.append(block)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        # 根据上采样个数和配置循环定义子Layer，避免重复写一样的程序\n",
    "        for out_channels in self.decoder_list:\n",
    "            block = self.add_sublayer('decoder_%s'.format(out_channels), \n",
    "                                      Decoder(in_channels, out_channels))\n",
    "            self.decoders.append(block)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.output_conv = paddle.nn.Conv2d(in_channels, \n",
    "                                            num_classes, \n",
    "                                            kernel_size=3, \n",
    "                                            padding='same')\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        y = self.conv_1(inputs)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            y = encoder(y)\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(y)\n",
    "        \n",
    "        y = self.output_conv(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Nf7hQ60G4sj"
   },
   "source": [
    "### 4.5 模型可视化\n",
    "\n",
    "调用飞桨提供的summary接口对组建好的模型进行可视化，方便进行模型结构和参数信息的查看和确认。\n",
    "@TODO，需要替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1_MXfWkZeSdE",
    "outputId": "4c9870de-9eb6-47e8-e88c-79509ef78cf5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddle.static import InputSpec\n",
    "\n",
    "paddle.disable_static()\n",
    "num_classes = 4\n",
    "model = paddle.Model(PetModel(num_classes))\n",
    "model.summary((3, 160, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9Trlcvj8R7L"
   },
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Sskbyz58X4J"
   },
   "source": [
    "### 5.1 配置信息\n",
    "\n",
    "定义训练BATCH_SIZE、训练轮次和计算设备等信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fSkTiRB8OpP"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "device = paddle.set_device('gpu')\n",
    "paddle.disable_static(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_vaedRa8eoy"
   },
   "source": [
    "### 5.3 自定义Loss\n",
    "\n",
    "在这个任务中我们使用SoftmaxWithCrossEntropy损失函数来做计算，飞桨中有functional形式的API，这里我们做一个自定义操作，实现一个Class形式API放到模型训练中使用。没有直接使用CrossEntropyLoss的原因主要是对计算维度的自定义需求，本次需要进行softmax计算的维度是1，不是默认的最后一维，所以我们采用上面提到的损失函数，通过axis参数来指定softmax计算维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEZq_jT78jNe"
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithCrossEntropy(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxWithCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        loss = F.softmax_with_cross_entropy(input, \n",
    "                                            label, \n",
    "                                            return_softmax=False,\n",
    "                                            axis=1)\n",
    "        return paddle.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj6MPPMkJIdZ"
   },
   "source": [
    "### 5.4 启动模型训练\n",
    "\n",
    "使用模型代码进行Model实例生成，使用prepare接口定义优化器、损失函数和评价指标等信息，用于后续训练使用。在所有初步配置完成后，调用fit接口开启训练执行过程，调用fit时只需要将前面定义好的训练数据集、测试数据集、训练轮次（Epoch）和批次大小（batch_size）配置好即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "m-cVyjNreSdO",
    "outputId": "9b37dd07-746b-41cc-c8e2-687a83b1ad75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = paddle.optimizer.RMSProp(learning_rate=0.001, \n",
    "                                 rho=0.9, \n",
    "                                 momentum=0.0, \n",
    "                                 epsilon=1e-07, \n",
    "                                 centered=False,\n",
    "                                 parameters=model.parameters())\n",
    "model = paddle.Model(PetModel(num_classes))\n",
    "model.prepare(optim, SoftmaxWithCrossEntropy())\n",
    "model.fit(train_dataset, \n",
    "          val_dataset, \n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mouwS1kJRqJ"
   },
   "source": [
    "## 6.模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dvjxu91DJd1G"
   },
   "source": [
    "### 6.1 预测数据集准备和预测\n",
    "\n",
    "继续使用PetDataset来实例化待预测使用的数据集。这里我们为了方便没有在另外准备预测数据，复用了评估数据。\n",
    "\n",
    "我们可以直接使用model.predict接口来对数据集进行预测操作，只需要将预测数据集传递到接口内即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur088_vjeSdR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_results = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DpAEFBSJioy"
   },
   "source": [
    "### 6.2 预测结果可视化\n",
    "\n",
    "从我们的预测数据集中抽3个动物来看看预测的效果，展示一下原图、标签图和预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mfaFkO5S1PU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(predict_results))\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "mask_idx = 0\n",
    "\n",
    "for data in val_dataset:\n",
    "    if i > 8: \n",
    "        break\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(data[0].transpose((1, 2, 0)).astype('uint8'))\n",
    "    plt.title('Input Image')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(3, 3, i + 2)\n",
    "    plt.imshow(np.squeeze(data[1], axis=0).astype('uint8'), cmap='gray')\n",
    "    plt.title('Label')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # 模型只有一个输出，所以我们通过predict_results[0]来取出1000个预测的结果\n",
    "    # 映射原始图片的index来取出预测结果，提取mask进行展示\n",
    "    data = predict_results[0][mask_idx][0].transpose((1, 2, 0))\n",
    "    mask = np.argmax(data, axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "    plt.subplot(3, 3, i + 3)\n",
    "    plt.imshow(np.squeeze(mask, axis=2).astype('uint8'), cmap='gray')\n",
    "    plt.title('Predict')\n",
    "    plt.axis(\"off\")\n",
    "    i += 3\n",
    "    mask_idx += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pets_image_segmentation_U_Net_like.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1599641797278"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}