{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PaddlePaddle实现的端到端自动语音识别\n",
    "\n",
    "**作者:** [夜雨飘零](https://github.com/yeyupiaoling)<br/>\n",
    "**日期:** 2021.05<br/>\n",
    "**摘要:** 本示例教程演示如何在PaddlePaddle实现的端到端自动语音识别<br/>\n",
    "\n",
    "# 一、简介\n",
    "本项目可以在[AI Studio](https://aistudio.baidu.com/aistudio/projectdetail/1597936)在线运行。\n",
    "\n",
    "PPASR基于PaddlePaddle实现的端到端自动语音识别，本项目最大的特点简单，在保证准确率不低的情况下，项目尽量做得浅显易懂，能够让每个想入门语音识别的开发者都能够轻松上手。PPASR只使用卷积神经网络，无其他特殊网络结构，模型简单易懂，且是端到端的，不需要音频对齐，因为本项目使用了CTC Loss作为损失函数。在传统的语音识别的模型中，我们对语音模型进行训练之前，往往都要将文本与语音进行严格的对齐操作。在传统的语音识别的模型中，我们对语音模型进行训练之前，往往都要将文本与语音进行严格的对齐操作，这种对齐非常浪费时间，而且对齐之后，模型预测出的label只是局部分类的结果，而无法给出整个序列的输出结果，往往要对预测出的label做一些后处理才可以得到我们最终想要的结果。基于这种情况，就出现了CTC（Connectionist temporal classification），使用CTC Loss就不需要进行音频对齐，直接输入是一句完整的语音数据，输出的是整个序列结果，这种情况OCR也是同样的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、安装环境\n",
    "\n",
    " - 本项目可以在Windows或者Ubuntu都可以运行，安装环境很简单，只需要执行以下命令即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.19.3 scipy==1.6.1 tqdm pytest-runner librosa==0.6.3 python-Levenshtein==0.12.2 visualdl==2.1.1 --user\r\n",
    "!pip install SoundFile==0.9.0.post1 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、数据准备\n",
    "\n",
    "1. 在`data`目录下是公开数据集的下载和制作训练数据列表和字典的，本项目提供了下载公开的中文普通话语音数据集，分别是Aishell，Free ST-Chinese-Mandarin-Corpus，THCHS-30 这三个数据集，总大小超过28G。下载这三个数据只需要执行一下代码即可，当然如果想快速训练，也可以只下载其中一个。特别提醒，这样下载会比较慢，最好是自己上传数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !python3 data1/aishell.py\n",
    "#!python3 data1/free_st_chinese_mandarin_corpus.py\n",
    "!python3 data1/thchs_30.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " - 如果开发者有自己的数据集，可以使用自己的数据集进行训练，当然也可以跟上面下载的数据集一起训练。自定义的语音数据需要符合一下格式：\n",
    "    1. 语音文件需要放在`dataset/audio/`目录下，例如我们有个`wav`的文件夹，里面都是语音文件，我们就把这个文件存放在`dataset/audio/`。\n",
    "    2. 然后把数据列表文件存在`dataset/annotation/`目录下，程序会遍历这个文件下的所有数据列表文件。例如这个文件下存放一个`my_audio.txt`，它的内容格式如下。每一行数据包含该语音文件的相对路径和该语音文件对应的中文文本，要注意的是该中文文本只能包含纯中文，不能包含标点符号、阿拉伯数字以及英文字母。\n",
    "```shell script\n",
    "dataset/audio/wav/0175/H0175A0171.wav 我需要把空调温度调到二十度\n",
    "dataset/audio/wav/0175/H0175A0377.wav 出彩中国人\n",
    "dataset/audio/wav/0175/H0175A0470.wav 据克而瑞研究中心监测\n",
    "dataset/audio/wav/0175/H0175A0180.wav 把温度加大到十八"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " - 执行下面的命令，创建数据列表，以及建立词表，也就是数据字典，把所有出现的字符都存放子在`zh_vocab.json`文件中，生成的文件都存放在`dataset/`目录下。最最最重要的是还计算了数据集的均值和标准值，计算得到的均值和标准值需要更新在训练参数`data_mean`和`data_std`中，之后的评估和预测同样需要用到。有几个参数需要注意，参数`is_change_frame_rate`是指定在生成数据集的时候，是否要把音频的采样率转换为16000Hz，最好是使用默认值。参数`min_duration`和`max_duration`限制音频的长度，特别是有些音频太长，会导致显存不足，训练直接崩掉。\n",
    "\n",
    "我们来说说这些文件和数据的具体作用，创建数据列表是为了在训练是读取数据，读取数据程序通过读取图像列表的每一行都能得到音频的文件路径、音频长度以及这句话的内容。通过路径读取音频文件并进行预处理，音频长度用于统计数据总长度，文字内容就是输入数据的标签，在训练是还需要数据字典把这些文字内容转置整型的数字，比如`是`这个字在数据字典中排在第5，那么它的标签就是4，标签从0开始。至于最后生成的均值和标准值，因为我们的数据在训练之前还需要归一化，因为每个数据的分布不一样，不同图像，最大最小值都是确定的，所以我们要统计一批数据来计算均值和标准值，之后的数据的归一化都使用这个均值和标准值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "-----------  Configuration Arguments -----------\n",
      "annotation_path: dataset/annotation/\n",
      "count_threshold: 0\n",
      "is_change_frame_rate: True\n",
      "manifest_path: dataset/manifest.train\n",
      "manifest_prefix: dataset/\n",
      "vocab_path: dataset/zh_vocab.json\n",
      "------------------------------------------------\n",
      "开始生成数据列表...\n",
      "100%|███████████████████████████████████| 13388/13388 [00:06<00:00, 2131.76it/s]\n",
      "完成生成数据列表，数据集总长度为34.16小时！\n",
      "开始生成数据字典...\n",
      "100%|██████████████████████████████████| 13254/13254 [00:00<00:00, 17726.18it/s]\n",
      "数据字典生成完成！\n",
      "开始抽取10%的数据计算均值和标准值...\n",
      "100%|█████████████████████████████████████| 13254/13254 [15:32<00:00, 14.21it/s]\n",
      "【特别重要】：均值：-2.427870, 标准值：44.181725, 请根据这两个值修改训练参数！\n"
     ]
    }
   ],
   "source": [
    "!python3 create_manifest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以用使用`python create_manifest.py --help`命令查看各个参数的说明和默认值。\n",
    "```shell\n",
    "usage: create_manifest.py [-h] [----annotation_path ANNOTATION_PATH]\n",
    "                          [--manifest_prefix MANIFEST_PREFIX]\n",
    "                          [--is_change_frame_rate IS_CHANGE_FRAME_RATE]\n",
    "                          [--min_duration MIN_DURATION]\n",
    "                          [--max_duration MAX_DURATION]\n",
    "                          [--count_threshold COUNT_THRESHOLD]\n",
    "                          [--vocab_path VOCAB_PATH]\n",
    "                          [--manifest_path MANIFEST_PATH]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  ----annotation_path ANNOTATION_PATH\n",
    "                        标注文件的路径 默认: dataset/annotation/.\n",
    "  --manifest_prefix MANIFEST_PREFIX\n",
    "                        训练数据清单，包括音频路径和标注信息 默认: dataset/.\n",
    "  --is_change_frame_rate IS_CHANGE_FRAME_RATE\n",
    "                        是否统一改变音频为16000Hz，这会消耗大量的时间 默认: True.\n",
    "  --min_duration MIN_DURATION\n",
    "                        过滤最短的音频长度 默认: 0.\n",
    "  --max_duration MAX_DURATION\n",
    "                        过滤最长的音频长度，当为-1的时候不限制长度 默认: 20.\n",
    "  --count_threshold COUNT_THRESHOLD\n",
    "                        字符计数的截断阈值，0为不做限制 默认: 0.\n",
    "  --vocab_path VOCAB_PATH\n",
    "                        生成的数据字典文件 默认: dataset/zh_vocab.json.\n",
    "  --manifest_path MANIFEST_PATH\n",
    "                        数据列表路径 默认: dataset/manifest.train.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、训练模型\n",
    "创建数据列表之后就可以就可以开始训练语音识别模型了。\n",
    "\n",
    "## 4.1 创建模型\n",
    "PPASR模型是一个只使用卷积层的模型，并没有使用更加复杂的RNN模型，以下就是使用PaddlePaddle实现的一个语音识别模型。使用动态图自定义网络模型非常简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/model.py\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from paddle.nn.initializer import KaimingNormal\r\n",
    "\r\n",
    "\r\n",
    "# 门控线性单元 Gated Linear Units (GLU)\r\n",
    "class GLU(nn.Layer):\r\n",
    "    def __init__(self, axis):\r\n",
    "        super(GLU, self).__init__()\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "        self.axis = axis\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        a, b = paddle.split(x, num_or_sections=2, axis=self.axis)\r\n",
    "        act_b = self.sigmoid(b)\r\n",
    "        out = paddle.multiply(x=a, y=act_b)\r\n",
    "        return out\r\n",
    "\r\n",
    "\r\n",
    "# 基本卷积块\r\n",
    "class ConvBlock(nn.Layer):\r\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, p=0.5):\r\n",
    "        super(ConvBlock, self).__init__()\r\n",
    "        self.conv = nn.Conv1D(in_channels, out_channels, kernel_size, stride, padding, weight_attr=KaimingNormal())\r\n",
    "        self.conv = nn.utils.weight_norm(self.conv)\r\n",
    "        self.act = GLU(axis=1)\r\n",
    "        self.dropout = nn.Dropout(p)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.dropout(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "# PPASR模型\r\n",
    "class PPASR(nn.Layer):\r\n",
    "    def __init__(self, vocabulary, data_mean=None, data_std=None, name=\"PPASR\"):\r\n",
    "        super(PPASR, self).__init__(name_scope=name)\r\n",
    "        # 数据均值和标准值到模型中，方便以后推理使用\r\n",
    "        if data_mean is None:\r\n",
    "            data_mean = paddle.to_tensor(1.0)\r\n",
    "        if data_std is None:\r\n",
    "            data_std = paddle.to_tensor(1.0)\r\n",
    "        self.register_buffer(\"data_mean\", data_mean, persistable=True)\r\n",
    "        self.register_buffer(\"data_std\", data_std, persistable=True)\r\n",
    "        # 模型的输出大小，字典大小+1\r\n",
    "        self.output_units = len(vocabulary) + 1\r\n",
    "        self.conv1 = ConvBlock(128, 500, 48, 2, padding=97, p=0.2)\r\n",
    "        self.conv2 = ConvBlock(250, 500, 7, 1, p=0.3)\r\n",
    "        self.conv3 = ConvBlock(250, 2000, 32, 1, p=0.3)\r\n",
    "        self.conv4 = ConvBlock(1000, 2000, 1, 1, p=0.3)\r\n",
    "        self.out = nn.utils.weight_norm(nn.Conv1D(1000, self.output_units, 1, 1))\r\n",
    "\r\n",
    "    def forward(self, x, input_lens=None):\r\n",
    "        x = self.conv1(x)\r\n",
    "        for i in range(7):\r\n",
    "            x = self.conv2(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = self.out(x)\r\n",
    "        if input_lens is not None:\r\n",
    "            return x, paddle.to_tensor(input_lens / 2 + 1, dtype='int64')\r\n",
    "        return x\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.2 解码方法\n",
    "\n",
    "模型输出的结果需要解码才能得到真实的文本结果，本项目使用的是贪心策略解码方法，贪心策略是在每一步选择概率最大的输出值，这样就可以得到最终解码的输出序列。然而，CTC网络的输出序列只对应了搜索空间的一条路径，一个最终标签可对应搜索空间的N条路径，所以概率最大的路径并不等于最终标签的概率最大，即不是最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/decoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/decoder.py\r\n",
    "import Levenshtein as Lev\r\n",
    "import paddle\r\n",
    "\r\n",
    "\r\n",
    "class GreedyDecoder(object):\r\n",
    "    def __init__(self, vocabulary, blank_index=0):\r\n",
    "        self.int_to_char = dict([(i, c) for (i, c) in enumerate(vocabulary)])\r\n",
    "        self.blank_index = blank_index\r\n",
    "\r\n",
    "    # 给定一个数字序列列表，返回相应的字符串\r\n",
    "    def convert_to_strings(self, sequences, sizes=None, remove_repetitions=False, return_offsets=False):\r\n",
    "        strings = []\r\n",
    "        offsets = [] if return_offsets else None\r\n",
    "        for x in range(len(sequences)):\r\n",
    "            seq_len = sizes[x] if sizes is not None else len(sequences[x])\r\n",
    "            string, string_offsets = self.process_string(sequences[x], seq_len, remove_repetitions)\r\n",
    "            strings.append([string])\r\n",
    "            if return_offsets:\r\n",
    "                offsets.append([string_offsets])\r\n",
    "        if return_offsets:\r\n",
    "            return strings, offsets\r\n",
    "        else:\r\n",
    "            return strings\r\n",
    "\r\n",
    "    # 获取字符，并删除重复的字符\r\n",
    "    def process_string(self, sequence, size, remove_repetitions=False):\r\n",
    "        string = \"\"\r\n",
    "        offsets = []\r\n",
    "        sequence = sequence.numpy()\r\n",
    "        for i in range(size):\r\n",
    "            char = self.int_to_char[sequence[i].item()]\r\n",
    "            if char != self.int_to_char[self.blank_index]:\r\n",
    "                # 是否删除重复的字符\r\n",
    "                if remove_repetitions and i != 0 and char == self.int_to_char[sequence[i - 1].item()]:\r\n",
    "                    pass\r\n",
    "                else:\r\n",
    "                    string = string + char\r\n",
    "                    offsets.append(i)\r\n",
    "        return string, paddle.to_tensor(offsets, dtype='int64')\r\n",
    "\r\n",
    "    def cer(self, s1, s2):\r\n",
    "        \"\"\"\r\n",
    "       通过计算两个字符串的距离，得出字错率\r\n",
    "        \"\"\"\r\n",
    "        s1, s2, = s1.replace(\" \", \"\"), s2.replace(\" \", \"\")\r\n",
    "        return Lev.distance(s1, s2)\r\n",
    "\r\n",
    "    def decode(self, probs, sizes=None):\r\n",
    "        \"\"\"\r\n",
    "        解码，传入结果的概率解码得到字符串，删除序列中的重复元素和空格。\r\n",
    "        \"\"\"\r\n",
    "        max_probs = paddle.argmax(probs, 2)\r\n",
    "        strings, offsets = self.convert_to_strings(\r\n",
    "            max_probs,\r\n",
    "            sizes,\r\n",
    "            remove_repetitions=True,\r\n",
    "            return_offsets=True)\r\n",
    "        return strings, offsets\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.3 数据读取\n",
    "在读取音频数据的同时也在做预处理，本项目主要是将音频执行梅尔频率倒谱系数(MFCCs)处理，然后在使用出来的数据进行训练，在读取音频时，使用`librosa.load(wav_path, sr=16000)`函数读取音频文件，再使用`librosa.feature.mfcc()`执行数据处理。MFCC全称梅尔频率倒谱系数。梅尔频率是基于人耳听觉特性提出来的， 它与Hz频率成非线性对应关系。梅尔频率倒谱系数(MFCC)则是利用它们之间的这种关系，计算得到的Hz频谱特征，主要计算方式分别是预加重，分帧，加窗，快速傅里叶变换(FFT)，梅尔滤波器组，离散余弦变换(DCT)，最后提取语音数据特征和降低运算维度。本项目使用的全部音频的采样率都是16000Hz，如果其他采样率的音频都需要转为16000Hz，`create_manifest.py`程序也提供了把音频转为16000Hz。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/data.py\r\n",
    "import json\r\n",
    "import wave\r\n",
    "\r\n",
    "import librosa\r\n",
    "import numpy as np\r\n",
    "import soundfile\r\n",
    "from paddle.io import Dataset\r\n",
    "\r\n",
    "\r\n",
    "# 加载二进制音频文件，转成短时傅里叶变换\r\n",
    "def load_audio_stft(wav_path, mean=None, std=None):\r\n",
    "    with wave.open(wav_path) as wav:\r\n",
    "        wav = np.frombuffer(wav.readframes(wav.getnframes()), dtype=\"int16\").astype(\"float32\")\r\n",
    "    stft = librosa.stft(wav, n_fft=255, hop_length=160, win_length=200, window=\"hamming\")\r\n",
    "    spec, phase = librosa.magphase(stft)\r\n",
    "    spec = np.log1p(spec)\r\n",
    "    if mean is not None and std is not None:\r\n",
    "        spec = (spec - mean) / std\r\n",
    "    return spec\r\n",
    "\r\n",
    "\r\n",
    "# 读取音频文件转成梅尔频率倒谱系数(MFCCs)\r\n",
    "def load_audio_mfcc(wav_path, mean=None, std=None):\r\n",
    "    wav, sr = librosa.load(wav_path, sr=16000)\r\n",
    "    mfccs = librosa.feature.mfcc(y=wav, sr=sr, n_mfcc=128, n_fft=512, hop_length=128).astype(\"float32\")\r\n",
    "    if mean is not None and std is not None:\r\n",
    "        mfccs = (mfccs - mean) / std\r\n",
    "    return mfccs\r\n",
    "\r\n",
    "\r\n",
    "# 改变音频采样率为16000Hz\r\n",
    "def change_rate(audio_path):\r\n",
    "    data, sr = soundfile.read(audio_path)\r\n",
    "    if sr != 16000:\r\n",
    "        data = librosa.resample(data, sr, target_sr=16000)\r\n",
    "        soundfile.write(audio_path, data, samplerate=16000)\r\n",
    "\r\n",
    "\r\n",
    "# 音频数据加载器\r\n",
    "class PPASRDataset(Dataset):\r\n",
    "    def __init__(self, data_list, dict_path, mean=None, std=None, min_duration=0, max_duration=-1):\r\n",
    "        super(PPASRDataset, self).__init__()\r\n",
    "        self.mean = mean\r\n",
    "        self.std = std\r\n",
    "        # 获取数据列表\r\n",
    "        with open(data_list, 'r', encoding='utf-8') as f:\r\n",
    "            lines = f.readlines()\r\n",
    "        self.data_list = []\r\n",
    "        for line in lines:\r\n",
    "            line = json.loads(line)\r\n",
    "            # 跳过超出长度限制的音频\r\n",
    "            if line[\"duration\"] < min_duration:\r\n",
    "                continue\r\n",
    "            if max_duration != -1 and line[\"duration\"] > max_duration:\r\n",
    "                continue\r\n",
    "            self.data_list.append([line[\"audio_path\"], line[\"text\"]])\r\n",
    "        # 加载数据字典\r\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\r\n",
    "            labels = eval(f.read())\r\n",
    "        self.vocabulary = dict([(labels[i], i) for i in range(len(labels))])\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # 分割音频路径和标签\r\n",
    "        wav_path, transcript = self.data_list[idx]\r\n",
    "        # 读取音频并转换为梅尔频率倒谱系数(MFCCs)\r\n",
    "        mfccs = load_audio_mfcc(wav_path, self.mean, self.std)\r\n",
    "        # 将字符标签转换为int数据\r\n",
    "        transcript = list(filter(None, [self.vocabulary.get(x) for x in transcript]))\r\n",
    "        transcript = np.array(transcript, dtype='int32')\r\n",
    "        return mfccs, transcript\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data_list)\r\n",
    "\r\n",
    "\r\n",
    "# 对一个batch的数据处理\r\n",
    "def collate_fn(batch):\r\n",
    "    # 找出音频长度最长的\r\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].shape[1], reverse=True)\r\n",
    "    freq_size = batch[0][0].shape[0]\r\n",
    "    max_audio_length = batch[0][0].shape[1]\r\n",
    "    batch_size = len(batch)\r\n",
    "    # 找出标签最长的\r\n",
    "    batch_temp = sorted(batch, key=lambda sample: len(sample[1]), reverse=True)\r\n",
    "    max_label_length = len(batch_temp[0][1])\r\n",
    "    # 以最大的长度创建0张量\r\n",
    "    inputs = np.zeros((batch_size, freq_size, max_audio_length), dtype='float32')\r\n",
    "    labels = np.zeros((batch_size, max_label_length), dtype='int32')\r\n",
    "    input_lens = []\r\n",
    "    label_lens = []\r\n",
    "    for x in range(batch_size):\r\n",
    "        sample = batch[x]\r\n",
    "        tensor = sample[0]\r\n",
    "        target = sample[1]\r\n",
    "        seq_length = tensor.shape[1]\r\n",
    "        label_length = target.shape[0]\r\n",
    "        # 将数据插入都0张量中，实现了padding\r\n",
    "        inputs[x, :, :seq_length] = tensor[:, :]\r\n",
    "        labels[x, :label_length] = target[:]\r\n",
    "        input_lens.append(seq_length)\r\n",
    "        label_lens.append(len(target))\r\n",
    "    input_lens = np.array(input_lens, dtype='int64')\r\n",
    "    label_lens = np.array(label_lens, dtype='int64')\r\n",
    "    return inputs, labels, input_lens, label_lens\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.4 开始训练\n",
    "开始训练语音识别模型， 每训练一轮保存一次模型，模型保存在`models/`目录下，测试使用的是贪心解码路径解码方法。本项目支持多卡训练，在没有指定`CUDA_VISIBLE_DEVICES`时，会使用全部的GPU进行执行训练，也可以指定某几个GPU训练，如`export CUDA_VISIBLE_DEVICES=0,1`指定使用第1张和第2张显卡训练。除了参数`data_mean`和`data_std`需要根据计算的结果修改，其他的参数一般不需要改动，参数`num_workers`可以更加CPU的核数修改，这个参数是指定使用多少个线程读取数据。参数`pretrained_model`是指定预训练模型所在的文件夹，如果使用训练模型，必须使用跟预训练配套的数据字典，原因是，其一，数据字典的大小指定了模型的输出大小，如果使用了其他更大的数据字典，预训练模型就无法完全加载。其二，数值字典定义了文字的ID，不同的数据字典文字的ID可能不一样，这样预训练模型的作用就不是那么大了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size的第三个参数是变长的，这里为了能查看输出的大小变化，指定了一个值！\n",
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv1D-21      [[32, 128, 500]]      [32, 500, 324]       3,073,000   \n",
      "  Sigmoid-17      [[32, 250, 324]]      [32, 250, 324]           0       \n",
      "    GLU-17        [[32, 500, 324]]      [32, 250, 324]           0       \n",
      "  Dropout-17      [[32, 250, 324]]      [32, 250, 324]           0       \n",
      " ConvBlock-17     [[32, 128, 500]]      [32, 250, 324]           0       \n",
      "   Conv1D-22      [[32, 250, 288]]      [32, 500, 282]        876,000    \n",
      "  Sigmoid-18      [[32, 250, 282]]      [32, 250, 282]           0       \n",
      "    GLU-18        [[32, 500, 282]]      [32, 250, 282]           0       \n",
      "  Dropout-18      [[32, 250, 282]]      [32, 250, 282]           0       \n",
      " ConvBlock-18     [[32, 250, 288]]      [32, 250, 282]           0       \n",
      "   Conv1D-23      [[32, 250, 282]]     [32, 2000, 251]      16,004,000   \n",
      "  Sigmoid-19     [[32, 1000, 251]]     [32, 1000, 251]           0       \n",
      "    GLU-19       [[32, 2000, 251]]     [32, 1000, 251]           0       \n",
      "  Dropout-19     [[32, 1000, 251]]     [32, 1000, 251]           0       \n",
      " ConvBlock-19     [[32, 250, 282]]     [32, 1000, 251]           0       \n",
      "   Conv1D-24     [[32, 1000, 251]]     [32, 2000, 251]       2,004,000   \n",
      "  Sigmoid-20     [[32, 1000, 251]]     [32, 1000, 251]           0       \n",
      "    GLU-20       [[32, 2000, 251]]     [32, 1000, 251]           0       \n",
      "  Dropout-20     [[32, 1000, 251]]     [32, 1000, 251]           0       \n",
      " ConvBlock-20    [[32, 1000, 251]]     [32, 1000, 251]           0       \n",
      "   Conv1D-25     [[32, 1000, 251]]     [32, 2883, 251]       2,888,766   \n",
      "===========================================================================\n",
      "Total params: 24,845,766\n",
      "Trainable params: 24,845,766\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 7.81\n",
      "Forward/backward pass size (MB): 1133.94\n",
      "Params size (MB): 94.78\n",
      "Estimated Total Size (MB): 1236.54\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0: PiecewiseDecay set learning rate to 0.001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-12 10:47:56.302111] Train epoch 0, batch 0, loss: 168.749115\n",
      "[2021-05-12 11:02:46.394364] Train epoch 0, batch 100, loss: 7.148998\n"
     ]
    }
   ],
   "source": [
    "import argparse\r\n",
    "import functools\r\n",
    "import os\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.distributed as dist\r\n",
    "from paddle.io import DataLoader\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "from utils.data import PPASRDataset, collate_fn\r\n",
    "from utils.decoder import GreedyDecoder\r\n",
    "from utils.model import PPASR\r\n",
    "\r\n",
    "\r\n",
    "# 训练的批量大小\r\n",
    "batch_size = 32\r\n",
    "# 读取数据的线程数量\r\n",
    "num_workers = 4\r\n",
    "# 训练的轮数\r\n",
    "num_epoch = 100\r\n",
    "# 初始学习率的大小\r\n",
    "learning_rate = 1e-3\r\n",
    "# 数据集的均值\r\n",
    "data_mean = -2.427870\r\n",
    "# 数据集的标准值\r\n",
    "data_std = 44.181725\r\n",
    "# 过滤最短的音频长度\r\n",
    "min_duration = 0\r\n",
    "# 过滤最短的音频长度\r\n",
    "max_duration = 20\r\n",
    "# 训练数据的数据列表路径\r\n",
    "train_manifest = 'dataset/manifest.train'\r\n",
    "# 测试数据的数据列表路径\r\n",
    "test_manifest = 'dataset/manifest.test'\r\n",
    "# 数据字典的路径\r\n",
    "dataset_vocab = 'dataset/zh_vocab.json'\r\n",
    "# 模型保存的路径\r\n",
    "save_model = 'models/'\r\n",
    "# 预训练模型的路径，当为None则不使用预训练模型\r\n",
    "pretrained_model = None\r\n",
    "\r\n",
    "\r\n",
    "# 评估模型\r\n",
    "def evaluate(model, test_loader, greedy_decoder):\r\n",
    "    cer = []\r\n",
    "    for batch_id, (inputs, labels, _, _) in enumerate(test_loader()):\r\n",
    "        # 执行识别\r\n",
    "        outs = model(inputs)\r\n",
    "        outs = paddle.nn.functional.softmax(outs, 1)\r\n",
    "        outs = paddle.transpose(outs, perm=[0, 2, 1])\r\n",
    "        # 解码获取识别结果\r\n",
    "        out_strings, out_offsets = greedy_decoder.decode(outs)\r\n",
    "        labels = greedy_decoder.convert_to_strings(labels)\r\n",
    "        for out_string, label in zip(*(out_strings, labels)):\r\n",
    "            # 计算字错率\r\n",
    "            c = greedy_decoder.cer(out_string[0], label[0]) / float(len(label[0]))\r\n",
    "            cer.append(c)\r\n",
    "    cer = float(np.mean(cer))\r\n",
    "    return cer\r\n",
    "\r\n",
    "\r\n",
    "# 保存模型\r\n",
    "def save_model(epoch, model, optimizer):\r\n",
    "    model_path = os.path.join(save_model, 'epoch_%d' % epoch)\r\n",
    "    if epoch == num_epoch - 1:\r\n",
    "        model_path = os.path.join(save_model, 'step_final')\r\n",
    "    if not os.path.exists(model_path):\r\n",
    "        os.makedirs(model_path)\r\n",
    "    paddle.save(model.state_dict(), os.path.join(model_path, 'model.pdparams'))\r\n",
    "    paddle.save(optimizer.state_dict(), os.path.join(model_path, 'optimizer.pdopt'))\r\n",
    "\r\n",
    "\r\n",
    "def train():\r\n",
    "    # 日志记录器\r\n",
    "    writer = LogWriter(logdir='log')\r\n",
    "    # 设置支持多卡训练\r\n",
    "    dist.init_parallel_env()\r\n",
    "    # 获取训练数据\r\n",
    "    train_dataset = PPASRDataset(train_manifest, dataset_vocab,\r\n",
    "                                 mean=data_mean,\r\n",
    "                                 std=data_std,\r\n",
    "                                 min_duration=min_duration,\r\n",
    "                                 max_duration=max_duration)\r\n",
    "    train_loader = DataLoader(dataset=train_dataset,\r\n",
    "                              batch_size=batch_size,\r\n",
    "                              collate_fn=collate_fn,\r\n",
    "                              num_workers=num_workers,\r\n",
    "                              use_shared_memory=False)\r\n",
    "    train_loader_shuffle = DataLoader(dataset=train_dataset,\r\n",
    "                                      batch_size=batch_size,\r\n",
    "                                      collate_fn=collate_fn,\r\n",
    "                                      num_workers=num_workers,\r\n",
    "                                      shuffle=True,\r\n",
    "                                      use_shared_memory=False)\r\n",
    "    # 获取测试数据\r\n",
    "    test_dataset = PPASRDataset(test_manifest, dataset_vocab, mean=data_mean, std=data_std)\r\n",
    "    test_loader = DataLoader(dataset=test_dataset,\r\n",
    "                             batch_size=batch_size,\r\n",
    "                             collate_fn=collate_fn,\r\n",
    "                             num_workers=num_workers,\r\n",
    "                             use_shared_memory=False)\r\n",
    "    # 获取解码器，用于评估\r\n",
    "    greedy_decoder = GreedyDecoder(train_dataset.vocabulary)\r\n",
    "    # 获取模型，同时数据均值和标准值到模型中，方便以后推理使用\r\n",
    "    model = PPASR(train_dataset.vocabulary, data_mean=paddle.to_tensor(data_mean), data_std=paddle.to_tensor(data_std))\r\n",
    "    print('input_size的第三个参数是变长的，这里为了能查看输出的大小变化，指定了一个值！')\r\n",
    "    paddle.summary(model, input_size=(batch_size, 128, 500))\r\n",
    "    # 设置支持多卡训练\r\n",
    "    model = paddle.DataParallel(model)\r\n",
    "    # 设置优化方法\r\n",
    "    clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)\r\n",
    "    boundaries = [10, 20, 50]\r\n",
    "    lr = [0.1 ** l * learning_rate for l in range(len(boundaries) + 1)]\r\n",
    "    scheduler = paddle.optimizer.lr.PiecewiseDecay(boundaries=boundaries, values=lr, verbose=True)\r\n",
    "    optimizer = paddle.optimizer.Adam(parameters=model.parameters(),\r\n",
    "                                      learning_rate=scheduler,\r\n",
    "                                      grad_clip=clip)\r\n",
    "    # 获取损失函数\r\n",
    "    ctc_loss = paddle.nn.CTCLoss()\r\n",
    "    # 加载预训练模型\r\n",
    "    if pretrained_model is not None:\r\n",
    "        model.set_state_dict(paddle.load(os.path.join(pretrained_model, 'model.pdparams')))\r\n",
    "        optimizer.set_state_dict(paddle.load(os.path.join(pretrained_model, 'optimizer.pdopt')))\r\n",
    "    train_step = 0\r\n",
    "    test_step = 0\r\n",
    "    # 开始训练\r\n",
    "    for epoch in range(num_epoch):\r\n",
    "        # 第一个epoch不打乱数据\r\n",
    "        if epoch == 1:\r\n",
    "            train_loader = train_loader_shuffle\r\n",
    "        for batch_id, (inputs, labels, input_lens, label_lens) in enumerate(train_loader()):\r\n",
    "            out, out_lens = model(inputs, input_lens)\r\n",
    "            out = paddle.transpose(out, perm=[2, 0, 1])\r\n",
    "            # 计算损失\r\n",
    "            loss = ctc_loss(out, labels, out_lens, label_lens)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "            # 多卡训练只使用一个进程打印\r\n",
    "            if batch_id % 100 == 0 and dist.get_rank() == 0:\r\n",
    "                print('[%s] Train epoch %d, batch %d, loss: %f' % (datetime.now(), epoch, batch_id, loss))\r\n",
    "                writer.add_scalar('Train loss', loss, train_step)\r\n",
    "                train_step += 1\r\n",
    "            # 固定步数也要保存一次模型\r\n",
    "            if batch_id % 2000 == 0 and batch_id != 0:\r\n",
    "                # 保存模型\r\n",
    "                save_model(epoch=epoch, model=model, optimizer=optimizer)\r\n",
    "        # 执行评估\r\n",
    "        model.eval()\r\n",
    "        cer = evaluate(model, test_loader, greedy_decoder)\r\n",
    "        print('[%s] Test epoch %d, cer: %f' % (datetime.now(), epoch, cer))\r\n",
    "        writer.add_scalar('Test cer', cer, test_step)\r\n",
    "        test_step += 1\r\n",
    "        model.train()\r\n",
    "        # 记录学习率\r\n",
    "        writer.add_scalar('Learning rate', scheduler.last_lr, epoch)\r\n",
    "        # 保存模型\r\n",
    "        save_model(epoch=epoch, model=model, optimizer=optimizer)\r\n",
    "        scheduler.step()\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在训练过程中会保存VisualDL日志到log文件夹中，可以通过VisualDL可视化功能查看\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c7516b345d0843c1befb52fada3ed2e4f7c707e537604ca88b803f5c7ac87672)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、评估和预测\n",
    "\n",
    "在评估和预测中，对结果解码的贪心策略解码方法，贪心策略是在每一步选择概率最大的输出值，这样就可以得到最终解码的输出序列。然而，CTC网络的输出序列只对应了搜索空间的一条路径，一个最终标签可对应搜索空间的N条路径，所以概率最大的路径并不等于最终标签的概率最大，即不是最优解。但贪心策略是最简单易懂且快速地一种方法。在语音识别上使用最多的解码方法还有定向搜索策略，这种策略准确率更高，同时也相对复杂，解码速度也相对慢很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1 评估\n",
    "我们可以使用这个脚本对模型进行评估，通过字符错误率来评价模型的性能。目前只支持贪心策略解码方法。在评估中音频预处理的`mean`和`std`需要跟训练时一样，但这里不需要开发者手动指定，因为这两个参数在训练的时候就已经保持在模型中，这时只需从模型中读取这两个参数的值就可以。参数`model_path`指定模型所在的文件夹的路径。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:34<00:00,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别时间：35071ms，字错率：0.025435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\r\n",
    "import functools\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "from tqdm import tqdm\r\n",
    "from utils.data import PPASRDataset, collate_fn\r\n",
    "from utils.decoder import GreedyDecoder\r\n",
    "from utils.model import PPASR\r\n",
    "\r\n",
    "# 训练的批量大小\r\n",
    "batch_size = 32\r\n",
    "# 读取数据的线程数量\r\n",
    "num_workers = 8\r\n",
    "# 测试数据的数据列表路径\r\n",
    "test_manifest = 'dataset/manifest.test'\r\n",
    "# 数据字典的路径\r\n",
    "dataset_vocab = 'dataset/zh_vocab.json'\r\n",
    "# 模型的路径\r\n",
    "model_path = 'models/step_final/'\r\n",
    "\r\n",
    "# 获取测试数据\r\n",
    "test_dataset = PPASRDataset(test_manifest, dataset_vocab)\r\n",
    "test_loader = DataLoader(dataset=test_dataset,\r\n",
    "                         batch_size=batch_size,\r\n",
    "                         collate_fn=collate_fn,\r\n",
    "                         num_workers=num_workers,\r\n",
    "                         use_shared_memory=False)\r\n",
    "# 获取解码器，用于评估\r\n",
    "greedy_decoder = GreedyDecoder(test_dataset.vocabulary)\r\n",
    "# 获取模型\r\n",
    "model = PPASR(test_dataset.vocabulary)\r\n",
    "model.set_state_dict(paddle.load(os.path.join(model_path, 'model.pdparams')))\r\n",
    "# 获取保存在模型中的数据均值和标准值，设置数据处理器\r\n",
    "test_dataset.mean = model.data_mean.numpy()[0]\r\n",
    "test_dataset.std = model.data_std.numpy()[0]\r\n",
    "model.eval()\r\n",
    "\r\n",
    "\r\n",
    "# 评估模型\r\n",
    "def evaluate():\r\n",
    "    cer = []\r\n",
    "    for batch_id, (inputs, labels, _, _) in enumerate(tqdm(test_loader())):\r\n",
    "        # 执行识别\r\n",
    "        outs = model(inputs)\r\n",
    "        outs = paddle.nn.functional.softmax(outs, 1)\r\n",
    "        outs = paddle.transpose(outs, perm=[0, 2, 1])\r\n",
    "        # 解码获取识别结果\r\n",
    "        out_strings, out_offsets = greedy_decoder.decode(outs)\r\n",
    "        labels = greedy_decoder.convert_to_strings(labels)\r\n",
    "        for out_string, label in zip(*(out_strings, labels)):\r\n",
    "            # 计算字错率\r\n",
    "            c = greedy_decoder.cer(out_string[0], label[0]) / float(len(label[0]))\r\n",
    "            cer.append(c)\r\n",
    "    cer = float(np.mean(cer))\r\n",
    "    return cer\r\n",
    "\r\n",
    "start = time.time()\r\n",
    "cer = evaluate()\r\n",
    "end = time.time()\r\n",
    "print('识别时间：%dms，字错率：%f' % (round((end - start) * 1000), cer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.2 预测\n",
    "我们可以使用这个脚本使用模型进行预测，通过传递音频文件的路径进行识别。在预测中音频预处理的`mean`和`std`需要跟训练时一样，但这里不需要开发者手动指定，因为这两个参数在训练的时候就已经保持在模型中，这时只需从模型中读取这两个参数的值就可以。参数`model_path`指定模型所在的文件夹的路径，参数`wav_path`指定需要预测音频文件的路径。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别时间：795ms，识别结果：柳宗夏现年六十岁五十年代基入韩外交部工作一九九四年十二月任外交安保首席秘书\n"
     ]
    }
   ],
   "source": [
    "import functools\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "import paddle\r\n",
    "\r\n",
    "from utils.data import load_audio_mfcc\r\n",
    "from utils.decoder import GreedyDecoder\r\n",
    "from utils.model import PPASR\r\n",
    "\r\n",
    "\r\n",
    "# 用于识别的音频路径\r\n",
    "audio_path = 'dataset/test.wav'\r\n",
    "# 数据字典的路径\r\n",
    "dataset_vocab = 'dataset/zh_vocab.json'\r\n",
    "# 模型的路径\r\n",
    "model_path = 'models/step_final/'\r\n",
    "\r\n",
    "# 加载数据字典\r\n",
    "with open(dataset_vocab, 'r', encoding='utf-8') as f:\r\n",
    "    labels = eval(f.read())\r\n",
    "vocabulary = dict([(labels[i], i) for i in range(len(labels))])\r\n",
    "# 获取解码器\r\n",
    "greedy_decoder = GreedyDecoder(vocabulary)\r\n",
    "\r\n",
    "# 创建模型\r\n",
    "model = PPASR(vocabulary)\r\n",
    "model.set_state_dict(paddle.load(os.path.join(model_path, 'model.pdparams')))\r\n",
    "# 获取保存在模型中的数据均值和标准值\r\n",
    "data_mean = model.data_mean.numpy()[0]\r\n",
    "data_std = model.data_std.numpy()[0]\r\n",
    "model.eval()\r\n",
    "\r\n",
    "\r\n",
    "def infer():\r\n",
    "    # 读取音频文件转成梅尔频率倒谱系数(MFCCs)\r\n",
    "    mfccs = load_audio_mfcc(audio_path, mean=data_mean, std=data_std)\r\n",
    "\r\n",
    "    mfccs = paddle.to_tensor(mfccs, dtype='float32')\r\n",
    "    mfccs = paddle.unsqueeze(mfccs, axis=0)\r\n",
    "    # 执行识别\r\n",
    "    out = model(mfccs)\r\n",
    "    out = paddle.nn.functional.softmax(out, 1)\r\n",
    "    out = paddle.transpose(out, perm=[0, 2, 1])\r\n",
    "    # 执行解码\r\n",
    "    out_string, out_offset = greedy_decoder.decode(out)\r\n",
    "    return out_string[0][0]\r\n",
    "\r\n",
    "\r\n",
    "start = time.time()\r\n",
    "result_text = infer()\r\n",
    "end = time.time()\r\n",
    "print('识别时间：%dms，识别结果：%s' % (round((end - start) * 1000), result_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、参考资料\n",
    "\n",
    "1. https://github.com/yeyupiaoling/MASR\n",
    "2. https://www.paddlepaddle.org.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
