{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **强化学习——Deep Deterministic Policy Gradient (DDPG)**\n",
    "**作者：**[EastSmith](https://github.com/EastSmith)\n",
    "\n",
    "**日期：** 2021.06\n",
    "\n",
    "**AI Studio项目**：[点击体验](https://aistudio.baidu.com/aistudio/projectdetail/1702021)\n",
    "\n",
    "## **一、介绍**\n",
    "\n",
    "### 深度确定策略梯度（Deep Deterministic Policy Gradient，DDPG）\n",
    "* 它是一种学习连续动作的无模型策略算法。\n",
    "* 它结合了DPG（确定性策略梯度）和DQN（深度Q网络）的思想。它利用DQN中的经验重放和延迟更新的目标网络，并基于DPG，可以在连续的动作空间上运行。\n",
    "\n",
    "### 要解决的问题\n",
    "* 你正试图解决经典的倒立摆控制问题。在此设置中，你只能执行两个操作：向左摆动或向右摆动。\n",
    "* 对于Q-学习算法来说，这个问题的挑战在于动作是连续的而不是离散的。也就是说，你必须从-2到+2的无限操作中进行选择，而不是使用像-1或+1这样的两个离散操作。\n",
    "\n",
    "### 快速理论\n",
    "\n",
    "* 就像**演员-评论家**的方法一样，它有两个网络：\n",
    "\n",
    "\t**演员**-它提出一个给定状态的动作。\n",
    "\n",
    "\t**评论家**-它预测给定的状态和动作是好（正值）还是坏（负值）。\n",
    "\n",
    "* DDPG使用的另外2种技术：\n",
    "\t**首先，它使用两个目标网络。**\n",
    "\n",
    "\t为什么？因为它增加了训练的稳定性。简言之，它是从估计的目标和目标网络学习更新，从而保持它估计的目标稳定。 \n",
    "\n",
    "\t从概念上讲，这就像是说，“我有一个如何玩这个好主意，我要尝试一下，直到我找到更好的东西”，而不是说“我要在每一个动作之后重新学习如何玩好整个游戏”。\n",
    "\n",
    "\t**第二，使用经验回放。**\n",
    "\n",
    "\t它存储元组列表（状态、动作、奖励、下一个状态），而不是仅仅从最近的经验中学习，而是从取样中学习到迄今为止积累的所有经验。\n",
    "    \n",
    "### 现在，看看它是如何实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、环境配置**\n",
    "本教程基于Paddle 2.1 编写，如果您的环境不是本版本，请先参考官网[安装](https://www.paddlepaddle.org.cn/install/quick) Paddle 2.1 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from itertools import count\n",
    "from paddle.distribution import Normal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import paddle.nn.functional as F\n",
    "from visualdl import LogWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、实施深度确定策略梯度网络（Deep Deterministic Policy Gradient，DDPG）**\n",
    "* **这里定义了演员和评论家网络。这些都是具有ReLU激活的基本全连接模型。**  \n",
    "**注意**：你需要为Actor的最后一层使用tanh激活，将值映射到-1到1之间。\n",
    "* **Memory类定义了经验回放池。**  \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cf262e0efe394b78aa6e9ef094f78d6dedaf9edb3cb54559b70893236cd1e16c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义评论家网络结构\n",
    "# DDPG这种方法与Q学习紧密相关，可以看作是连续动作空间的深度Q学习。 \n",
    "class Critic(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 256)\n",
    "        self.fc2 = nn.Linear(256 + 1, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = paddle.concat((x, a), axis=1)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 定义演员网络结构\n",
    "# 为了使DDPG策略更好地进行探索，在训练时对其行为增加了干扰。 原始DDPG论文的作者建议使用时间相关的 OU噪声 ，\n",
    "# 但最近的结果表明，不相关的均值零高斯噪声效果很好。 由于后者更简单，因此是首选。\n",
    "class Actor(nn.Layer):\n",
    "    def __init__(self, is_train=True):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.noisy = Normal(0, 0.2)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def select_action(self, epsilon, state):\n",
    "        state = paddle.to_tensor(state,dtype=\"float32\").unsqueeze(0)\n",
    "        with paddle.no_grad():\n",
    "            action = self.forward(state).squeeze() + self.is_train * epsilon * self.noisy.sample([1]).squeeze(0)\n",
    "        return 2 * paddle.clip(action, -1, 1).numpy()\n",
    "\n",
    "# 重播缓冲区:这是智能体以前的经验， 为了使算法具有稳定的行为，重播缓冲区应该足够大以包含广泛的体验。\n",
    "# 如果仅使用最新数据，则可能会过分拟合，如果使用过多的经验，则可能会减慢模型的学习速度。 这可能需要一些调整才能正确。 \n",
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = True):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、训练模型**\n",
    "### 算法伪代码\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9eded846e2d849d5a68e4078ee1ef3963bd8da71f9a94171aecb42919d74068d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, episode reward is -1749.9524052543404\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "# 定义软更新的函数\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.set_value( target_param * (1.0 - tau) + param * tau) \n",
    "       \n",
    "# 定义环境、实例化模型\n",
    "env = gym.make('Pendulum-v0')\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_target = Actor()\n",
    "critic_target = Critic()\n",
    "\n",
    "# 定义优化器\n",
    "critic_optim = paddle.optimizer.Adam(parameters=critic.parameters(), learning_rate=3e-5)\n",
    "actor_optim = paddle.optimizer.Adam(parameters=actor.parameters(), learning_rate=1e-5)\n",
    "\n",
    "# 定义超参数\n",
    "explore = 50000\n",
    "epsilon = 1\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "\n",
    "memory_replay = Memory(50000)\n",
    "begin_train = False\n",
    "batch_size = 32\n",
    "\n",
    "learn_steps = 0\n",
    "\n",
    "writer = LogWriter('logs')\n",
    "\n",
    "# 训练循环\n",
    "for epoch in count():\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for time_step in range(200):\n",
    "        action = actor.select_action(epsilon, state)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "        episode_reward += reward\n",
    "        reward = (reward + 8.1) / 8.1\n",
    "        memory_replay.add((state, next_state, action, reward))\n",
    "        if memory_replay.size() > 1280:\n",
    "\n",
    "            learn_steps += 1\n",
    "            if not begin_train:\n",
    "                print('train begin!')\n",
    "                begin_train = True\n",
    "            experiences = memory_replay.sample(batch_size, False)\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = zip(*experiences)\n",
    "\n",
    "            batch_state = paddle.to_tensor(batch_state,dtype=\"float32\")\n",
    "            batch_next_state = paddle.to_tensor(batch_next_state,dtype=\"float32\")\n",
    "            batch_action = paddle.to_tensor(batch_action,dtype=\"float32\").unsqueeze(1)\n",
    "            batch_reward = paddle.to_tensor(batch_reward,dtype=\"float32\").unsqueeze(1)\n",
    "\n",
    "\n",
    "            # 均方误差 y - Q(s, a) ， y是目标网络所看到的预期收益， 而 Q(s, a)是Critic网络预测的操作值。\n",
    "            # y是一个移动的目标，评论者模型试图实现的目标；这个目标通过缓慢的更新目标模型来保持稳定。 \n",
    "            with paddle.no_grad():\n",
    "                Q_next = critic_target(batch_next_state, actor_target(batch_next_state))\n",
    "                Q_target = batch_reward + gamma * Q_next\n",
    "\n",
    "            critic_loss = F.mse_loss(critic(batch_state, batch_action), Q_target)\n",
    "\n",
    "\n",
    "            critic_optim.clear_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            writer.add_scalar('critic loss', critic_loss.numpy(), learn_steps)\n",
    "            # 使用Critic网络给定值的平均值来评价Actor网络采取的行动。 我们力求使这一数值最大化。 \n",
    "            # 因此，我们更新了Actor网络，对于一个给定状态，它产生的动作尽量让Critic网络给出高的评分。 \n",
    "            critic.eval()\n",
    "            actor_loss = - critic(batch_state, actor(batch_state))\n",
    "            # print(actor_loss.shape)\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_optim.clear_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            critic.train()\n",
    "            writer.add_scalar('actor loss', actor_loss.numpy(), learn_steps)\n",
    "\n",
    "            soft_update(actor_target, actor, tau)  \n",
    "            soft_update(critic_target, critic, tau) \n",
    "\n",
    "\n",
    "        if epsilon > 0:\n",
    "            epsilon -= 1 / explore\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar('episode reward', episode_reward, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:{}, episode reward is {}'.format(epoch, episode_reward))\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        paddle.save(actor.state_dict(), 'model/ddpg-actor' + str(epoch) + '.para')\n",
    "        paddle.save(critic.state_dict(), 'model/ddpg-critic' + str(epoch) + '.para')\n",
    "        print('model saved!')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/6badbd1d51e74b62ac8d9e36f68e57828a8c776ee0e949feb5ca5d15fe4159b4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **五、效果展示**\n",
    "在训练的早期\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ad3d21267861495589172870e7ff7137236dfd57fd25435f88c8b3e8b4e90789)\n",
    "\n",
    "\n",
    "在训练的后期\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/68ded218781644148771e3f15e86b68b177497f57da94874bd282e7e838889f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **六、总结和建议** \n",
    "* DDPG中同时用到了“基于价值”与“基于策略”这两种思想。\n",
    "* experience replay memory的使用：actor同环境交互时，产生的transition数据序列是在时间上高度关联的，如果这些数据序列直接用于训练，会导致神经网络的过拟合，不易收敛。\n",
    "DDPG的actor将transition数据先存入经验缓冲池, 然后在训练时，从经验缓冲池中随机采样mini-batch数据，这样采样得到的数据可以认为是无关联的。\n",
    "* target 网络和online 网络的使用， 使得学习过程更加稳定，收敛更有保障。\n",
    "* 如果训练进行的正确，平均奖励将随着时间的推移而增加。请随意尝试演员和评论家网络的不同学习率、tau值和架构。\n",
    "* 倒立摆问题的复杂度较低，但DDPG在许多其它问题上都有很好的应用。另一个很好的环境是LunarLandingContinuous-v2，但是需要更多的训练才能获得好的效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Paddle2.1.1",
   "language": "python",
   "name": "paddle2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
