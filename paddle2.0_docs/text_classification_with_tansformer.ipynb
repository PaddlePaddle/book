{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 通过Transformer实现文本分类\n",
    "作者：[YinHang2515](https://github.com/YinHang2515)\n",
    "\n",
    "日期：2020年11月20日\n",
    "\n",
    "本示例教程演示如何使用Transformer模型在IMDB数据集上完成文本分类的任务。\n",
    "\n",
    "IMDB数据集是一个对电影评论标注为正向评论与负向评论的数据集，共有25000条文本数据作为训练集，25000条文本数据作为测试集。 该数据集的官方地址为： http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.fluid.dygraph as dg\n",
    "import paddle.fluid.layers as layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 处理数据集\n",
    "首先通过paddle内置的dataset完成数据集的导入，并构建字典和相应的reader\n",
    "\n",
    "然后通过padding的方式对同一个batch中长度不一致的数据进行补齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB word dict....\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB word dict....\")\r\n",
    "word_dict = paddle.dataset.imdb.word_dict()\r\n",
    "\r\n",
    "train_reader = paddle.dataset.imdb.train(word_dict)\r\n",
    "test_reader = paddle.dataset.imdb.test(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:0\n",
      "and:1\n",
      "a:2\n",
      "of:3\n",
      "to:4\n",
      "...\n",
      "virtual:5143\n",
      "warriors:5144\n",
      "widely:5145\n",
      "<unk>:5146\n",
      "<pad>:5147\n",
      "totally 5148 words\n"
     ]
    }
   ],
   "source": [
    "# 添加<pad>\r\n",
    "word_dict['<pad>'] = len(word_dict)\r\n",
    "\r\n",
    "for k in list(word_dict)[:5]:\r\n",
    "    print(\"{}:{}\".format(k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"...\")\r\n",
    "\r\n",
    "for k in list(word_dict)[-5:]:\r\n",
    "    print(\"{}:{}\".format(k if isinstance(k, str) else k.decode('ASCII'), word_dict[k]))\r\n",
    "\r\n",
    "print(\"totally {} words\".format(len(word_dict)))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_dict)  \r\n",
    "maxlen = 200  \r\n",
    "seq_len = 200\r\n",
    "batch_size = 32\r\n",
    "epochs = 2\r\n",
    "pad_id = word_dict['<pad>']\r\n",
    "embed_dim = 32  # Embedding size for each token\r\n",
    "num_heads = 2  # Number of attention heads\r\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
    "\r\n",
    "classes = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200)\n",
      "(25000, 1)\n",
      "(25000, 200)\n",
      "(25000, 1)\n",
      "<unk> has much in common with the third man another <unk> film set among the <unk> of <unk> europe like <unk> there is much inventive camera work there is an innocent american who gets emotionally involved with a woman he doesnt really understand and whose <unk> is all the more striking in contrast with the <unk> br but id have to say that the third man has a more <unk> storyline <unk> is a bit disjointed in this respect perhaps this is <unk> it is presented as a <unk> and making it too coherent would spoil the effect br br this movie is <unk> <unk> in more than one sense one never sees the sun shine grim but intriguing and frightening <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> is the most original movie ive seen in years if you like unique thrillers that are influenced by film noir then this is just the right cure for all of those hollywood summer <unk> <unk> the theaters these days von <unk> <unk> like breaking the waves have gotten more <unk> but this is really his best work it is <unk> without being distracting and offers the perfect combination of suspense and dark humor its too bad he decided <unk> cameras were the wave of the future its hard to say who talked him away from the style he <unk> here but its everyones loss that he went into his heavily <unk> <unk> direction instead <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> von <unk> is never <unk> in trying out new techniques some of them are very original while others are best <unk> br he depicts <unk> germany as a <unk> train journey with so many cities lying in ruins <unk> <unk> a young american of german descent feels <unk> to help in their <unk> it is not a simple task as he quickly finds outbr br his uncle finds him a job as a night <unk> on the <unk> <unk> line his job is to <unk> to the needs of the passengers when the shoes are <unk> a <unk> mark is made on the <unk> a terrible argument <unk> when a passengers shoes are not <unk> despite the fact they have been <unk> there are many <unk> to the german <unk> of <unk> to such stupid <unk> br the <unk> journey is like an <unk> <unk> mans <unk> through life with all its <unk> and <unk> in one sequence <unk> <unk> through the back <unk> to discover them filled with <unk> bodies appearing to have just escaped from <unk> these images horrible as they are are <unk> as in a dream each with its own terrible impact yet <unk> br\n"
     ]
    }
   ],
   "source": [
    "#用padding的方式对齐数据\r\n",
    "def create_padded_dataset(reader):\r\n",
    "    padded_sents = []\r\n",
    "    labels = []\r\n",
    "    for batch_id, data in enumerate(reader):\r\n",
    "        sent, label = data\r\n",
    "        padded_sent = sent[:seq_len] + [pad_id] * (seq_len - len(sent))\r\n",
    "        padded_sents.append(padded_sent)\r\n",
    "        labels.append(label)\r\n",
    "    return np.array(padded_sents), np.expand_dims(np.array(labels), axis=1)\r\n",
    "\r\n",
    "train_sents, train_labels = create_padded_dataset(train_reader())\r\n",
    "test_sents, test_labels = create_padded_dataset(test_reader())\r\n",
    "\r\n",
    "print(train_sents.shape)\r\n",
    "print(train_labels.shape)\r\n",
    "print(test_sents.shape)\r\n",
    "print(test_labels.shape)\r\n",
    "\r\n",
    "for sent in train_sents[:3]:\r\n",
    "    print(ids_to_str(sent))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义多头自注意力机制 (Multi-head Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads=8):\r\n",
    "        super(MultiHeadSelfAttention, self).__init__()\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        if embed_dim % num_heads != 0:\r\n",
    "            raise ValueError(\r\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\r\n",
    "            )\r\n",
    "        self.projection_dim = embed_dim // num_heads\r\n",
    "        self.query_dense = dg.Linear(embed_dim, embed_dim)\r\n",
    "        self.key_dense = dg.Linear(embed_dim, embed_dim)\r\n",
    "        self.value_dense = dg.Linear(embed_dim, embed_dim)\r\n",
    "        self.combine_heads = dg.Linear(embed_dim, embed_dim)\r\n",
    "\r\n",
    "    def attention(self, query, key, value):\r\n",
    "        score = layers.matmul(query, key, transpose_y=True)\r\n",
    "        dim_key = layers.cast(layers.shape(key)[-1], 'float32')\r\n",
    "        scaled_score = score / layers.sqrt(dim_key)\r\n",
    "        weights = layers.softmax(scaled_score, axis=-1)\r\n",
    "        output = layers.matmul(weights, value)\r\n",
    "        return output, weights\r\n",
    "\r\n",
    "    def separate_heads(self, x, batch_size):\r\n",
    "        x = layers.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\r\n",
    "        return layers.transpose(x, perm=[0, 2, 1, 3])\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\r\n",
    "        batch_size = layers.shape(inputs)[0]\r\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n",
    "        query = self.separate_heads(\r\n",
    "            query, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        key = self.separate_heads(\r\n",
    "            key, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        value = self.separate_heads(\r\n",
    "            value, batch_size\r\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n",
    "        attention, weights = self.attention(query, key, value)\r\n",
    "        attention = layers.transpose(\r\n",
    "            attention, perm=[0, 2, 1, 3]\r\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\r\n",
    "        concat_attention = layers.reshape(\r\n",
    "            attention, (batch_size, -1, self.embed_dim)\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        output = self.combine_heads(\r\n",
    "            concat_attention\r\n",
    "        )  # (batch_size, seq_len, embed_dim)\r\n",
    "        return output\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义点式前馈网络（Point wise feed forward network）\n",
    "\n",
    "点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。\n",
    "\n",
    "这个网络不会改变向量的大小，只是做了一步提取特征的工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForwardNetwork(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, ff_dim):\r\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\r\n",
    "        self.linear1 = dg.Linear(embed_dim, ff_dim, act='relu')\r\n",
    "        self.linear2 = dg.Linear(ff_dim, embed_dim)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.linear2(out)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Layer):\r\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\r\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\r\n",
    "        self.token_emb = dg.Embedding(size=[vocab_size, embed_dim])\r\n",
    "        self.pos_emb = dg.Embedding(size=[maxlen, embed_dim])\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        maxlen = layers.shape(x)[-1]\r\n",
    "        positions = layers.range(start=0, end=maxlen, step=1, dtype='int64')\r\n",
    "        positions = self.pos_emb(positions)\r\n",
    "        x = self.token_emb(x)\r\n",
    "        return x + positions\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Layer):\r\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n",
    "        super(TransformerBlock, self).__init__()\r\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\r\n",
    "        self.ffn = PointWiseFeedForwardNetwork(embed_dim, ff_dim)\r\n",
    "        self.layernorm1 = dg.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.layernorm2 = dg.LayerNorm(embed_dim, epsilon=1e-6)\r\n",
    "        self.dropout1 = dg.Dropout(rate)\r\n",
    "        self.dropout2 = dg.Dropout(rate)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        attn_output = self.att(inputs)\r\n",
    "        attn_output = self.dropout1(attn_output)\r\n",
    "        out1 = self.layernorm1(inputs + attn_output)\r\n",
    "        ffn_output = self.ffn(out1)\r\n",
    "        ffn_output = self.dropout2(ffn_output)\r\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 组建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyNet(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "        self.emb = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
    "        self.trs = TransformerBlock(embed_dim, num_heads, ff_dim)\r\n",
    "        self.drop1 = dg.Dropout(0.1)\r\n",
    "        self.relu = dg.Linear(ff_dim, 20, act='relu')\r\n",
    "        self.drop2 = dg.Dropout(0.1)\r\n",
    "        self.soft = dg.Linear(20, 2, act='softmax')\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x)\r\n",
    "        x = self.trs(x)\r\n",
    "        x = layers.reduce_mean(x, dim=1)\r\n",
    "        x = self.drop1(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        x = self.drop2(x)\r\n",
    "        x = self.soft(x)\r\n",
    "        return x\r\n",
    "# class MyNet(paddle.nn.Layer):\r\n",
    "#     def __init__(self):\r\n",
    "#         super(MyNet, self).__init__()\r\n",
    "#         self.emb = paddle.nn.Embedding(vocab_size, embed_dim)\r\n",
    "#         self.fc = paddle.nn.Linear(in_features=embed_dim, out_features=2)\r\n",
    "#         self.dropout = paddle.nn.Dropout(0.5)\r\n",
    "\r\n",
    "#     def forward(self, x):\r\n",
    "#         x = self.emb(x)\r\n",
    "#         x = layers.reduce_mean(x, dim=1)\r\n",
    "#         x = self.dropout(x)\r\n",
    "#         x = self.fc(x)\r\n",
    "#         return x\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [0.71156496]\n",
      "epoch: 0, batch_id: 500, loss is: [0.48003972]\n",
      "[validation] accuracy/loss: 0.8522727489471436/0.4543215036392212\n",
      "epoch: 1, batch_id: 0, loss is: [0.52676773]\n",
      "epoch: 1, batch_id: 500, loss is: [0.42795134]\n",
      "[validation] accuracy/loss: 0.8535931706428528/0.45140576362609863\n"
     ]
    }
   ],
   "source": [
    "def train(model):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    opt = paddle.optimizer.Adam(learning_rate=1e-3, parameters=model.parameters())\r\n",
    "\r\n",
    "    for epoch in range(epochs):\r\n",
    "        # shuffle data\r\n",
    "        perm = np.random.permutation(len(train_sents))\r\n",
    "        train_sents_shuffled = train_sents[perm]\r\n",
    "        train_labels_shuffled = train_labels[perm]\r\n",
    "\r\n",
    "        for batch_id in range(len(train_sents_shuffled) // batch_size):\r\n",
    "            x_data = train_sents_shuffled[(batch_id * batch_size):((batch_id+1)*batch_size)]\r\n",
    "            y_data = train_labels_shuffled[(batch_id * batch_size):((batch_id+1)*batch_size)]\r\n",
    "\r\n",
    "            sent = paddle.to_tensor(x_data)\r\n",
    "            label = paddle.to_tensor(y_data)\r\n",
    "\r\n",
    "            logits = model(sent)\r\n",
    "            loss = paddle.nn.functional.softmax_with_cross_entropy(logits, label)\r\n",
    "\r\n",
    "            avg_loss = paddle.mean(loss)\r\n",
    "            if batch_id % 500 == 0:\r\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\r\n",
    "            avg_loss.backward()\r\n",
    "            opt.step()\r\n",
    "            opt.clear_grad()\r\n",
    "\r\n",
    "        # evaluate model after one epoch\r\n",
    "        model.eval()\r\n",
    "        accuracies = []\r\n",
    "        losses = []\r\n",
    "        for batch_id in range(len(test_sents) // batch_size):\r\n",
    "            x_data = test_sents[(batch_id * batch_size):((batch_id+1)*batch_size)]\r\n",
    "            y_data = test_labels[(batch_id * batch_size):((batch_id+1)*batch_size)]\r\n",
    "\r\n",
    "            sent = paddle.to_tensor(x_data)\r\n",
    "            label = paddle.to_tensor(y_data)\r\n",
    "\r\n",
    "            logits = model(sent)\r\n",
    "            loss = paddle.nn.functional.softmax_with_cross_entropy(logits, label)\r\n",
    "            acc = paddle.metric.accuracy(logits, label)\r\n",
    "\r\n",
    "            accuracies.append(acc.numpy())\r\n",
    "            losses.append(loss.numpy())\r\n",
    "\r\n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\r\n",
    "        print(\"[validation] accuracy/loss: {}/{}\".format(avg_acc, avg_loss))\r\n",
    "\r\n",
    "        model.train()\r\n",
    "\r\n",
    "model = MyNet()\r\n",
    "train(model)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到经过两轮的迭代训练，可以达到85%左右的准确率，当然你也可以通过调整参数、更改优化方式等等来进一步提升性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
