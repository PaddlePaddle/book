{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data55290\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificialNoAnomaly    README.md       realAWSCloudwatch  realTraffic\r\n",
      "artificialWithAnomaly  realAdExchange  realKnownCause\t  realTweets\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 8.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.0.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, you need to use the persistence path as the following:\n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可:\r\n",
    "# Also add the following code, so that every time the environment (kernel) starts, just run the following code:\r\n",
    "import sys\r\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 题目 通过AutoEncoder实现时序数据异常检测\n",
    "\n",
    "作者信息：[https://github.com/liu824](http://)\\\n",
    "创建时间：2020年10月10日10:25:53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 介绍\n",
    "#### 该脚本演示了如何使用重建卷积自动编码器模型来检测时间序列数据中的异常。\n",
    "#### 数据来源于Numenta Anomaly Benchmark(NAB)数据集：[https://www.kaggle.com/boltzmannbrain/nab](http://)\n",
    "#### 项目已经提供已经下载好的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !unzip data/data55290/archive.zip -d work/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import paddle.fluid as fluid\r\n",
    "import paddle.fluid.layers as layers\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from paddle.fluid.dygraph import Conv2D\r\n",
    "from paddle.fluid.dygraph.nn import Conv2DTranspose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载数据 我们将使用Numenta Anomaly Benchmark(NAB)数据集。它提供了包含标记的异常行为周期的人为时间序列数据。数据是有序的，带有时间戳的单值指标。\n",
    "\n",
    "### 我们将使用该art_daily_small_noise.csv文件进行训练，并使用该 art_daily_jumpsup.csv文件进行测试。该数据集的简单性使我们能够有效地演示异常检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_url_root = \"work/\"\r\n",
    "\r\n",
    "df_small_noise_url_suffix = \"artificialNoAnomaly/artificialNoAnomaly/art_daily_small_noise.csv\"\r\n",
    "df_small_noise_url = master_url_root + df_small_noise_url_suffix\r\n",
    "df_small_noise = pd.read_csv(\r\n",
    "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\r\n",
    ")\r\n",
    "\r\n",
    "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/artificialWithAnomaly/art_daily_jumpsup.csv\"\r\n",
    "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\r\n",
    "df_daily_jumpsup = pd.read_csv(\r\n",
    "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_small_noise.head())\r\n",
    "\r\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 可视化数据 时间序列数据无异常 我们将使用以下数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\r\n",
    "df_small_noise.plot(legend=False, ax=ax)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 时间序列数据异常 我们将使用以下数据进行测试，并查看数据中突然跳升是否被检测为异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\r\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备训练数据\n",
    "### 从training timeseries数据文件中获取数据值，并对值数据进行规范化。我们有一个14天内每天5分钟的值。\n",
    " 24 * 60 / 5 = 288 timesteps per day\\\n",
    " 288 * 14 = 4032 data points in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize and save the mean and std we get,\r\n",
    "# for normalizing test data.\r\n",
    "training_mean = df_small_noise.mean()\r\n",
    "training_std = df_small_noise.std()\r\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\r\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 创建序列 创建结合TIME_STEPS训练数据中的连续数据值的序列。时间滑窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TIME_STEPS = 288\r\n",
    "\r\n",
    "# Generated training sequences for use in the model.\r\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\r\n",
    "    output = []\r\n",
    "    for i in range(len(values) - time_steps):\r\n",
    "        output.append(values[i : (i + time_steps)])\r\n",
    "    return np.stack(output)\r\n",
    "\r\n",
    "\r\n",
    "x_train = create_sequences(df_training_value.values)\r\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 创建data_generete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\r\n",
    "\r\n",
    "def data_generete():\r\n",
    "    for i in x_train:\r\n",
    "        yield i\r\n",
    "\r\n",
    "batch_reader = fluid.io.batch(data_generete, batch_size=128)\r\n",
    "\r\n",
    "for id,data in enumerate(batch_reader()):\r\n",
    "    print(id,len(data),data[0].shape)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 建立模型 我们将建立卷积重建自动编码器模型。该模型将接受形状的输入，(batch_size, sequence_length, num_features)并返回相同形状的输出。在这种情况下，sequence_length288 num_features为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#飞桨中并没有1x1的卷积，下面是1x1卷积的实现方法，按照原代码每次squeeze之后会接unsqueeze,\r\n",
    "#其实每次输入并不需要进行这么多多余的操作，因此在网络中并没有使用Conv1D，Conv1DTranspose,在此附上仅供参考\r\n",
    "def Conv1D(input,num_filters,filter_size,stride,padding):\r\n",
    "    y=layers.unsqueeze(input,axes=[0])\r\n",
    "    y=layers.conv2d(y, num_filters, filter_size, stride, padding)\r\n",
    "    y=layers.squeeze(y,axes=[0,3])\r\n",
    "    return y\r\n",
    "def Conv1DTranspose(input,num_filters,filter_size,stride,padding):\r\n",
    "    y=layers.unsqueeze(input,axes=[0])\r\n",
    "    y=layers.conv2d(y, num_filters, filter_size, stride, padding)\r\n",
    "    y=layers.squeeze(y,axes=[0,3])\r\n",
    "    return y\r\n",
    "\r\n",
    "class MyModel(fluid.dygraph.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyModel,self).__init__()\r\n",
    "        self.conv1=Conv2D(num_channels=1,num_filters=32,filter_size=(7,1),stride=(2,1),padding=(3,0),act='relu')\r\n",
    "        self.conv2=Conv2D(num_channels=32,num_filters=16,filter_size=(7,1),stride=(2,1),padding=(3,0),act='relu')\r\n",
    "        self.conv2dtranspose1=Conv2DTranspose(num_channels=16,num_filters=16,filter_size=(6,1),stride=(2,1),padding=(2,0),act='relu')#paddle中没有padding='same'，为了输出对齐我对filter_size和padding进行了调整\r\n",
    "        self.conv2dtranspose2=Conv2DTranspose(num_channels=16,num_filters=32,filter_size=(6,1),stride=(2,1),padding=(2,0),act='relu')#paddle中没有padding='same'，为了输出对齐我对filter_size和padding进行了调整\r\n",
    "        self.conv2dtranspose3=Conv2DTranspose(num_channels=32,num_filters=1,filter_size=(7,1),stride=(1,1),padding=(3,0))\r\n",
    "    def forward(self,input):\r\n",
    "        y=layers.unsqueeze(input,axes=[1])\r\n",
    "        y=self.conv1(y)\r\n",
    "        y=layers.dropout(y, dropout_prob=0.2)\r\n",
    "        y=self.conv2(y)\r\n",
    "        y=self.conv2dtranspose1(y)\r\n",
    "        y=layers.dropout(y, dropout_prob=0.2)\r\n",
    "        y=self.conv2dtranspose2(y)\r\n",
    "        y=self.conv2dtranspose3(y)\r\n",
    "        y=layers.squeeze(input=y,axes=[1])\r\n",
    "        return y\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Batch=0\r\n",
    "Batchs=[]\r\n",
    "all_train_loss=[]\r\n",
    "def draw_train_loss(Batchs, train_loss):\r\n",
    "    title=\"training loss\"\r\n",
    "    plt.title(title, fontsize=24)\r\n",
    "    plt.xlabel(\"batch\", fontsize=14)\r\n",
    "    plt.ylabel(\"loss\", fontsize=14)\r\n",
    "    plt.plot(Batchs, train_loss, color='red', label='training loss')\r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "place = fluid.CUDAPlace(0) \r\n",
    "with fluid.dygraph.guard(place):\r\n",
    "    model=MyModel() #模型实例化\r\n",
    "    model.train() #训练模式\r\n",
    "    # opt=fluid.optimizer.SGDOptimizer(learning_rate=train_parameters['learning_strategy']['lr'], parameter_list=model.parameters())#优化器选用SGD随机梯度下降，学习率为0.001.\r\n",
    "    opt=fluid.optimizer.AdamOptimizer(learning_rate=0.001, parameter_list=model.parameters()) \r\n",
    "    epochs_num=50#迭代次数\r\n",
    "    batch_reader = fluid.io.batch(data_generete, batch_size=128)\r\n",
    "    for pass_num in range(epochs_num):\r\n",
    "        for batch_id, data in enumerate(batch_reader()): \r\n",
    "            x_data = np.array(data).astype('float32')\r\n",
    "            y_data = np.array(data).astype('float32')\r\n",
    "            \r\n",
    "            x_data = fluid.dygraph.to_variable(x_data)\r\n",
    "            y_data = fluid.dygraph.to_variable(y_data)\r\n",
    "      \r\n",
    "            # print(x_data.shape, y_data.shape)\r\n",
    "\r\n",
    "            predict=model(x_data)\r\n",
    "            # print(predict.shape)\r\n",
    "            loss=fluid.layers.mse_loss(predict,y_data)\r\n",
    "            avg_loss=fluid.layers.mean(loss)#获取loss值\r\n",
    "            avg_loss.backward()       \r\n",
    "            opt.minimize(avg_loss)    #优化器对象的minimize方法对参数进行更新 \r\n",
    "            model.clear_gradients()   #model.clear_gradients()来重置梯度\r\n",
    "            if batch_id!=0 and batch_id%10==0:\r\n",
    "                Batch = Batch+10 \r\n",
    "                Batchs.append(Batch)\r\n",
    "                all_train_loss.append(avg_loss.numpy()[0])\r\n",
    "        print(\"epoch:{},batch_id:{},train_loss:{}\".format(pass_num,batch_id,avg_loss.numpy()))     \r\n",
    "\r\n",
    "    fluid.save_dygraph(model.state_dict(),'MyModel')#保存模型\r\n",
    "    fluid.save_dygraph(opt.state_dict(),'MyModel')#保存模型\r\n",
    "    print(\"Final loss: {}\".format(avg_loss.numpy()))    \r\n",
    "#让我们绘制训练图和验证损失图，以了解训练的进行情况。     \r\n",
    "draw_train_loss(Batchs,all_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 让我们绘制训练图和验证损失图，以了解训练的进行情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_train_loss(Batchs,all_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 检测异常\n",
    "我们将通过确定模型重构输入数据的程度来检测异常。\n",
    "\n",
    "在训练样本上找到MAE损失。\n",
    "查找最大MAE损失值。这是我们的模型在尝试重建样本时执行的最糟糕的情况。我们将其threshold用于异常检测。\n",
    "如果样本的重建损失大于该threshold 值，那么我们可以推断出该模型正在看到它不熟悉的模式。我们将此样品标记为anomaly。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#模型评估\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    accs = []\r\n",
    "    model_dict, _ = fluid.load_dygraph('MyModel')\r\n",
    "    # model = MyDNN()\r\n",
    "    model=MyModel()\r\n",
    "    model.load_dict(model_dict) #加载模型参数\r\n",
    "    train_mae_loss=[]\r\n",
    "    batch_reader = fluid.io.batch(data_generete, batch_size=1)\r\n",
    "    for batch_id, data in enumerate(batch_reader()):\r\n",
    "        x_data = np.array(data).astype('float32')\r\n",
    "        x_data = fluid.dygraph.to_variable(x_data)\r\n",
    "        x_train_pred=model(x_data)\r\n",
    "        loss=fluid.layers.mse_loss(x_train_pred,x_data)\r\n",
    "        mae_loss=fluid.layers.mean(loss)#获取loss值\r\n",
    "        # train_mae_loss = np.mean(np.abs(x_train_pred - x_data), axis=1)\r\n",
    "        train_mae_loss.append(mae_loss.numpy()[0])\r\n",
    "        # print(train_mae_loss.numpy()[0])\r\n",
    "        if batch_id%10==0:\r\n",
    "            print(\"batch:{} is ok\".format(batch_id))\r\n",
    "    train_mae_loss=np.reshape(np.array(train_mae_loss),(np.array(train_mae_loss).shape[0],1))\r\n",
    "    plt.hist(train_mae_loss, bins=50)\r\n",
    "    plt.xlabel(\"Train MAE loss\")\r\n",
    "    plt.ylabel(\"No of samples\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    # Get reconstruction loss threshold.\r\n",
    "    threshold = np.max(train_mae_loss)\r\n",
    "    print(\"Reconstruction error threshold: \", threshold)   \r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\r\n",
    "plt.plot(x_train[0])\r\n",
    "plt.plot(x_train_pred.numpy()[0])\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_test(values, mean, std):\r\n",
    "    values -= mean\r\n",
    "    values /= std\r\n",
    "    return values\r\n",
    "\r\n",
    "\r\n",
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\r\n",
    "fig, ax = plt.subplots()\r\n",
    "df_test_value.plot(legend=False, ax=ax)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Create sequences from test values.\r\n",
    "x_test = create_sequences(df_test_value.values)\r\n",
    "print(\"Test input shape: \", x_test.shape)\r\n",
    "def testdata_generete():\r\n",
    "    for i in x_test:\r\n",
    "        yield i\r\n",
    "\r\n",
    "\r\n",
    "#Get test MAE loss\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    accs = []\r\n",
    "    model_dict, _ = fluid.load_dygraph('MyModel')\r\n",
    "    # model = MyDNN()\r\n",
    "    model=MyModel()\r\n",
    "    model.load_dict(model_dict) #加载模型参数\r\n",
    "    test_mae_loss=[]\r\n",
    "    testbatch_reader = fluid.io.batch(testdata_generete, batch_size=1)\r\n",
    "    for batch_id, data in enumerate(testbatch_reader()):\r\n",
    "        x_data = np.array(data).astype('float32')\r\n",
    "        x_data = fluid.dygraph.to_variable(x_data)\r\n",
    "        x_train_pred=model(x_data)\r\n",
    "        loss=fluid.layers.mse_loss(x_train_pred,x_data)\r\n",
    "        mae_loss=fluid.layers.mean(loss)#获取loss值\r\n",
    "        # train_mae_loss = np.mean(np.abs(x_train_pred - x_data), axis=1)\r\n",
    "        test_mae_loss.append(mae_loss.numpy()[0])\r\n",
    "        # print(train_mae_loss.numpy()[0])\r\n",
    "        # if batch_id%10==0:\r\n",
    "        #     print(\"batch:{} is ok\".format(batch_id))\r\n",
    "    test_mae_loss=np.array(test_mae_loss)\r\n",
    "# # Get test MAE loss.\r\n",
    "# x_test_pred = model.predict(x_test)\r\n",
    "# test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\r\n",
    "# test_mae_loss = test_mae_loss.reshape((-1))\r\n",
    "\r\n",
    "plt.hist(test_mae_loss, bins=50)\r\n",
    "plt.xlabel(\"test MAE loss\")\r\n",
    "plt.ylabel(\"No of samples\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Detect all the samples which are anomalies.\r\n",
    "anomalies = test_mae_loss > threshold\r\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\r\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 绘制异常值图\n",
    "现在我们知道异常的数据样本。这样，我们将从timestamps原始测试数据中找到相应的内容。我们将使用以下方法来做到这一点：\n",
    "\n",
    "假设time_steps = 3，我们有10个训练值。我们x_train将如下所示：\n",
    "\n",
    "0 1 2\\\n",
    "1 2 3\\\n",
    "2 3 4\\\n",
    "3 4 5\\\n",
    "4 5 6\\\n",
    "5 6 7\\\n",
    "6 7 8\\\n",
    "7 8 9\\\n",
    "除初始和最终time_steps-1数据值外，所有其他值都将以time_steps样本数显示 。因此，如果我们知道样本[（3，4，5），（4，5，6），（5，6，7）]是异常，则可以说数据点5是异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\r\n",
    "anomalous_data_indices = []\r\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\r\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\r\n",
    "        anomalous_data_indices.append(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\r\n",
    "fig, ax = plt.subplots()\r\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\r\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.4 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
